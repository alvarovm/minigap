{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***  \n",
    "<h1><center>\n",
    "    miniGAP\n",
    "</center></h1>    \n",
    "\n",
    "***  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can perform all the functions of miniGAP within this notebook or you can create a python script from the last cell of this notebook and run the script in the terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization Tasks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debugging TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell gives us a couple options for debugging Tensorflow.\n",
    "# It is the first code cell, because it must be run, before the TensorFlow library is imported and it is most convenient to import all modules in the next cell\n",
    "# To enable this debugging, you must change one of the debugging flags to True and run this cell *before* importing running later cells\n",
    "# Currently this is only done manually from the notebook, but could be included as a JSON setting in the future if desirable\n",
    "\n",
    "tf_cpu_debugging =False\n",
    "if tf_cpu_debugging:\n",
    "    import os\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "    import tensorflow as tf\n",
    "    #tf.debugging.set_log_device_placement(True)\n",
    "    a = tf.constant(1)\n",
    "    \n",
    "    if tf.test.gpu_device_name():\n",
    "        print(\"GPUs recognized by tensorflow:\", tf.config.list_physical_devices('GPU'))\n",
    "    else:\n",
    "        print(\"No GPU found\")\n",
    "\n",
    "tf_gpu_debugging = False\n",
    "if tf_gpu_debugging:\n",
    "#     See here for possible option to reset memory github.com/tensorflow/tensorflow/issues/36465\n",
    "#     import os \n",
    "#     os.environ['TF_GPU_ALLOCATOR']='cuda_malloc_async'\n",
    "    import tensorflow as tf\n",
    "    tf.debugging.set_log_device_placement(True)\n",
    "    a = tf.constant(1)\n",
    "    if tf.test.gpu_device_name():\n",
    "        print(\"GPUs recognized by tensorflow:\", tf.config.list_physical_devices('GPU'))\n",
    "    else:\n",
    "        print(\"No GPU found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell performs a couple initialization tasks that have to start before even importing all the libraries or settings\n",
    "# 1) Defines a version of miniGAP (hardcoded)\n",
    "# 2) Determines whether this is being run as a notebook or a script with in_notebook(). Some tasks are only performed in the notebook and vice versa.\n",
    "#    in_notebook() is a function that returns True if this code is run from an ipython kernel or False otherwise\n",
    "#    I need to do this first, because the following task is performed conditional on this being run as a script\n",
    "# 3) Times the initial setup of miniGAP if we are running the miniGAP script.\n",
    "#    Initial setup refers to everything starting with this task up to and including gathering structural info (like energies)\n",
    "# 4) Import functions and libraries used by miniGAP \n",
    "# 5) Determines a path to the miniGAP home directory which can be used for reading or writing files.\n",
    "# 6) Determines the date for use in naming output files\n",
    "# 7) Imports a banner to printed at the beginning of script output\n",
    "# 8) Sets a printing format\n",
    "# 9) Compiles some functions as tf.functions\n",
    "# 10) Sets a list of values to be interpreted as Nonetype\n",
    "\n",
    "version = \"0.0.0\"\n",
    "\n",
    "import sys\n",
    "sys.path.append('../code')\n",
    "from general_helpers import *\n",
    "# True if run from ipython kernel or False otherwise\n",
    "in_notebook = check_if_in_notebook()\n",
    "\n",
    "# Time initial setup\n",
    "import time\n",
    "if not in_notebook:\n",
    "    TimeBeforeStartUp = time.time()\n",
    "\n",
    "# import functions\n",
    "## import functions from my files\n",
    "from Molecular_Dynamics import generate_md_traj, make_diatomic\n",
    "from miniGAP_helpers import *\n",
    "from general_helpers import *\n",
    "\n",
    "## import functions from libraries\n",
    "import os.path as path\n",
    "import argparse\n",
    "import json\n",
    "from collections import namedtuple\n",
    "import datetime as dt \n",
    "import resource\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import gpflow\n",
    "from itertools import islice\n",
    "import matplotlib.pyplot as plt\n",
    "from gpflow.utilities import print_summary\n",
    "    \n",
    "# Sets the miniGAP home directory. This assumes the notebook or script is located one directory below the home directory.\n",
    "if in_notebook:\n",
    "    miniGAP_parent_directory = \"../\"\n",
    "else:\n",
    "    miniGAP_parent_directory = path.dirname(path.dirname(path.realpath(__file__))) + \"/\"\n",
    "    \n",
    "# Save date for use in naming output files\n",
    "today = dt.datetime.today()\n",
    "today_string = \"_{:d}_{:02d}_{:02d}\".format(today.year, today.month, today.day)\n",
    "today_string_alt = \"{:d}/{:02d}/{:02d}\".format(today.year, today.month, today.day)\n",
    "\n",
    "# Save date for use in naming output files\n",
    "version_placeholder = \"_VERSION_PLACEHOLDER_\"\n",
    "date_placeholder = \"_DATE_PLACEHOLDER_\"\n",
    "version_formatted = \"{:{str_len}s}\".format(version, str_len=len(version_placeholder) )\n",
    "date_formatted = \"{:{str_len}s}\".format(today_string_alt, str_len=len(date_placeholder) )\n",
    "miniGAP_banner_filename = miniGAP_parent_directory + \"code/miniGAP_banner.txt\"\n",
    "with open(miniGAP_banner_filename, \"r\") as banner_file:\n",
    "    banner = banner_file.read()\n",
    "banner = banner.replace(version_placeholder, version_formatted).replace(date_placeholder, date_formatted)\n",
    "\n",
    "# Sets the printing format of gpflow model hyperparameters\n",
    "if in_notebook:\n",
    "    gpflow.config.set_default_summary_fmt(\"notebook\")\n",
    "else:\n",
    "    gpflow.config.set_default_summary_fmt(\"grid\")\n",
    "    \n",
    "# Compiles some functions as TensorFlow tf functions not all of which are currently used\n",
    "# Compiled tf functions are several times faster than normal functions\n",
    "mse_tf = tf.function(mse, autograph=False, jit_compile=False)\n",
    "mse_2factor_tf = tf.function(mse_2factor, autograph=False, jit_compile=False)\n",
    "train_hyperparams_without_forces_tf = tf.function(train_hyperparams_without_forces, autograph=False, jit_compile=False)\n",
    "predict_energies_from_weights_tf = tf.function(predict_energies_from_weights, autograph=False, jit_compile=False)\n",
    "\n",
    "# This could be used to define what input values will be interpretted as None.\n",
    "# This may be useful if there are common user-input mistakes when setting a value to None from the commandline or JSON input.\n",
    "# However, I do not use this yet.\n",
    "nonetypes = [None, \"None\", \"null\", \"\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some input parameter notes (not comprehensive):\n",
    "\n",
    "my_priority = #\"efficiency\" for experimenting with something new or otherwise \"consistency\"\n",
    "\n",
    "controls initial train_test_split breaking apart training and test data  \n",
    "split_seed = 2\n",
    "\n",
    "controls in-training train_test_split breaking apart training_j and validation_j data  \n",
    "valid_split_seed = 2\n",
    "\n",
    "controls multiple tf stochastic processes including:  \n",
    "1) Adams optimizer AND   \n",
    "2) batching through tf.data.Dataset.from_tensor_slices in training  \n",
    "tf_seed = 2\n",
    "\n",
    "controls batching through tf.data.Dataset.from_tensor_slices in training  \n",
    "shuffle_seed = 2\n",
    "\n",
    "kernel_type = #\"polynomial\" for actual GAP or \"exponentiated_quadratic\" possibly for debugging  \n",
    "\n",
    "prediction_calculation = #\"direct\" OR \"predict_f\" OR \"cholesky\" OR \"alpha\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell imports settings from the JSON file and saves them to JSON_settings_dict \n",
    "# Some settings of JSON_settings_dict may be overwritten by commandline args, but the JSON_settings_dict variable is not edited.\n",
    "# It is unused now, but might be useful for debugging.\n",
    "# Note 1: You can change the input parameters in the JSON file and rerun the notebook starting from here\n",
    "# Note 2: To debug or reformat the JSON file, I recommend jsonformatter.org\n",
    "# Note 3: The settings file from which these data are imported has a nested structure. This is exclusively for ease of navigation for the user.\n",
    "#         The parent settings names such as \"debugging_settings\" are completely ignored by the code\n",
    "#         Therefore, you can also use a settings file saved from a previous run, which may not have a nested structure.\n",
    "\n",
    "settings_json_filename = miniGAP_parent_directory + \"code/miniGAP_settings.json\"\n",
    "with open(settings_json_filename, encoding = 'utf-8') as settings_file_object:\n",
    "    JSON_settings_dict_nested = json.load(settings_file_object)\n",
    "JSON_settings_dict = flatten_dict(JSON_settings_dict_nested)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell allows the miniGAP script to accept commandline parameters\n",
    "# These commandline parameters have priority over the JSON settings\n",
    "# Most, but not all of the JSON settings can be overwritten using a commandline option\n",
    "\n",
    "if not in_notebook:\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    # arguments for debugging\n",
    "    parser.add_argument('--verbose', type=bool, help=\"Print out details at each step\") \n",
    "    parser.add_argument('-vt', '--print_timings', type=bool, help=\"Print out details at each step\") \n",
    "\n",
    "    # arguments specific to forming dataset (including potentially creating md trajectory)\n",
    "    parser.add_argument('-sf', '--structure_file', help=\"Specify a structure file to import. 'None' will be interpretted as using no structure file.\")\n",
    "    parser.add_argument('-cf', '--chemical_formula',  help=\"If no structure file is supplied, you can specify a single structure here and perform md \\\n",
    "    to generate a trajectory that you will use as your dataset. If neither this nor a structure file are supplied, we will use diatomics.\")\n",
    "    parser.add_argument(\"-md\", '--molecular_dynamics', type=bool, help=\"Indicate if you want molecular dynamics performed. Will generate diatomic if no structure given\")\n",
    "    # available as a JSON parameter, but commandline argument is buggy\n",
    "#     parser.add_argument('-mdi', '--md_indices', default=[0], type=int, nargs='*', help=\"If performing molecular dynamics on a structure file with multiple structures, you can give indices of all structures to perform md on.\")\n",
    "    parser.add_argument('-mdi', '--md_index', type=int, help=\"If performing molecular dynamics on a structure file with multiple structures, you can give the index of the structure to perform md on.\")\n",
    "    parser.add_argument('-de', '--diatomic_element',  choices = [\"N\", \"O\", \"H\"], help=\"If generating diatomics, you can specify element\")\n",
    "    parser.add_argument('-dbl', '--diatomic_bond_length',  type=float, help=\"If generating diatomics, you can specify initial bond length\")\n",
    "    parser.add_argument('-mdt', '--md_temp',  type=float, help=\"If performing molecular dynamics, specify temperatutre (K) of MD\")\n",
    "    parser.add_argument('-mda', '--md_algorithm',  choices = [\"VelocityVerlet\", \"Berendsen\"], type=str, help=\"If performing molecular dynamics, specify algorithm of MD\")\n",
    "    parser.add_argument('-mts', '--md_time_step',  type=float, help=\"If performing molecular dynamics, specify time step (fs) of MD\")\n",
    "    parser.add_argument('-mds', '--md_seed',  type=int, help=\"If performing molecular dynamics, change this seed to get different trajectories\")\n",
    "    parser.add_argument('-mec', '--md_energy_calculator',  choices = [\"EMT\", \"LJ\", \"Morse\"], help = \"If performing molecular dynamics, specify ASE energy/force calculator\")\n",
    "    parser.add_argument('-n', '--n_structs',  type=int, help=\"Specify # of md generated structures or # of structures to use from input file\")\n",
    "\n",
    "    # arguments specific to soap\n",
    "    parser.add_argument('--rcut',  type=float, help= \"Choice of SOAP cut off radius\")\n",
    "    parser.add_argument('--nmax',  type=int, help=\"Choice of SOAP n_max\")\n",
    "    parser.add_argument('--lmax',  type=int, help=\"Choice of SOAP l_max\")\n",
    "\n",
    "    # arguments specific to learning\n",
    "    parser.add_argument('-ss', '--split_seed', type=int, help=\"Random seed for cross-validation\")\n",
    "    parser.add_argument('-tf', '--train_fraction', type=float, help=\"Specify the fraction of structures used in training\")\n",
    "    parser.add_argument('-ne', '--n_epochs', type=int, help=\"Number of epochs\")\n",
    "\n",
    "#     some housekeeping\n",
    "#     parser.add_argument('remainder', nargs=argparse.REMAINDER, help=argparse.SUPPRESS)\n",
    "\n",
    "    cmdline_args = parser.parse_args()\n",
    "    cmdline_args_dict = vars(cmdline_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell creates a namedtuple variable, 's', which stores all settings. For details on how to use a namedtuple, see next cell.\n",
    "# If this is run as a script then, prior to creating s, we check if there are commandline arguments.\n",
    "# Any commandline argument overwrites the default argument from the JSON settings file in the dictionary 'settings_dict'\n",
    "# The settings from the JSON file are used whenever no commandline argument exists.\n",
    "\n",
    "settings_dict = JSON_settings_dict.copy()\n",
    "# Synchronize the JSON and commandline settings\n",
    "if not in_notebook:\n",
    "    for setting_name in settings_dict.keys():\n",
    "        if setting_name in cmdline_args_dict.keys():\n",
    "            if cmdline_args_dict[setting_name] != None:\n",
    "                settings_dict[setting_name] = cmdline_args_dict[setting_name]\n",
    "    \n",
    "    for setting_name in cmdline_args_dict.keys():\n",
    "        if setting_name not in settings_dict.keys():\n",
    "            print(\"The commandline argument {} is currently nonfunctional because it does not exist in the JSON file.\".format(setting_name))\n",
    "\n",
    "# Created the namedtuple variable storing all the settings\n",
    "# I name the namedtuple settings object with one letter 's' instead of using 'Settings' to minimize disruption of the code\n",
    "# However, you may need for the variable name and the string assigned to 'typename' to be the same if you are pickling.\n",
    "# 'typename' is the first parameter of namedtuple() and was 'Settings' when I wrote this comment\n",
    "SettingsNamespace = namedtuple(\"Settings\", settings_dict.keys())\n",
    "s = SettingsNamespace(*settings_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the flag to True to see a demonstration of how to use a namedtuple variable\n",
    "\n",
    "demonstrate_named_tuple=False\n",
    "\n",
    "if demonstrate_named_tuple:\n",
    "    # You can iterate through the elements of a named_tuple\n",
    "    # s._fields contains the setting names or fields\n",
    "    n_settings_to_print = 10\n",
    "    print(\"The first {} fields and values within s:\".format(n_settings_to_print) )\n",
    "    for setting_field, setting_value in zip(s._fields[:n_settings_to_print], s[:n_settings_to_print]):\n",
    "        print(\"\\t{} = {}\".format(setting_field, setting_value))\n",
    "    print(\"\\t...\\n\")\n",
    "    \n",
    "    # You can acccess variables of a named tuple in 2 ways\n",
    "    print(\"2 ways of accessing values stored in s:\")\n",
    "    # 1) As an attribute\n",
    "    print(\"\\t1) s.structure_file = {}\".format(s.structure_file) )\n",
    "    \n",
    "    # 2) Indexing\n",
    "    index_for_demonstration = [i for i in range(len(s._fields)) if s._fields[i] == \"structure_file\"][0]\n",
    "    print(\"\\t2) s[{}] = {}\".format(index_for_demonstration, s[index_for_demonstration]))\n",
    "\n",
    "    # I exclusively use this method in miniGAP code because this allows me to just prepend 's.' to any variable I had been using \n",
    "    #     from my previous code, which defined all variables within a notebook and had no JSON or commandline arg input.\n",
    "    # Another convenience of using a namedtuple is that I can just replace 's.' with 'self.' in the futurewhen when I convert \n",
    "    #     the majority of miniGAP code into a GAP class.\n",
    "\n",
    "# For more details see docs.python.org/3/library/collections.html#collections.namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print banner in script unless supressed\n",
    "# Had to wait until we had the suppress setting to do this\n",
    "if not (in_notebook or s.suppress_banner):\n",
    "    print(banner, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not create '../results/Debugging_Sean_2021_12_10', because it already existed. Created '../results/Debugging_Sean_2021_12_10_15' instead.\n",
      "Input settings for this calculation stored in ../results/Debugging_Sean_2021_12_10_15/miniGAP.settings\n"
     ]
    }
   ],
   "source": [
    "# This cell allows for saving results\n",
    "# If the user so chooses:\n",
    "#     1) A new subdirectory will be created for the results output files\n",
    "#            The new subdirectory is /minigap/results/CALCULATIONTITLE where CALCULATIONTITLE is determined by user input and existing directories\n",
    "#     2) All output (stdout) printed to terminal is also saved to a file in CALCULATIONTITLE if run from a script\n",
    "#     3) The settings are saved to a file in CALCULATIONTITLE similar to the JSON input file\n",
    "\n",
    "if s.make_output_files:\n",
    "    # Make new subdirectory\n",
    "    # The name can be given by s.title or it will just be 'results'\n",
    "    # If s.append_date_to_title is set to True, the date will be added to the end\n",
    "    # If a directory already exists with the attempted name, the lowest possible integer to make a unique name is added to the end\n",
    "    calculation_results_directory = make_miniGAP_results_subdirectory(s, date=today_string, miniGAP_parent_directory=miniGAP_parent_directory)\n",
    "    \n",
    "    # Start saving output to a log file as well as printing it to the terminal\n",
    "    # Not currently implemented for notebook, because cells might not be run in order which would create a confusing log\n",
    "    if not in_notebook:\n",
    "        log_filename = calculation_results_directory + \"miniGAP.log\"\n",
    "        if s.verbose:\n",
    "            print(\"Logging all output starting after this message into {}\".format(log_filename))  \n",
    "        logger = Logger(log_filename)\n",
    "        \n",
    "        # Place banner as first entry into logfile\n",
    "        # We had to print to terminal first so we use logfile_only=True to avoid dublication \n",
    "        logger.write(banner, logfile_only=True)\n",
    "    \n",
    "    # Save settings used for this calculation for future reference\n",
    "    output_JSON_filename = calculation_results_directory + \"miniGAP.settings\"    \n",
    "    with open(output_JSON_filename, 'w', encoding='utf-8') as settings_output_file:\n",
    "        json.dump(settings_dict, settings_output_file, ensure_ascii=False, indent=4)\n",
    "    if s.verbose:\n",
    "        print(\"Input settings for this calculation stored in {}\".format(output_JSON_filename))\n",
    "else:\n",
    "    # This variable is not used, but needs to exist for positional arguments\n",
    "    # Ideally we would handle this differently\n",
    "    calculation_results_directory = \"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization Tasks (continued)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stack size set to unlimited\n",
      "1 GPU(s) recognized by tensorflow: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# This cell handles initialization tasks which could not be completed previously.\n",
    "# These tasks potentially print out some output depending on the s.verbose setting so they could not be performed\n",
    "#     1) prior to importing the settings, or\n",
    "#     2) prior to initiating the logging (or else the outputs would not be logged.) \n",
    "\n",
    "# miniGAP uses a lot memory so it is good to allow it access to as much as possible\n",
    "try:\n",
    "    resource.setrlimit( resource.RLIMIT_STACK, ( resource.RLIM_INFINITY, resource.RLIM_INFINITY ) )\n",
    "    if s.verbose:\n",
    "        print(\"Stack size set to unlimited\")\n",
    "except:\n",
    "    print(\"Warning: Unable to raise stack size limit. Typically miniGAP uses a lot of memory and operates better if you allow the stack size to be unlimited. You can try to do this with the command 'ulimit -s unlimited'\")\n",
    "\n",
    "# Check on GPU availability\n",
    "if s.verbose:\n",
    "    print(\"{} GPU(s) recognized by tensorflow:\".format(len(tf.config.list_physical_devices('GPU'))), tf.config.list_physical_devices('GPU'))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported 1000 structures from ../data/qm7b.db. Structures were taken uniformly from throughout dataset which contains 7211 total structures.\n",
      "Compiling structures into list took 2.95 seconds\n"
     ]
    }
   ],
   "source": [
    "# This cell compiles the structure dataset to be used by miniGAP\n",
    "# You have several options for how to choosing this dataset:\n",
    "# 1) Import structure dataset directly from file\n",
    "# 2) Import a structure from a file and then run an MD simulation with ASE. \n",
    "#    The structures in the MD trajectory will be used as the dataset.\n",
    "# 3) Do not use a file ('\"structure_file\" : null' in the JSON). Specify the chemical formula of a molecule within the g2 collection and then run an MD simulation with ASE.\n",
    "#    The structures in the MD trajectory will be used as the dataset.\n",
    "#    Information about the g2 collection:\n",
    "#      - https://aip.scitation.org/doi/10.1063/1.473182\n",
    "#      - https://wiki.fysik.dtu.dk/ase/ase/build/build.html#molecules\n",
    "# 4) Do not use a file or specify a chemical formula('\"structure_file\" : null'  and '\"chemical_formula\" : null' in the JSON).\n",
    "#    This is like option 3, but the starter molecule will be a diatomic.\n",
    "#    You can specify the diatomic element and initial bond length in the JSON or commandline arguments.\n",
    "\n",
    "# Note 1: If you don't include a path in the filename, miniGAP will look in the /minigap/data/ directory \n",
    "# Note 2: Currently only the force fields with a native ASE implementation are implemented for MD generation of a dataset.\n",
    "#         These forcfield are \"EMT\", \"LJ\", and \"Morse\". All perform very poorly for nearly all molecules and structures.\n",
    "#         An exception is diatomic molecules, for which they capture the most important behavior.\n",
    "# Note 3: You can see here that I use the function 'TickTock'. The real function is 'CompileStructureList'.\n",
    "#         'TickTock' is is just for timing purposes. See the next cell for more details.\n",
    "\n",
    "StructureList, TimeCompileStructures = TickTock(CompileStructureList, s, in_notebook, miniGAP_parent_directory, calculation_results_directory)\n",
    "if s.print_timings:\n",
    "    print(\"Compiling structures into list took {:.2f} seconds\".format(TimeCompileStructures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You will see the TickTock function used throughout this notebook\n",
    "# It is a helper function that allows me to time other functions concisely.\n",
    "# For an example usage set the below flag to True and inspect the code\n",
    "\n",
    "see_example_of_TickTock_usage = False\n",
    "if see_example_of_TickTock_usage:\n",
    "    def example_function(a, b, c=\"DEFAULT_VALUE\", d=\"DEFAULT_VALUE\", function_call_type=\"normal\"):\n",
    "        summation=0\n",
    "        for i in range(a):\n",
    "            summation += b\n",
    "        print(\"This {} function call accepted the argument '{}' for the positional parameter 'a'\".format(function_call_type, a))\n",
    "        print(\"This {} function call accepted the argument '{}' for the positional parameter 'b'\".format(function_call_type, b))\n",
    "        print(\"This {} function call accepted the argument '{}' for the keyword parameter 'c'\".format(function_call_type, c))\n",
    "        print(\"This {} function call accepted the argument '{}' for the keyword parameter 'd'\".format(function_call_type, d))\n",
    "        return summation\n",
    "    \n",
    "    a_value = 1234567\n",
    "    b_value = 1\n",
    "    c_value = \"CAT\"\n",
    "    d_value = \"DOG\"\n",
    "    \n",
    "    # You can use the function the normal way:\n",
    "    normal_function_output = example_function(a_value, b_value, c=c_value, d=d_value, function_call_type=\"normal\")\n",
    "    print(\"This normal function call returned {}\\n\".format(normal_function_output) )\n",
    "    \n",
    "    # You can time the function with a slight modification, no extra lines needed\n",
    "    TickTock_function_output, function_time = TickTock(example_function, a_value, b_value, c=c_value, d=d_value, function_call_type=\"TickTock\")\n",
    "    print(\"This TickTock function call returned {}\".format(TickTock_function_output) )\n",
    "    print(\"This TickTock function call took {:.2f} seconds to run\".format(function_time) )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set convert flag to True and run this cell to convert your file to a database right now from this notebook\n",
    "# I do not recommend this. The database creation is very slow and will lock you out from running any cells for a long time.\n",
    "# It is better to do this from a terminal with the script convert_to_db.py\n",
    "# For example, you can run the command '/relative/path/to/minigap/code/convert_to_db.py structure_file_a.xyz structure_file_b.xyz structure_file_c.xyz'\n",
    "# This command would create the files structure_file_a.db, structure_file_b.db and structure_file_c.db in your /relative/path/to/minigap/data/ directory\n",
    "# To overwrite an existing .db file use the --existing_file_behavior flag\n",
    "# convert_to_db.py --help will explain some more details\n",
    "convert_to_db_here_and_now = False\n",
    "if convert_to_db_here_and_now and in_notebook:\n",
    "    !python ../code/convert_to_db.py $s.structure_file \n",
    "    # Use the following line instead of the previous line if you need to overwrite an existing database\n",
    "    # !python ../code/convert_to_db.py -efb overwrite $s.structure_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this flag to True if you want to visualize your structure within this jupyter notebook (no pop-up window)\n",
    "# For more details on the plotting function, refer to the Visualize_Structures notebook\n",
    "\n",
    "visualize_structure_in_notebook = False\n",
    "\n",
    "# Having to use (s.save_dataset_animation & s.make_output_files) is tedious\n",
    "# Perhaps I can make s.make_output_files overwrite s.save_dataset_animation upon import\n",
    "if visualize_structure_in_notebook or (s.save_dataset_animation & s.make_output_files):\n",
    "    StructureDatasetAnimation, TimeVisualize = TickTock(create_miniGAP_visualization, StructureList, s, calculation_results_directory)\n",
    "    if visualize_structure_in_notebook and in_notebook:\n",
    "        display(StructureDatasetAnimation)\n",
    "    del StructureDatasetAnimation\n",
    "    \n",
    "    if s.verbose and (s.save_dataset_animation & s.make_output_files):\n",
    "        print(\"Created animation of the structural dataset used in this calculation in the results directory. This might be useful to verify the correct structures are being used.\")\n",
    "    \n",
    "    if s.print_timings:\n",
    "        print(\"Created animation of structures in {:.2f} seconds \".format(TimeVisualize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathered energy and structure info in 2.98 seconds\n"
     ]
    }
   ],
   "source": [
    "# This cell \n",
    "\n",
    "[EnergyList, ForceList, PosList, NAtList], TimeGather = TickTock(GatherStructureInfo, StructureList, gather_forces = s.use_forces, use_self_energies=s.use_self_energies, \n",
    "                                                     alt_energy_keyword = s.alt_energy_keyword)\n",
    "if s.print_timings:\n",
    "    gather_forces_message= \", force\" if s.use_forces else \"\"\n",
    "    print(\"Gathered energy{} and structure info in {:.2f} seconds\".format(gather_forces_message, TimeGather))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AtomCounts = [len(struct) for struct in StructureList[:3]]\n",
    "# LocalBools = tf.repeat(tf.eye(len(AtomCounts), dtype=np.float64), AtomCounts, axis=1)\n",
    "# print(LocalBools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Proof of Concept \n",
    "# ################################################################################\n",
    "# # struct_list = [A, B, C, D]\n",
    "# # all_energies_nested = [struct.get_potential_energies() for struct in struct_list]\n",
    "# all_energies_nested = [[0], [1,2,3], [4,5,6,7], [8,9],[10,11]]\n",
    "# n_atoms = np.array([len(struct) for struct in all_energies_nested])\n",
    "# ################################################################################\n",
    "# train_structure_indices = [0,2,3]; test_structure_indices = [1,4]\n",
    "# ################################################################################\n",
    "# train_n_atoms = n_atoms[train_structure_indices]; test_n_atoms = n_atoms[test_structure_indices]\n",
    "# train_energies_nested = [all_energies_nested[i] for i in train_structure_indices]; test_energies_nested = [all_energies_nested[i] for i in test_structure_indices]\n",
    "# train_energies = np.concatenate(train_energies_nested); test_energies = np.concatenate(test_energies_nested)\n",
    "# train_atom_indices = np.repeat(range(len(train_structure_indices)), train_n_atoms); test_atom_indices = np.repeat(range(len(test_structure_indices)), test_n_atoms)\n",
    "# # [training hyperparamters and making predictions goes here]\n",
    "# # now we have predicted_energies_local =  predict_energies(*args, **kwargs)\n",
    "# predicted_energies_local = my_prediction(test_energies)\n",
    "# predicted_energies_global = [0] * len(test_structure_indices)\n",
    "# for i in range(len(predicted_energies_local)):\n",
    "#     predicted_energy_local_i = predicted_energies_local[i]\n",
    "#     predicted_energies_global[test_atom_indices[i]] += predicted_energy_local_i\n",
    "# print(predicted_energies_global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Proof of Concept New 12/10\n",
    "# ################################################################################\n",
    "# # struct_list = [A, B, C, D]\n",
    "# # all_energies_nested = [struct.get_potential_energies() for struct in struct_list]\n",
    "# all_energies_nested = [[0], [1,2,3], [4,5,6,7], [8,9],[10,11]]\n",
    "# n_atoms = np.array([len(struct) for struct in all_energies_nested])\n",
    "# ################################################################################\n",
    "# train_structure_indices = [0,2,3]; test_structure_indices = [1,4]\n",
    "# ################################################################################\n",
    "# train_n_atoms = n_atoms[train_structure_indices]; test_n_atoms = n_atoms[test_structure_indices]\n",
    "# train_energies_nested = [all_energies_nested[i] for i in train_structure_indices]; test_energies_nested = [all_energies_nested[i] for i in test_structure_indices]\n",
    "# train_energies = np.concatenate(train_energies_nested); test_energies = np.concatenate(test_energies_nested)\n",
    "\n",
    "# train_atom_indices = np.repeat(range(len(train_structure_indices)), train_n_atoms); test_atom_indices = np.repeat(range(len(test_structure_indices)), test_n_atoms)\n",
    "# # [training hyperparamters and making predictions goes here]\n",
    "# # now we have predicted_energies_local =  predict_energies(*args, **kwargs)\n",
    "# predicted_energies_local = my_prediction(test_energies)\n",
    "# predicted_energies_global = [0] * len(test_structure_indices)\n",
    "# for i in range(len(predicted_energies_local)):\n",
    "#     predicted_energy_local_i = predicted_energies_local[i]\n",
    "#     predicted_energies_global[test_atom_indices[i]] += predicted_energy_local_i\n",
    "# print(predicted_energies_global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell completes the timing started in the first cell if this code is executed from a script\n",
    "\n",
    "if s.print_timings and not in_notebook:\n",
    "    TimeAfterStartUp = time.time()\n",
    "    TimeStartUp = TimeAfterStartUp - TimeBeforeStartUp\n",
    "    print(\"Completed the initial setup of miniGAP (including compiling structural data) in {:.2f} seconds\".format(TimeStartUp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated SOAP descriptors in 2.61 seconds\n"
     ]
    }
   ],
   "source": [
    "[SoapDerivativeList, SoapList], TimeSoap = TickTock(GenerateDescriptorsAndDerivatives, StructureList, s.nmax, s.lmax, s.rcut, s.smear, s.attach_SOAP_center, s.is_periodic, s.use_forces)\n",
    "calculate_derivatives_message = \" and derivatives\" if s.use_forces else \"\"\n",
    "if s.print_timings:\n",
    "    print(\"Generated SOAP descriptors{} in {:.2f} seconds\".format(calculate_derivatives_message, TimeSoap))\n",
    "elif s.verbose:\n",
    "    print(\"Generated SOAP descriptors{}.\".format(calculate_derivatives_message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reformatted data to build model in 1.21 seconds.\n"
     ]
    }
   ],
   "source": [
    "out_data, TimePrepare = TickTock(PrepareDataForTraining, \n",
    "                                sp_list=SoapList, \n",
    "                                dsp_dx_list = SoapDerivativeList, \n",
    "                                en_list = EnergyList,\n",
    "                                frc_list = ForceList ,\n",
    "                                pos_list = PosList, \n",
    "                                nat_list = NAtList,\n",
    "                                split_seed = s.split_seed, \n",
    "                                prepare_forces = s.use_forces, \n",
    "                                train_fract = s.train_fraction,\n",
    "                                scale_soaps = s.scale_soaps\n",
    "                                )\n",
    "\n",
    "if s.print_timings:\n",
    "    print(\"Reformatted data to build model in {:.2f} seconds.\".format(TimePrepare))\n",
    "\n",
    "if not s.use_forces:\n",
    "    train_sps_full, test_sps_full, train_ens, test_ens, train_nats, test_nats, train_indices, test_indices, train_struct_bools, test_struct_bools, soap_scaler, ens_scaler, ens_var = out_data\n",
    "else:\n",
    "    train_sps_full, test_sps_full, train_ens, test_ens, train_nats, test_nats, train_indices, test_indices, train_struct_bools, test_struct_bools, soap_scaler, ens_scaler, ens_var, train_dsp_dx, test_dsp_dx, train_frcs, test_frcs, frcs_var = out_data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using any sparsity.\n"
     ]
    }
   ],
   "source": [
    "n_samples_full, n_features_full = train_sps_full.shape\n",
    "SparsifySoapsOutput, TimeSparsify = TickTock( SparsifySoaps, train_soaps = train_sps_full, test_soaps = test_sps_full, train_energies=train_ens, sparsify_samples=s.sparse_gpflow, \n",
    "                                    n_samples=s.n_sparse, sparsify_features=s.sparse_features, n_features=s.n_sparse_features, selection_method=\"PCovCUR\",\n",
    "                                    score_tolerance=1e-5, score_threshold=1e-5, iterative_selection=False, plot_importances=False) \n",
    "train_sps, sparse_train_sps, test_sps = SparsifySoapsOutput\n",
    "if s.sparse_gpflow or s.sparse_features:\n",
    "    sparsity_samples_message = \"\" if not s.sparse_gpflow else \"samples ({} --> {})\".format(n_samples_full, sparse_train_sps.shape[0])\n",
    "    sparsity_features_message = \"\" if not s.sparse_features else \"features ({} --> {})\".format(n_features_full, train_sps.shape[-1])\n",
    "    sparsity_both_message = \" and \" if (s.sparse_gpflow and s.sparse_features) else \"\"\n",
    "    if s.print_timings:\n",
    "        print(\"Sparsified model {}{}{} in {:.2f} seconds.\".format( sparsity_samples_message, sparsity_both_message, sparsity_features_message, TimeSparsify) )\n",
    "    elif s.verbose:\n",
    "        print(\"Sparsified model {}{}{}\".format( sparsity_samples_message, sparsity_both_message, sparsity_features_message) )\n",
    "else:\n",
    "    if s.verbose:\n",
    "        print(\"Not using any sparsity.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future investigations for hyperparameter training\n",
    "---\n",
    "1. Does Adam optimizer have problems sometimes within a tf.function?\n",
    "2. Custom loss function vs optimizer.minimize\n",
    "3. Set certain variable untrainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-10 09:37:21.200904: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-12-10 09:37:21.774661: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9685 MB memory:  -> device: 0, name: NVIDIA TITAN V, pci bus id: 0000:01:00.0, compute capability: 7.0\n",
      "2021-12-10 09:37:21.945992: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using a degree 2 polynomial kernel.\n",
      "Alert: Double check the training validity for degree =/= 1 when not using predict_f\n",
      "Training using 12631 atoms total using 2 batches with 6316 atoms per batch.\n",
      "Epoch 0\n",
      "TRACING train_hyperparams_without_forces\n",
      "TRACING train_hyperparams_without_forces\n",
      "predict energies =  [[0.47193315203607566]\n",
      " [0.21524011962236739]\n",
      " [-1.5248433572471478]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-10 09:37:28.185600: I tensorflow/core/util/cuda_solvers.cc:179] Creating GpuSolver handles for stream 0x562d847d6e20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradients =  (0.03664196789552121, -0.028692115861507335)\n",
      "valid_ens[:3] = [ 0.47059933  0.38357885 -1.79552556]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.09994735915323709 + 0 = 0.09994735915323709 \n",
      "predict energies =  [[0.60555249796677535]\n",
      " [0.035282990382154322]\n",
      " [-1.8226356952954235]]\n",
      "gradients =  (0.028685685404106376, -0.022348342898798525)\n",
      "valid_ens[:3] = [ 0.47059933  0.38357885 -1.79552556]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.09250009470379848 + 0.09994735915323709 = 0.19244745385703557 \n",
      "Epoch 0,  mse = 0.09622372692851779\n",
      "predict energies =  [[0.98233396760493519]\n",
      " [0.22560828880386929]\n",
      " [-1.4606752133972218]]\n",
      "gradients =  (0.033689739982754763, -0.026116425023155931)\n",
      "valid_ens[:3] = [ 0.47059933  0.38357885 -1.79552556]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.09583593259784293 + 0 = 0.09583593259784293 \n",
      "predict energies =  [[0.87112003749116229]\n",
      " [0.34841457467828779]\n",
      " [-1.7390198826856142]]\n",
      "gradients =  (0.023879088103334639, -0.018419031165198354)\n",
      "valid_ens[:3] = [ 0.47059933  0.38357885 -1.79552556]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.08554812023276068 + 0.09583593259784293 = 0.1813840528306036 \n",
      "Epoch 1,  mse = 0.0906920264153018\n",
      "Epoch 2\n",
      "predict energies =  [[0.5822234269711527]\n",
      " [0.12483221488095619]\n",
      " [-1.7958436935937496]]\n",
      "gradients =  (0.026915958311220373, -0.020660569136832112)\n",
      "valid_ens[:3] = [ 0.47059933  0.38357885 -1.79552556]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.09471172382163331 + 0 = 0.09471172382163331 \n",
      "predict energies =  [[0.75533350188521875]\n",
      " [0.56589701338510345]\n",
      " [-1.5350757721342689]]\n",
      "gradients =  (0.032268687499623, -0.024649646261680373)\n",
      "valid_ens[:3] = [ 0.47059933  0.38357885 -1.79552556]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.0871817073743856 + 0.09471172382163331 = 0.18189343119601892 \n",
      "Epoch 2,  mse = 0.09094671559800946\n",
      "predict energies =  [[0.59011266970252907]\n",
      " [0.7025057584595864]\n",
      " [-1.6211082836533977]]\n",
      "gradients =  (0.033279847938609053, -0.025298730719044069)\n",
      "valid_ens[:3] = [ 0.47059933  0.38357885 -1.79552556]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.09827099095914628 + 0 = 0.09827099095914628 \n",
      "predict energies =  [[0.64072730043273585]\n",
      " [0.45938676824714553]\n",
      " [-1.6641743187331457]]\n",
      "gradients =  (0.02291257770156635, -0.017333004065221667)\n",
      "valid_ens[:3] = [ 0.47059933  0.38357885 -1.79552556]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.08754837287371878 + 0.09827099095914628 = 0.18581936383286507 \n",
      "Epoch 3,  mse = 0.09290968191643253\n",
      "Epoch 4\n",
      "predict energies =  [[0.57971371565435437]\n",
      " [0.51967870765973589]\n",
      " [-1.9611470688901838]]\n",
      "gradients =  (0.037481832491658318, -0.0282193877557915)\n",
      "valid_ens[:3] = [ 0.47059933  0.38357885 -1.79552556]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.09834531252192814 + 0 = 0.09834531252192814 \n",
      "predict energies =  [[0.43670148293085859]\n",
      " [0.32115776111639449]\n",
      " [-1.5011145773576047]]\n",
      "gradients =  (0.024538100058381807, -0.018385461810702688)\n",
      "valid_ens[:3] = [ 0.47059933  0.38357885 -1.79552556]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.0831793510604149 + 0.09834531252192814 = 0.18152466358234304 \n",
      "Epoch 4,  mse = 0.09076233179117152\n",
      "predict energies =  [[0.98028767993923283]\n",
      " [0.38015040723375049]\n",
      " [-1.3615557895852115]]\n",
      "gradients =  (0.030107436558875628, -0.022451739816244813)\n",
      "valid_ens[:3] = [ 0.47059933  0.38357885 -1.79552556]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.0821291526173599 + 0 = 0.0821291526173599 \n",
      "predict energies =  [[0.563400653372718]\n",
      " [0.18536652259894892]\n",
      " [-1.7610377667109487]]\n",
      "gradients =  (0.026790468891177865, -0.019884022792134508)\n",
      "valid_ens[:3] = [ 0.47059933  0.38357885 -1.79552556]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.08899124067254761 + 0.0821291526173599 = 0.17112039328990752 \n",
      "Epoch 5,  mse = 0.08556019664495376\n",
      "Epoch 6\n",
      "predict energies =  [[0.40276210215095781]\n",
      " [0.38236880903705273]\n",
      " [-1.9456797309090894]]\n",
      "gradients =  (0.034232684727500823, -0.025289094651752671)\n",
      "valid_ens[:3] = [ 0.47059933  0.38357885 -1.79552556]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.08820220773133068 + 0 = 0.08820220773133068 \n",
      "predict energies =  [[0.68631250564146717]\n",
      " [-0.092559526309510254]\n",
      " [-1.5386877999395061]]\n",
      "gradients =  (0.030302383261905375, -0.022280601115805775)\n",
      "valid_ens[:3] = [ 0.47059933  0.38357885 -1.79552556]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.07992144182179298 + 0.08820220773133068 = 0.16812364955312364 \n",
      "Epoch 6,  mse = 0.08406182477656182\n",
      "predict energies =  [[0.62404690905001148]\n",
      " [0.28930906004795631]\n",
      " [-1.342326900882211]]\n",
      "gradients =  (0.032597484698536322, -0.023856162773266392)\n",
      "valid_ens[:3] = [ 0.47059933  0.38357885 -1.79552556]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.081809644187844 + 0 = 0.081809644187844 \n",
      "predict energies =  [[0.72131234565518287]\n",
      " [0.031709938982274366]\n",
      " [-1.7682873240050752]]\n",
      "gradients =  (0.03014618468888311, -0.021959161354313906)\n",
      "valid_ens[:3] = [ 0.47059933  0.38357885 -1.79552556]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.07793278004165423 + 0.081809644187844 = 0.15974242422949825 \n",
      "Epoch 7,  mse = 0.07987121211474912\n",
      "Epoch 8\n",
      "predict energies =  [[0.57114645093754812]\n",
      " [0.40539679895701569]\n",
      " [-1.7331614279976051]]\n",
      "gradients =  (0.028935195932761365, -0.020979210270354227)\n",
      "valid_ens[:3] = [ 0.47059933  0.38357885 -1.79552556]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.07089333740319946 + 0 = 0.07089333740319946 \n",
      "predict energies =  [[0.62001243551384211]\n",
      " [0.063480015112430621]\n",
      " [-1.5998976443095687]]\n",
      "gradients =  (0.033978503658745773, -0.024522451616912831)\n",
      "valid_ens[:3] = [ 0.47059933  0.38357885 -1.79552556]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.09117493711082336 + 0.07089333740319946 = 0.16206827451402284 \n",
      "Epoch 8,  mse = 0.08103413725701142\n",
      "predict energies =  [[0.54363916149468183]\n",
      " [-0.010976093367738637]\n",
      " [-1.405115445645978]]\n",
      "gradients =  (0.028087499089773704, -0.020177461853045947)\n",
      "valid_ens[:3] = [ 0.47059933  0.38357885 -1.79552556]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.07706422430163683 + 0 = 0.07706422430163683 \n",
      "predict energies =  [[0.59960210975299877]\n",
      " [0.64927408804076658]\n",
      " [-1.6672428040357299]]\n",
      "gradients =  (0.026263989293210888, -0.0187815579429407)\n",
      "valid_ens[:3] = [ 0.47059933  0.38357885 -1.79552556]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.07791429656162885 + 0.07706422430163683 = 0.1549785208632657 \n",
      "Epoch 9,  mse = 0.07748926043163284\n"
     ]
    }
   ],
   "source": [
    "# Initialize kernels and model hyperparameters\n",
    "tf.random.set_seed(s.tf_seed)\n",
    "\n",
    "TimeBeforePreEpoch = time.time()\n",
    "\n",
    "\n",
    "noise_init = 1e-4 #.001# 0.0005499093576274776 #1.625e-4\n",
    "obs_noise = tf.Variable(noise_init, dtype=s.dtype, name=\"noise\")\n",
    "\n",
    "degree=2\n",
    "kernel = pick_kernel(s.kernel_type, amplitude=1, verbose=s.verbose, degree=degree)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Split training data into training and validation sets\n",
    "# Now validation set acts as temporary est set\n",
    "# train_test_split and tensorflow tensors don't get along so I temporarily convert them back to numpy arrays\n",
    "\n",
    "train_indices_j, valid_indices_j  = train_test_split(np.arange(len(train_sps)), random_state = s.valid_split_seed, test_size=(1-s.valid_fract))\n",
    "\n",
    "train_sps_j, valid_sps_j = train_sps[train_indices_j], train_sps[valid_indices_j]\n",
    "train_ens_j, valid_ens_j = train_ens[train_indices_j], train_ens[valid_indices_j]\n",
    "\n",
    "if s.use_forces: \n",
    "    train_dsp_dx_j, valid_dsp_dx_j = train_dsp_dx[train_indices_j], train_dsp_dx[valid_indices_j]\n",
    "    train_frcs_j, valid_frcs_j = train_frcs[train_indices_j], train_frcs[valid_indices_j]\n",
    "    train_nats_j, valid_nats_j = train_nats[train_indices_j], train_nats[valid_indices_j]\n",
    "\n",
    "# Convert to tensorflow constant tensors\n",
    "# train_sps_j = tf.constant(train_sps_j, dtype=s.dtype)\n",
    "# train_ens_j = tf.constant(train_ens_j, dtype=s.dtype)\n",
    "valid_sps_j = tf.constant(valid_sps_j, dtype=s.dtype)\n",
    "# valid_ens_j = tf.constant(valid_ens_j, dtype=s.dtype)\n",
    "\n",
    "if s.sparse_gpflow:\n",
    "    sparse_train_sps = tf.Variable(sparse_train_sps, shape=sparse_train_sps.shape, dtype=s.dtype, trainable=False)\n",
    "\n",
    "if s.use_forces:\n",
    "    train_dsp_dx_j = tf.constant(train_dsp_dx_j, dtype=s.dtype)\n",
    "    train_frcs_j = tf.constant(train_frcs_j, dtype=s.dtype)    \n",
    "    valid_dsp_dx_j = tf.constant(valid_dsp_dx_j, dtype=s.dtype)\n",
    "    valid_frcs_j = tf.constant(valid_frcs_j, dtype=s.dtype)        \n",
    "\n",
    "test_sps = tf.constant(test_sps, dtype=s.dtype)\n",
    "\n",
    "\n",
    "# Batch data if  training set is larger than batch_size_max\n",
    "if len(train_sps_j) < s.batch_size_max:\n",
    "    iterations_per_epoch = 1\n",
    "    batch_size = len(train_sps_j)\n",
    "    if s.verbose:\n",
    "        print(\"Training using {} atoms without batching.\".format(len(train_sps_j)))\n",
    "else:\n",
    "    iterations_per_epoch = int(np.ceil(len(train_sps_j)/s.batch_size_max))\n",
    "    batch_size = int(np.ceil(len(train_sps_j)/iterations_per_epoch))\n",
    "    if s.verbose:\n",
    "        print(\"Training using {} atoms total using {} batches with {} atoms per batch.\".format( len(train_sps_j), iterations_per_epoch, batch_size ))\n",
    "\n",
    "# training(out_data)\n",
    "        \n",
    "TimeBeforeEpoch0 = time.time()\n",
    "\n",
    "\n",
    "mse_history = []    \n",
    "hyperparam_history = []\n",
    "\n",
    "\n",
    "\n",
    "# import warnings\n",
    "# \n",
    "# from gpflow.models import VGP, GPR, SGPR, SVGP\n",
    "# from gpflow.optimizers import NaturalGradient\n",
    "# from gpflow.optimizers.natgrad import XiSqrtMeanVar\n",
    "# from gpflow import set_trainable\n",
    "\n",
    "\n",
    "if s.my_priority == \"efficiency\":\n",
    "    # I don't know what this does\n",
    "    autotune = tf.data.experimental.AUTOTUNE\n",
    "    \n",
    "    \n",
    "    batches = (\n",
    "        tf.data.Dataset.from_tensor_slices((train_sps_j, train_ens_j))\n",
    "        .prefetch(autotune) \n",
    "        .shuffle(buffer_size=len(train_sps_j), seed=s.shuffle_seed)\n",
    "        .repeat(count=None)\n",
    "        .batch(batch_size)\n",
    "    )\n",
    "    \n",
    "    batch_iterator = iter(batches)\n",
    "\n",
    "    # I also don't know why we use this\n",
    "    from gpflow.ci_utils import ci_niter, ci_range\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=s.learn_rate)\n",
    "    \n",
    "else:\n",
    "    batches = (\n",
    "        tf.data.Dataset.from_tensor_slices((train_sps_j, train_ens_j)) \n",
    "        .shuffle(buffer_size=len(train_sps_j), seed=s.shuffle_seed) \n",
    "        .repeat(count=None)\n",
    "        .batch(batch_size)\n",
    "    )    \n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=s.learn_rate)\n",
    "#     optimizer = tf.keras.optimizers.SGD(learning_rate=s.learn_rate)\n",
    "    \n",
    "train_hyperparams_without_forces_tf = tf.function(train_hyperparams_without_forces, autograph=False, jit_compile=False)\n",
    "\n",
    "# new code to make tf.function training work\n",
    "# --------------------------------------------\n",
    "train_sps_j_i = tf.Variable(train_sps[:batch_size], shape=(batch_size, train_sps.shape[-1]), dtype=s.dtype, trainable=False )\n",
    "train_ens_j_i = tf.Variable(train_ens[:batch_size], shape=(batch_size, 1), dtype=s.dtype, trainable=False ) \n",
    "if s.sparse_gpflow:\n",
    "    if sparse_train_sps.shape[0] >= batch_size:\n",
    "        print(\"Warning: Batch size is not greater than sparse soap size.\\nThis may cause errors in the predict_f function which assumes the inducing points to be fewer than the data points.\")\n",
    "    if s.my_priority == \"efficiency\":\n",
    "        gpr_model = gpflow.models.SVGP( kernel=kernel, likelihood=gpflow.likelihoods.Gaussian(),  inducing_variable=sparse_train_sps)\n",
    "        gpr_model.likelihood.variance.assign(obs_noise)\n",
    "        gpflow.set_trainable(gpr_model.q_mu, False)\n",
    "        gpflow.set_trainable(gpr_model.q_sqrt, False)\n",
    "    else:\n",
    "        gpr_model = gpflow.models.SGPR(data=(train_sps_j_i, train_ens_j_i), kernel=kernel, noise_variance=obs_noise, inducing_variable=sparse_train_sps)\n",
    "else:\n",
    "    if s.my_priority == \"efficiency\":\n",
    "        # it seems I cannot use  noise_variance=obs_noise for this which makes it not GAP...\n",
    "        gpr_model = gpflow.models.VGP( data=(train_sps_j_i, train_ens_j_i), kernel=kernel, likelihood=gpflow.likelihoods.Gaussian())\n",
    "    else:\n",
    "        gpr_model = gpflow.models.GPR( data=(train_sps_j_i, train_ens_j_i), kernel=kernel, noise_variance=obs_noise)\n",
    "# --------------------------------------------\n",
    "\n",
    "\n",
    "print_frequency = max(s.min_print_frequency, int(s.n_epochs/10))\n",
    "\n",
    "if s.my_priority == \"efficiency\":\n",
    "    hyperparam_history.append([(0, np.exp(var.numpy() )) for var in gpr_model.trainable_variables])  \n",
    "    gpr_objective = gpr_model.training_loss_closure(batch_iterator,  compile=True)\n",
    "    for j in range(ci_niter(s.n_epochs)):\n",
    "        if not j % print_frequency:\n",
    "            print(\"Epoch {}\".format(j))\n",
    "        optimizer.minimize(gpr_objective, var_list=gpr_model.trainable_variables)\n",
    "        mse_history.append((j+1, gpr_model.elbo(data=(train_sps, train_ens))))\n",
    "        hyperparam_history.append([(j+1, np.exp(var.numpy()) ) for var in gpr_model.trainable_variables]) \n",
    "    #optimizer.minimize(gpr_model.training_loss, gpr_model.trainable_variables, options=dict(maxiter=s.n_epochs))\n",
    "        \n",
    "elif s.my_priority == \"consistency\":\n",
    "\n",
    "    hyperparam_history.append([(0, var.numpy()) for var in gpr_model.trainable_parameters])  \n",
    "    for j in range(s.n_epochs):\n",
    "        if not j % print_frequency:\n",
    "            print(\"Epoch {}\".format(j))\n",
    "            #print(\" \".join([\"{} = {:.2e} \".format(var.name, np.exp(var.numpy())) for var in trainable_variables]))\n",
    "\n",
    "        mse_ens_j = 0\n",
    "        for i, (train_sps_j_i, train_ens_j_i) in enumerate(islice(batches, iterations_per_epoch)):\n",
    "            if not s.use_forces: #and not s.sparse_gpflow :\n",
    "                gpr_model.data[0].assign(train_sps_j_i)\n",
    "                gpr_model.data[1].assign(train_ens_j_i)        \n",
    "                mse_ens_j_i = train_hyperparams_without_forces_tf(gpr_model, valid_sps_j, valid_ens_j, optimizer)\n",
    "                print(\"valid_ens[:3] = {}\".format( valid_ens_j[:3].flatten()) )\n",
    "#                 print(mse_ens_j_i.numpy(), valid_ens_j[:3].numpy().flatten(), train_ens_j_i[:3].numpy().flatten()  )\n",
    "\n",
    "\n",
    "            else:\n",
    "                print(\"Using older approach (not converted to tf.function yet)\")\n",
    "                with tf.GradientTape() as tape:\n",
    "                    with tf.GradientTape(watch_accessed_variables=False) as tape_sps:\n",
    "                        tape_sps.watch(valid_sps_j)\n",
    "                        if s.sparse_gpflow:\n",
    "                            gpr_model = gpflow.models.SGPR(data=(train_sps_j_i, train_ens_j_i), kernel=kernel, inducing_variable=sparse_train_sps)\n",
    "    #                         gpflow.set_trainable(gpr_model.inducing_variable, False)\n",
    "                            if i < 3:\n",
    "                                print_summary(gpr_model)            \n",
    "                        else:\n",
    "                            gpr_model.data[0].assign(train_sps_j_i)\n",
    "                            gpr_model.data[1].assign(train_ens_j_i)\n",
    "                            #gpr_model = gpflow.models.GPR(data=(train_sps_j_i, train_ens_j_i), kernel=kernel)\n",
    "                        #gpr_model.likelihood.variance.assign(obs_noise)                \n",
    "                        predict_ens_j_i = gpr_model.predict_f(valid_sps_j)[0]\n",
    "\n",
    "        #                 gpr_model = gpflow.models.GPR(data=(sps_j_i, train_ens_j_i), kernel=kernel_gpf)\n",
    "        #                 gpr_model.likelihood.variance.assign(obs_noise_gpf)\n",
    "        #                 predict_ens_j_i_gpf = gpr_model.predict_f(valid_sps_j)\n",
    "\n",
    "                    if s.use_forces:\n",
    "                        predict_d_ens_j_i = tape_sps.gradient(predict_ens_j_i, valid_sps_j)\n",
    "                        # In the following line I needed to include '* n_atoms' after breaking energies into local energies\n",
    "                        # The reason is that I am effectively breaking the connection between E and F when breaking energies into local energies\n",
    "                        # F = -dE/dx =/= -dE_local/dx where E_local = E/n_atoms - E_free\n",
    "                        # When I split energies into local energies I initially calculated -dE_local/dx which is -dE/dx / n_atoms\n",
    "                        # We can't just rescale the validation_frcs beforehand, by dividing them by n_atoms, because this \n",
    "                        # rescales their weight in the mse by n_atoms which\n",
    "                        # would lead to forces from smaller structures being overweighted in importances to mse\n",
    "#                         print(valid_dsp_dx_j.shape, predict_d_ens_j_i.shape, valid_nats_j.shape)\n",
    "#                         print(valid_nats_j)\n",
    "                        predict_frcs_j_i = -1*np.einsum('ijk,ik->ij', valid_dsp_dx_j, predict_d_ens_j_i)  * valid_nats_j\n",
    "                        mse_j_i = mse_2factor_tf(predict_ens_j_i, valid_ens_j, 1/ens_var,\n",
    "                                                predict_frcs_j_i, valid_frcs_j, 1/frcs_var)\n",
    "                        mse_ens_j_i = mse_tf(predict_ens_j_i, valid_ens_j)\n",
    "                    else:\n",
    "                        mse_j_i = mse_tf(predict_ens_j_i, valid_ens_j)\n",
    "                        mse_ens_j_i = mse_j_i\n",
    "\n",
    "\n",
    "        #         grads = tape.gradient(mse_j_i, trainable_variables)\n",
    "        #         optimizer.apply_gradients(zip(grads, trainable_variables))\n",
    "                grads = tape.gradient(mse_j_i, gpr_model.trainable_variables)\n",
    "                # print(gpr_model.trainable_variables[0])#grads[0])\n",
    "                optimizer.apply_gradients(zip(grads, gpr_model.trainable_variables))\n",
    "                if i < 3:\n",
    "                    print_summary(gpr_model)\n",
    "\n",
    "                if not gpr_model.data[0][0,0].numpy() == train_sps_j_i[0,0].numpy() :\n",
    "                    print(\"ERRORERRORERRORERRORERRORERRORERROR\")\n",
    "\n",
    "            print(\"Adding mse_ens_j_i to mse_ens_j: {} + {} = {} \".format(mse_ens_j_i.numpy(), mse_ens_j , mse_ens_j_i.numpy() + mse_ens_j  ))\n",
    "            mse_ens_j += mse_ens_j_i\n",
    "\n",
    "        mse_ens_j /= iterations_per_epoch\n",
    "        print(\"Epoch {},  mse = {}\".format(j, mse_ens_j))\n",
    "        mse_history.append((j+1, mse_ens_j))\n",
    "        hyperparam_history.append([(j+1, var.numpy()) for var in gpr_model.trainable_parameters])    \n",
    "else:\n",
    "    print(\"{} is not a reconized value for my_priority.\\n Training did not occur.\".format(s.my_priority))\n",
    "\n",
    "\n",
    "\n",
    "TimeBeforeWeights = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am currently (11/30) converting the hyperparameter learning in this cell into a function\n",
    "# Next will be the post-hyperparameter part of the learning (in the next cell)\n",
    "\n",
    "# # Initialize kernels and model hyperparameters\n",
    "\n",
    "\n",
    "\n",
    "# def train_hyperparams(train_sps, train_ens, sparse_train_sps, kernel, settings):\n",
    "#     tf.random.set_seed(settings.tf_seed)\n",
    "\n",
    "\n",
    "#     noise_init = 1e-4 #.001# 0.0005499093576274776 #1.625e-4\n",
    "#     obs_noise = tf.Variable(noise_init, dtype=settings.dtype, name=\"noise\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     # Split training data into training and validation sets\n",
    "#     # Now validation set acts as temporary est set\n",
    "#     # train_test_split and tensorflow tensors don't get along so I temporarily convert them back to numpy arrays\n",
    "\n",
    "#     train_indices_j, valid_indices_j  = train_test_split(np.arange(len(train_sps)), random_state = settings.valid_split_seed, test_size=(1-settings.valid_fract))\n",
    "\n",
    "#     train_sps_j, valid_sps_j = train_sps[train_indices_j], train_sps[valid_indices_j]\n",
    "#     train_ens_j, valid_ens_j = train_ens[train_indices_j], train_ens[valid_indices_j]\n",
    "\n",
    "#     if settings.use_forces: \n",
    "#         train_dsp_dx_j, valid_dsp_dx_j = train_dsp_dx[train_indices_j], train_dsp_dx[valid_indices_j]\n",
    "#         train_frcs_j, valid_frcs_j = train_frcs[train_indices_j], train_frcs[valid_indices_j]\n",
    "\n",
    "#     # Convert to tensorflow constant tensors\n",
    "#     train_sps_j = tf.constant(train_sps_j, dtype=settings.dtype)\n",
    "#     train_ens_j = tf.constant(train_ens_j, dtype=settings.dtype)\n",
    "#     valid_sps_j = tf.constant(valid_sps_j, dtype=settings.dtype)\n",
    "#     valid_ens_j = tf.constant(valid_ens_j, dtype=settings.dtype)\n",
    "#     if settings.sparse_gpflow:\n",
    "#         sparse_train_sps = tf.Variable(sparse_train_sps, shape=sparse_train_spsettings.shape, dtype=settings.dtype, trainable=False)\n",
    "\n",
    "#     if settings.use_forces:\n",
    "#         train_dsp_dx_j = tf.constant(train_dsp_dx_j, dtype=settings.dtype)\n",
    "#         train_frcs_j = tf.constant(train_frcs_j, dtype=settings.dtype)    \n",
    "#         valid_dsp_dx_j = tf.constant(valid_dsp_dx_j, dtype=settings.dtype)\n",
    "#         valid_frcs_j = tf.constant(valid_frcs_j, dtype=settings.dtype)        \n",
    "\n",
    "#     test_sps = tf.constant(test_sps, dtype=settings.dtype)\n",
    "\n",
    "\n",
    "#     # Batch data if  training set is larger than batch_size_max\n",
    "#     if len(train_sps_j) < settings.batch_size_max:\n",
    "#         iterations_per_epoch = 1\n",
    "#         batch_size = len(train_sps_j)\n",
    "#         if s.verbose:\n",
    "#             print(\"Training using {} atoms without batching.\".format(len(train_sps_j)))\n",
    "#     else:\n",
    "#         iterations_per_epoch = int(np.ceil(len(train_sps_j)/settings.batch_size_max))\n",
    "#         batch_size = int(np.ceil(len(train_sps_j)/iterations_per_epoch))\n",
    "#         if s.verbose:\n",
    "#             print(\"Training using {} atoms total using {} batches with {} atoms per batch.\".format( len(train_sps_j), iterations_per_epoch, batch_size ))\n",
    "\n",
    "#     # training(out_data)\n",
    "\n",
    "#     TimeBeforeEpoch0 = time.time()\n",
    "\n",
    "\n",
    "#     mse_history = []    \n",
    "#     hyperparam_history = []\n",
    "\n",
    "\n",
    "\n",
    "#     # import warnings\n",
    "#     # \n",
    "#     # from gpflow.models import VGP, GPR, SGPR, SVGP\n",
    "#     # from gpflow.optimizers import NaturalGradient\n",
    "#     # from gpflow.optimizers.natgrad import XiSqrtMeanVar\n",
    "#     # from gpflow import set_trainable\n",
    "\n",
    "\n",
    "#     if settings.my_priority == \"efficiency\":\n",
    "#         # I don't know what this does\n",
    "#         autotune = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "\n",
    "#         batches = (\n",
    "#             tf.data.Dataset.from_tensor_slices((train_sps_j, train_ens_j))\n",
    "#             .prefetch(autotune) \n",
    "#             .shuffle(buffer_size=len(train_sps_j), seed=settings.shuffle_seed)\n",
    "#             .repeat(count=None)\n",
    "#             .batch(batch_size)\n",
    "#         )\n",
    "\n",
    "#         batch_iterator = iter(batches)\n",
    "\n",
    "#         # I also don't know why we use this\n",
    "#         from gpflow.ci_utils import ci_niter, ci_range\n",
    "\n",
    "#         optimizer = tf.keras.optimizers.Adam(learning_rate=settings.learn_rate)\n",
    "\n",
    "#     else:\n",
    "#         batches = (\n",
    "#             tf.data.Dataset.from_tensor_slices((train_sps_j, train_ens_j)) \n",
    "#             .shuffle(buffer_size=len(train_sps_j), seed=settings.shuffle_seed) \n",
    "#             .repeat(count=None)\n",
    "#             .batch(batch_size)\n",
    "#         )    \n",
    "\n",
    "#         optimizer = tf.keras.optimizers.Adam(learning_rate=settings.learn_rate)\n",
    "#         #optimizer = tf.keras.optimizers.SGD(learning_rate=settings.learn_rate)\n",
    "\n",
    "\n",
    "\n",
    "#     # new code to make tf.function training work\n",
    "#     # --------------------------------------------\n",
    "#     train_sps_j_i = tf.Variable(train_sps[:batch_size], shape=(batch_size, train_sps.shape[-1]), dtype=settings.dtype, trainable=False )\n",
    "#     train_ens_j_i = tf.Variable(train_ens[:batch_size], shape=(batch_size, 1), dtype=settings.dtype, trainable=False ) \n",
    "#     if settings.sparse_gpflow:\n",
    "#         if sparse_train_sps.shape[0] >= batch_size:\n",
    "#             print(\"Warning: Batch size is not greater than sparse soap size.\\nThis may cause errors in the predict_f function which assumes the inducing points to be fewer than the data points.\")\n",
    "#         if settings.my_priority == \"efficiency\":\n",
    "#             gpr_model = gpflow.models.SVGP( kernel=kernel, likelihood=gpflow.likelihoods.Gaussian(),  inducing_variable=sparse_train_sps)\n",
    "#             gpr_model.likelihood.variance.assign(obs_noise)\n",
    "#             gpflow.set_trainable(gpr_model.q_mu, False)\n",
    "#             gpflow.set_trainable(gpr_model.q_sqrt, False)\n",
    "#         else:\n",
    "#             gpr_model = gpflow.models.SGPR(data=(train_sps_j_i, train_ens_j_i), kernel=kernel, noise_variance=obs_noise, inducing_variable=sparse_train_sps)\n",
    "#     else:\n",
    "#         if settings.my_priority == \"efficiency\":\n",
    "#             # it seems I cannot use  noise_variance=obs_noise for this which makes it not GAP...\n",
    "#             gpr_model = gpflow.models.VGP( data=(train_sps_j_i, train_ens_j_i), kernel=kernel, likelihood=gpflow.likelihoods.Gaussian())\n",
    "#         else:\n",
    "#             gpr_model = gpflow.models.GPR( data=(train_sps_j_i, train_ens_j_i), kernel=kernel, noise_variance=obs_noise)\n",
    "#     # --------------------------------------------\n",
    "\n",
    "\n",
    "#     print_frequency = max(settings.min_print_frequency, int(settings.n_epochs/10))\n",
    "\n",
    "#     if settings.my_priority == \"efficiency\":\n",
    "#         hyperparam_history.append([(0, np.exp(var.numpy() )) for var in gpr_model.trainable_variables])  \n",
    "#         gpr_objective = gpr_model.training_loss_closure(batch_iterator,  compile=True)\n",
    "#         for j in range(ci_niter(settings.n_epochs)):\n",
    "#             if not j % print_frequency:\n",
    "#                 print(\"Epoch {}\".format(j))\n",
    "#             optimizer.minimize(gpr_objective, var_list=gpr_model.trainable_variables)\n",
    "#             mse_history.append((j+1, gpr_model.elbo(data=(train_sps, train_ens))))\n",
    "#             hyperparam_history.append([(j+1, np.exp(var.numpy()) ) for var in gpr_model.trainable_variables]) \n",
    "#         #optimizer.minimize(gpr_model.training_loss, gpr_model.trainable_variables, options=dict(maxiter=settings.n_epochs))\n",
    "\n",
    "#     elif settings.my_priority == \"consistency\":\n",
    "\n",
    "#         hyperparam_history.append([(0, var.numpy()) for var in gpr_model.trainable_parameters])  \n",
    "#         for j in range(settings.n_epochs):\n",
    "#             if not j % print_frequency:\n",
    "#                 print(\"Epoch {}\".format(j))\n",
    "#                 #print(\" \".join([\"{} = {:.2e} \".format(var.name, np.exp(var.numpy())) for var in trainable_variables]))\n",
    "\n",
    "#             mse_ens_j = 0\n",
    "#             for i, (train_sps_j_i, train_ens_j_i) in enumerate(islice(batches, iterations_per_epoch)):\n",
    "#                 if not settings.use_forces: #and not settings.sparse_gpflow :\n",
    "#                     gpr_model.data[0].assign(train_sps_j_i)\n",
    "#                     gpr_model.data[1].assign(train_ens_j_i)        \n",
    "#                     mse_ens_j_i = train_hyperparams_without_forces_tf(gpr_model, valid_sps_j, valid_ens_j)\n",
    "#                     print(\"valid_ens[:3] = {}\".format( valid_ens_j[:3].numpy().flatten()) )\n",
    "#     #                 print(mse_ens_j_i.numpy(), valid_ens_j[:3].numpy().flatten(), train_ens_j_i[:3].numpy().flatten()  )\n",
    "\n",
    "\n",
    "#                 else:\n",
    "#                     print(\"Using older approach (not converted to tf.function yet)\")\n",
    "#                     with tf.GradientTape() as tape:\n",
    "#                         with tf.GradientTape(watch_accessed_variables=False) as tape_sps:\n",
    "#                             tape_sps.watch(valid_sps_j)\n",
    "#                             if settings.sparse_gpflow:\n",
    "#                                 gpr_model = gpflow.models.SGPR(data=(train_sps_j_i, train_ens_j_i), kernel=kernel, inducing_variable=sparse_train_sps)\n",
    "#         #                         gpflow.set_trainable(gpr_model.inducing_variable, False)\n",
    "#                                 if i < 3:\n",
    "#                                     print_summary(gpr_model)            \n",
    "#                             else:\n",
    "#                                 gpr_model.data[0].assign(train_sps_j_i)\n",
    "#                                 gpr_model.data[1].assign(train_ens_j_i)\n",
    "#                                 #gpr_model = gpflow.models.GPR(data=(train_sps_j_i, train_ens_j_i), kernel=kernel)\n",
    "#                             #gpr_model.likelihood.variance.assign(obs_noise)                \n",
    "#                             predict_ens_j_i = gpr_model.predict_f(valid_sps_j)[0]\n",
    "\n",
    "#             #                 gpr_model = gpflow.models.GPR(data=(sps_j_i, train_ens_j_i), kernel=kernel_gpf)\n",
    "#             #                 gpr_model.likelihood.variance.assign(obs_noise_gpf)\n",
    "#             #                 predict_ens_j_i_gpf = gpr_model.predict_f(valid_sps_j)\n",
    "\n",
    "#                         if settings.use_forces:\n",
    "#                             predict_d_ens_j_i = tape_sps.gradient(predict_ens_j_i, valid_sps_j)\n",
    "#                             # In the following line I needed to include '* n_atoms' after breaking energies into local energies\n",
    "#                             # The reason is that I am effectively breaking the connection between E and F when doing that\n",
    "#                             # F = -dE/dx =/= -dE_local/dx where E_local = E/n_atoms - E_free\n",
    "#                             # When I split energies into local energies I initially calculated -dE_local/dx which is -dE/dx / n_atoms\n",
    "#                             # This fix is prone to breaking the code and is not robust to systems with different structure size\n",
    "#                             # Need to improve this with a better fix\n",
    "#                             predict_frcs_j_i = -1*np.einsum('ijk,ik->ij', valid_dsp_dx_j, predict_d_ens_j_i) * n_atoms\n",
    "#                             mse_j_i = mse_2factor_tf(predict_ens_j_i, valid_ens_j, 1/ens_var,\n",
    "#                                                     predict_frcs_j_i, valid_frcs_j, 1/frcs_var)\n",
    "#                             mse_ens_j_i = mse_tf(predict_ens_j_i, valid_ens_j)\n",
    "#                         else:\n",
    "#                             mse_j_i = mse_tf(predict_ens_j_i, valid_ens_j)\n",
    "#                             mse_ens_j_i = mse_j_i\n",
    "\n",
    "\n",
    "#             #         grads = tape.gradient(mse_j_i, trainable_variables)\n",
    "#             #         optimizer.apply_gradients(zip(grads, trainable_variables))\n",
    "#                     grads = tape.gradient(mse_j_i, gpr_model.trainable_variables)\n",
    "#                     # print(gpr_model.trainable_variables[0])#grads[0])\n",
    "#                     optimizer.apply_gradients(zip(grads, gpr_model.trainable_variables))\n",
    "#                     if i < 3:\n",
    "#                         print_summary(gpr_model)\n",
    "\n",
    "#                     if not gpr_model.data[0][0,0].numpy() == train_sps_j_i[0,0].numpy() :\n",
    "#                         print(\"ERRORERRORERRORERRORERRORERRORERROR\")\n",
    "\n",
    "#                 print(\"Adding mse_ens_j_i to mse_ens_j: {} + {} = {} \".format(mse_ens_j_i.numpy(), mse_ens_j , mse_ens_j_i.numpy() + mse_ens_j  ))\n",
    "#                 mse_ens_j += mse_ens_j_i\n",
    "\n",
    "#             mse_ens_j /= iterations_per_epoch\n",
    "#             print(\"Epoch {},  mse = {}\".format(j, mse_ens_j))\n",
    "#             mse_history.append((j+1, mse_ens_j))\n",
    "#             hyperparam_history.append([(j+1, var.numpy()) for var in gpr_model.trainable_parameters])    \n",
    "#     else:\n",
    "#         print(\"{} is not a reconized value for my_priority.\\n Training did not occur.\".format(settings.my_priority))\n",
    "    \n",
    "#     return gpr_model\n",
    "\n",
    "\n",
    "\n",
    "# TimeBeforeWeights = time.time()\n",
    "# train_hyperparams(train_sps, train_ens, sparse_train_sps, kernel=kernel, settings=s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating weights\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>name                   </th><th>class    </th><th>transform       </th><th>prior  </th><th>trainable  </th><th>shape  </th><th>dtype  </th><th style=\"text-align: right;\">       value</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>GPR.kernel.variance    </td><td>Parameter</td><td>Softplus        </td><td>       </td><td>True       </td><td>()     </td><td>float64</td><td style=\"text-align: right;\">0.769521    </td></tr>\n",
       "<tr><td>GPR.kernel.degree      </td><td>Parameter</td><td>Identity        </td><td>       </td><td>False      </td><td>()     </td><td>float64</td><td style=\"text-align: right;\">2           </td></tr>\n",
       "<tr><td>GPR.kernel.offset      </td><td>Parameter</td><td>Softplus        </td><td>       </td><td>False      </td><td>()     </td><td>float64</td><td style=\"text-align: right;\">2.22507e-308</td></tr>\n",
       "<tr><td>GPR.likelihood.variance</td><td>Parameter</td><td>Softplus + Shift</td><td>       </td><td>True       </td><td>()     </td><td>float64</td><td style=\"text-align: right;\">0.000147435 </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting final energies\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-10 09:38:08.089005: W tensorflow/core/common_runtime/bfc_allocator.cc:462] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.47GiB (rounded to 1575849984)requested by op Cholesky\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2021-12-10 09:38:08.089064: I tensorflow/core/common_runtime/bfc_allocator.cc:1010] BFCAllocator dump for GPU_0_bfc\n",
      "2021-12-10 09:38:08.089090: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (256): \tTotal Chunks: 65, Chunks in use: 64. 16.2KiB allocated for chunks. 16.0KiB in use in bin. 793B client-requested in use in bin.\n",
      "2021-12-10 09:38:08.089106: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (512): \tTotal Chunks: 1, Chunks in use: 0. 512B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2021-12-10 09:38:08.089122: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (1024): \tTotal Chunks: 2, Chunks in use: 1. 2.5KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.\n",
      "2021-12-10 09:38:08.089136: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (2048): \tTotal Chunks: 1, Chunks in use: 0. 2.5KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2021-12-10 09:38:08.089149: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (4096): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2021-12-10 09:38:08.089166: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (8192): \tTotal Chunks: 5, Chunks in use: 4. 60.5KiB allocated for chunks. 47.8KiB in use in bin. 47.3KiB client-requested in use in bin.\n",
      "2021-12-10 09:38:08.089179: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2021-12-10 09:38:08.089199: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (32768): \tTotal Chunks: 3, Chunks in use: 3. 148.5KiB allocated for chunks. 148.5KiB in use in bin. 148.0KiB client-requested in use in bin.\n",
      "2021-12-10 09:38:08.089217: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (65536): \tTotal Chunks: 4, Chunks in use: 3. 428.0KiB allocated for chunks. 318.2KiB in use in bin. 318.0KiB client-requested in use in bin.\n",
      "2021-12-10 09:38:08.089231: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2021-12-10 09:38:08.089244: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2021-12-10 09:38:08.089258: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (524288): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2021-12-10 09:38:08.089271: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (1048576): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2021-12-10 09:38:08.089284: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (2097152): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2021-12-10 09:38:08.089298: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2021-12-10 09:38:08.089311: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2021-12-10 09:38:08.089324: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2021-12-10 09:38:08.089337: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2021-12-10 09:38:08.089354: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (67108864): \tTotal Chunks: 1, Chunks in use: 1. 117.61MiB allocated for chunks. 117.61MiB in use in bin. 117.61MiB client-requested in use in bin.\n",
      "2021-12-10 09:38:08.089373: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (134217728): \tTotal Chunks: 8, Chunks in use: 7. 1.16GiB allocated for chunks. 1.02GiB in use in bin. 1017.29MiB client-requested in use in bin.\n",
      "2021-12-10 09:38:08.089388: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (268435456): \tTotal Chunks: 7, Chunks in use: 6. 8.18GiB allocated for chunks. 6.78GiB in use in bin. 6.78GiB client-requested in use in bin.\n",
      "2021-12-10 09:38:08.089405: I tensorflow/core/common_runtime/bfc_allocator.cc:1033] Bin for 1.47GiB was 256.00MiB, Chunk State: \n",
      "2021-12-10 09:38:08.089427: I tensorflow/core/common_runtime/bfc_allocator.cc:1039]   Size: 1.40GiB | Requested Size: 8B | in_use: 0 | bin_num: 20, prev:   Size: 1.47GiB | Requested Size: 1.47GiB | in_use: 1 | bin_num: -1\n",
      "2021-12-10 09:38:08.089439: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] Next region of size 10156441600\n",
      "2021-12-10 09:38:08.089453: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1477e2000000 of size 256 next 1\n",
      "2021-12-10 09:38:08.089465: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1477e2000100 of size 1280 next 2\n",
      "2021-12-10 09:38:08.089476: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1477e2000600 of size 256 next 3\n",
      "2021-12-10 09:38:08.089486: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1477e2000700 of size 256 next 4\n",
      "2021-12-10 09:38:08.089497: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1477e2000800 of size 256 next 5\n",
      "2021-12-10 09:38:08.089507: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1477e2000900 of size 256 next 6\n",
      "2021-12-10 09:38:08.089518: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1477e2000a00 of size 256 next 7\n",
      "2021-12-10 09:38:08.089528: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1477e2000b00 of size 256 next 8\n",
      "2021-12-10 09:38:08.089539: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1477e2000c00 of size 256 next 19\n",
      "2021-12-10 09:38:08.089549: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1477e2000d00 of size 256 next 20\n",
      "2021-12-10 09:38:08.089560: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1477e2000e00 of size 256 next 11\n",
      "2021-12-10 09:38:08.089570: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1477e2000f00 of size 256 next 12\n",
      "2021-12-10 09:38:08.089581: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1477e2001000 of size 256 next 18\n",
      "2021-12-10 09:38:08.089591: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1477e2001100 of size 256 next 9\n",
      "2021-12-10 09:38:08.089602: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1477e2001200 of size 256 next 13\n",
      "2021-12-10 09:38:08.089612: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1477e2001300 of size 256 next 10\n",
      "2021-12-10 09:38:08.089626: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1477e2001400 of size 123327488 next 14\n",
      "2021-12-10 09:38:08.089637: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1477e959e800 of size 136152064 next 16\n",
      "2021-12-10 09:38:08.089648: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1477f1776c00 of size 1109507072 next 15\n",
      "2021-12-10 09:38:08.089660: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 147833992800 of size 101120 next 17\n",
      "2021-12-10 09:38:08.089670: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1478339ab300 of size 256 next 21\n",
      "2021-12-10 09:38:08.089681: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1478339ab400 of size 256 next 22\n",
      "2021-12-10 09:38:08.089692: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1478339ab500 of size 12544 next 73\n",
      "2021-12-10 09:38:08.089703: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1478339ae600 of size 12544 next 67\n",
      "2021-12-10 09:38:08.089714: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1478339b1700 of size 12544 next 68\n",
      "2021-12-10 09:38:08.089725: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 1478339b4800 of size 13056 next 26\n",
      "2021-12-10 09:38:08.089735: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1478339b7b00 of size 256 next 61\n",
      "2021-12-10 09:38:08.089746: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1478339b7c00 of size 256 next 52\n",
      "2021-12-10 09:38:08.089757: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1478339b7d00 of size 256 next 69\n",
      "2021-12-10 09:38:08.089767: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1478339b7e00 of size 256 next 78\n",
      "2021-12-10 09:38:08.089778: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1478339b7f00 of size 256 next 79\n",
      "2021-12-10 09:38:08.089788: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1478339b8000 of size 256 next 64\n",
      "2021-12-10 09:38:08.089799: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1478339b8100 of size 256 next 83\n",
      "2021-12-10 09:38:08.089809: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1478339b8200 of size 256 next 74\n",
      "2021-12-10 09:38:08.089820: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1478339b8300 of size 256 next 80\n",
      "2021-12-10 09:38:08.089830: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1478339b8400 of size 256 next 82\n",
      "2021-12-10 09:38:08.089841: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1478339b8500 of size 256 next 81\n",
      "2021-12-10 09:38:08.089851: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 1478339b8600 of size 256 next 93\n",
      "2021-12-10 09:38:08.089862: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1478339b8700 of size 256 next 59\n",
      "2021-12-10 09:38:08.089872: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1478339b8800 of size 256 next 84\n",
      "2021-12-10 09:38:08.089883: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1478339b8900 of size 256 next 100\n",
      "2021-12-10 09:38:08.089894: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 1478339b8a00 of size 1280 next 72\n",
      "2021-12-10 09:38:08.089904: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1478339b8f00 of size 256 next 56\n",
      "2021-12-10 09:38:08.089915: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1478339b9000 of size 256 next 90\n",
      "2021-12-10 09:38:08.089925: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1478339b9100 of size 256 next 94\n",
      "2021-12-10 09:38:08.089935: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1478339b9200 of size 256 next 57\n",
      "2021-12-10 09:38:08.089946: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 1478339b9300 of size 512 next 62\n",
      "2021-12-10 09:38:08.089956: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1478339b9500 of size 256 next 65\n",
      "2021-12-10 09:38:08.089967: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1478339b9600 of size 256 next 66\n",
      "2021-12-10 09:38:08.089977: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1478339b9700 of size 256 next 70\n",
      "2021-12-10 09:38:08.089988: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1478339b9800 of size 256 next 92\n",
      "2021-12-10 09:38:08.089998: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1478339b9900 of size 256 next 71\n",
      "2021-12-10 09:38:08.090009: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1478339b9a00 of size 256 next 75\n",
      "2021-12-10 09:38:08.090019: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1478339b9b00 of size 256 next 54\n",
      "2021-12-10 09:38:08.090030: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1478339b9c00 of size 256 next 99\n",
      "2021-12-10 09:38:08.090040: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 1478339b9d00 of size 2560 next 23\n",
      "2021-12-10 09:38:08.090051: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1478339ba700 of size 256 next 38\n",
      "2021-12-10 09:38:08.090062: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1478339ba800 of size 256 next 39\n",
      "2021-12-10 09:38:08.090072: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1478339ba900 of size 256 next 40\n",
      "2021-12-10 09:38:08.090083: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1478339baa00 of size 256 next 41\n",
      "2021-12-10 09:38:08.090094: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1478339bab00 of size 256 next 42\n",
      "2021-12-10 09:38:08.090104: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1478339bac00 of size 256 next 43\n",
      "2021-12-10 09:38:08.090115: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1478339bad00 of size 256 next 44\n",
      "2021-12-10 09:38:08.090126: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1478339bae00 of size 11264 next 45\n",
      "2021-12-10 09:38:08.090136: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1478339bda00 of size 256 next 46\n",
      "2021-12-10 09:38:08.090147: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1478339bdb00 of size 50688 next 47\n",
      "2021-12-10 09:38:08.090158: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1478339ca100 of size 256 next 48\n",
      "2021-12-10 09:38:08.090168: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1478339ca200 of size 256 next 49\n",
      "2021-12-10 09:38:08.090179: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1478339ca300 of size 50688 next 50\n",
      "2021-12-10 09:38:08.090189: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1478339d6900 of size 256 next 51\n",
      "2021-12-10 09:38:08.090200: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1478339d6a00 of size 50688 next 98\n",
      "2021-12-10 09:38:08.090211: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1478339e3000 of size 112384 next 77\n",
      "2021-12-10 09:38:08.090221: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 1478339fe700 of size 112384 next 28\n",
      "2021-12-10 09:38:08.090232: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 147833a19e00 of size 112384 next 33\n",
      "2021-12-10 09:38:08.090243: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 147833a35500 of size 136152064 next 76\n",
      "2021-12-10 09:38:08.090253: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 14783bc0d900 of size 136152064 next 63\n",
      "2021-12-10 09:38:08.090264: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 147843de5d00 of size 136152064 next 60\n",
      "2021-12-10 09:38:08.090275: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 14784bfbe100 of size 145826816 next 24\n",
      "2021-12-10 09:38:08.090285: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 147854ad0500 of size 256 next 25\n",
      "2021-12-10 09:38:08.090296: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 147854ad0600 of size 256 next 34\n",
      "2021-12-10 09:38:08.090306: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 147854ad0700 of size 256 next 27\n",
      "2021-12-10 09:38:08.090317: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 147854ad0800 of size 256 next 86\n",
      "2021-12-10 09:38:08.090327: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 147854ad0900 of size 256 next 29\n",
      "2021-12-10 09:38:08.090338: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 147854ad0a00 of size 256 next 31\n",
      "2021-12-10 09:38:08.090348: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 147854ad0b00 of size 256 next 32\n",
      "2021-12-10 09:38:08.090359: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 147854ad0c00 of size 256 next 36\n",
      "2021-12-10 09:38:08.090369: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 147854ad0d00 of size 256 next 37\n",
      "2021-12-10 09:38:08.090380: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 147854ad0e00 of size 256 next 30\n",
      "2021-12-10 09:38:08.090393: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 147854ad0f00 of size 174034176 next 55\n",
      "2021-12-10 09:38:08.090407: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 14785f0c9c00 of size 174034176 next 97\n",
      "2021-12-10 09:38:08.090427: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1478696c2900 of size 206729472 next 35\n",
      "2021-12-10 09:38:08.090447: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 147875be9a00 of size 554797568 next 95\n",
      "2021-12-10 09:38:08.090467: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 147896d02400 of size 1232834560 next 85\n",
      "2021-12-10 09:38:08.090486: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1478e04bb400 of size 1575849984 next 96\n",
      "2021-12-10 09:38:08.090498: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 14793e394400 of size 1232834560 next 53\n",
      "2021-12-10 09:38:08.090509: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 147987b4d400 of size 1575849984 next 91\n",
      "2021-12-10 09:38:08.090520: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 1479e5a26400 of size 1505532928 next 18446744073709551615\n",
      "2021-12-10 09:38:08.090531: I tensorflow/core/common_runtime/bfc_allocator.cc:1071]      Summary of in-use Chunks by size: \n",
      "2021-12-10 09:38:08.090547: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 64 Chunks of size 256 totalling 16.0KiB\n",
      "2021-12-10 09:38:08.090561: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 1280 totalling 1.2KiB\n",
      "2021-12-10 09:38:08.090573: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 11264 totalling 11.0KiB\n",
      "2021-12-10 09:38:08.090586: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 3 Chunks of size 12544 totalling 36.8KiB\n",
      "2021-12-10 09:38:08.090599: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 3 Chunks of size 50688 totalling 148.5KiB\n",
      "2021-12-10 09:38:08.090612: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 101120 totalling 98.8KiB\n",
      "2021-12-10 09:38:08.090625: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 112384 totalling 219.5KiB\n",
      "2021-12-10 09:38:08.090638: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 123327488 totalling 117.61MiB\n",
      "2021-12-10 09:38:08.090650: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 4 Chunks of size 136152064 totalling 519.38MiB\n",
      "2021-12-10 09:38:08.090663: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 174034176 totalling 331.94MiB\n",
      "2021-12-10 09:38:08.090676: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 206729472 totalling 197.15MiB\n",
      "2021-12-10 09:38:08.090689: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 554797568 totalling 529.10MiB\n",
      "2021-12-10 09:38:08.090701: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 1109507072 totalling 1.03GiB\n",
      "2021-12-10 09:38:08.090713: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 1232834560 totalling 2.30GiB\n",
      "2021-12-10 09:38:08.090725: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 1575849984 totalling 2.93GiB\n",
      "2021-12-10 09:38:08.090737: I tensorflow/core/common_runtime/bfc_allocator.cc:1078] Sum Total of in-use chunks: 7.92GiB\n",
      "2021-12-10 09:38:08.090750: I tensorflow/core/common_runtime/bfc_allocator.cc:1080] total_region_allocated_bytes_: 10156441600 memory_limit_: 10156441600 available bytes: 0 curr_region_allocation_bytes_: 20312883200\n",
      "2021-12-10 09:38:08.090769: I tensorflow/core/common_runtime/bfc_allocator.cc:1086] Stats: \n",
      "Limit:                     10156441600\n",
      "InUse:                      8504951808\n",
      "MaxInUse:                   8505288960\n",
      "NumAllocs:                        1846\n",
      "MaxAllocSize:               1575849984\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2021-12-10 09:38:08.090798: W tensorflow/core/common_runtime/bfc_allocator.cc:474] **************************************************************************************______________\n",
      "2021-12-10 09:38:08.090874: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at cholesky_op_gpu.cu.cc:123 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[14035,14035] and type double on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[14035,14035] and type double on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Cholesky]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_21662/2769888070.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Predicting final energies\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_calculation\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"predict_f\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mpredict_ens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_ens_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgpr_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_sps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_gpflow\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/minigap/lib/python3.9/site-packages/gpflow/models/gpr.py\u001b[0m in \u001b[0;36mpredict_f\u001b[0;34m(self, Xnew, full_cov, full_output_cov)\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposterior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXnew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \"\"\"\n\u001b[0;32m--> 163\u001b[0;31m         return self.posterior(posteriors.PrecomputeCacheType.NOCACHE).fused_predict_f(\n\u001b[0m\u001b[1;32m    164\u001b[0m             \u001b[0mXnew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_cov\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfull_cov\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_output_cov\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfull_output_cov\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         )\n",
      "\u001b[0;32m~/anaconda3/envs/minigap/lib/python3.9/site-packages/gpflow/posteriors.py\u001b[0m in \u001b[0;36mfused_predict_f\u001b[0;34m(self, Xnew, full_cov, full_output_cov)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mDoes\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmake\u001b[0m \u001b[0muse\u001b[0m \u001b[0mof\u001b[0m \u001b[0mcaching\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \"\"\"\n\u001b[0;32m--> 144\u001b[0;31m         mean, cov = self._conditional_fused(\n\u001b[0m\u001b[1;32m    145\u001b[0m             \u001b[0mXnew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_cov\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfull_cov\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_output_cov\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfull_output_cov\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         )\n",
      "\u001b[0;32m~/anaconda3/envs/minigap/lib/python3.9/site-packages/gpflow/posteriors.py\u001b[0m in \u001b[0;36m_conditional_fused\u001b[0;34m(self, Xnew, full_cov, full_output_cov)\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0mKmm_plus_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_noise_cov\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKmm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlikelihood_variance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m         return base_conditional(\n\u001b[0m\u001b[1;32m    290\u001b[0m             \u001b[0mKmn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKmm_plus_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_cov\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfull_cov\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         )  # [N, P], [N, P] or [P, N, N]\n",
      "\u001b[0;32m~/anaconda3/envs/minigap/lib/python3.9/site-packages/gpflow/conditionals/util.py\u001b[0m in \u001b[0;36mbase_conditional\u001b[0;34m(Kmn, Kmm, Knn, f, full_cov, q_sqrt, white)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;32mor\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \"\"\"\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mLm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcholesky\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKmm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     return base_conditional_with_lm(\n\u001b[1;32m     58\u001b[0m         \u001b[0mKmn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mKmn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKnn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mKnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_cov\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfull_cov\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_sqrt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mq_sqrt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwhite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/minigap/lib/python3.9/site-packages/tensorflow/python/ops/gen_linalg_ops.py\u001b[0m in \u001b[0;36mcholesky\u001b[0;34m(input, name)\u001b[0m\n\u001b[1;32m    774\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 776\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    777\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/minigap/lib/python3.9/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7105\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7106\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7107\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[14035,14035] and type double on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Cholesky]"
     ]
    }
   ],
   "source": [
    "\n",
    "TimeBeforeWeights = time.time()\n",
    "print(\"Calculating weights\")\n",
    "\n",
    "if s.my_priority == \"efficiency\" and s.sparse_gpflow == True:\n",
    "    gpr_model =gpr_model\n",
    "elif s.my_priority == \"consistency\":\n",
    "    if s.sparse_gpflow:\n",
    "        gpr_model = gpflow.models.SGPR(data=(train_sps, train_ens), kernel=kernel, noise_variance = gpr_model.likelihood.variance, inducing_variable  = sparse_train_sps)\n",
    "    else:\n",
    "        gpr_model = gpflow.models.GPR( data=(train_sps, train_ens), kernel=kernel, noise_variance = gpr_model.likelihood.variance)      \n",
    "\n",
    "print_summary(gpr_model)\n",
    "\n",
    "if s.sparse_gpflow:\n",
    "    if s.prediction_calculation in (\"direct\", \"cholesky\"):\n",
    "        print(\"Alert: {} prediction approach not implemented for sparse model. Using alpha approach instead.\".format(s.prediction_calculation))\n",
    "        trained_weights = gpr_model.posterior().alpha\n",
    "    elif s.prediction_calculation == \"alpha\":\n",
    "        print(\"Attempting to calculate trained weights using alpha method for sparse gpr model.\")\n",
    "        trained_weights = gpr_model.posterior().alpha\n",
    "        print(\"Successfully calculated trained weights using alpha method for sparse gpr model.\")\n",
    "else:\n",
    "    if s.prediction_calculation in (\"direct\", \"cholesky\"):\n",
    "        KNN = gpr_model.kernel(train_sps)\n",
    "        KNN_diag = tf.linalg.diag_part(KNN)\n",
    "        variance_diag = tf.fill(tf.shape(KNN_diag), gpr_model.likelihood.variance)\n",
    "        KNN_plus_variance = tf.linalg.set_diag(KNN, KNN_diag + variance_diag)\n",
    "        if s.prediction_calculation == \"direct\":\n",
    "            KNN_inv =  tf.linalg.inv(KNN_plus_variance)\n",
    "            trained_weights = tf.matmul(KNN_inv, train_ens)\n",
    "        else:\n",
    "            LNN = tf.linalg.cholesky(KNN_plus_variance)\n",
    "            LNN_inv = tf.linalg.inv(LNN)\n",
    "            KNN_inv_from_L = tf.matmul(LNN_inv, LNN_inv,transpose_a=True)\n",
    "            trained_weights = tf.matmul(KNN_inv_from_L, train_ens)\n",
    "    elif s.prediction_calculation == \"alpha\":\n",
    "        print(\"ERROR: alpha not implemented for gpflow GPR. Skipping prediction\")\n",
    "    \n",
    "TimeAfterTraining = time.time()\n",
    "\n",
    "\n",
    "with tf.GradientTape(watch_accessed_variables=False) as tape_sps:\n",
    "    tape_sps.watch(test_sps)  \n",
    "    print(\"Predicting final energies\")\n",
    "    if s.prediction_calculation == \"predict_f\":\n",
    "        predict_ens, predict_ens_var = gpr_model.predict_f(test_sps)\n",
    "    else:\n",
    "        if s.sparse_gpflow:\n",
    "            predict_ens = tf.reshape( predict_energies_from_weights_tf(trained_weights, sparse_train_sps, test_sps, degree), [-1,1])\n",
    "        else:\n",
    "            predict_ens = tf.reshape( predict_energies_from_weights_tf(trained_weights,        train_sps, test_sps, degree), [-1,1])\n",
    "    \n",
    "if s.use_forces:\n",
    "    print(\"Predicting final forces\")    \n",
    "    predict_d_ens = tape_sps.gradient(predict_ens, test_sps)\n",
    "    predict_frcs = -1*np.einsum('ijk,ik->ij', test_dsp_dx, predict_d_ens) * test_nats\n",
    "\n",
    "TimeAfterPrediction = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescale\n",
    "test_ens_rescaled = ens_scaler.inverse_transform(test_ens)\n",
    "predict_ens_rescaled = ens_scaler.inverse_transform(predict_ens)\n",
    "if s.prediction_calculation == \"predict_f\":\n",
    "    predict_ens_var_rescaled =  np.array(predict_ens_var * ens_scaler.scale_ **2)\n",
    "    \n",
    "if s.use_forces:\n",
    "    test_frcs_rescaled = test_frcs * ens_scaler.scale_\n",
    "    predict_frcs_rescaled = predict_frcs * ens_scaler.scale_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainingCellNonEpochsTraining = TimeBeforeEpoch0 - TimeBeforePreEpoch + TimeAfterTraining - TimeBeforeWeights \n",
    "if s.n_epochs:\n",
    "    TimePerEpoch = (TimeBeforeWeights - TimeBeforeEpoch0)/s.n_epochs\n",
    "else:\n",
    "    TimePerEpoch = \"N/A\"\n",
    "PredictionTime = TimeAfterPrediction - TimeAfterTraining\n",
    "\n",
    "if s.print_timings:\n",
    "    print(\"{:50s}: {:.3f}\".format(\"Training time outside of epochs in training cell\", TrainingCellNonEpochsTraining))\n",
    "    if s.n_epochs:\n",
    "        print(\"{:50s}: {:.3f}\".format( \"Training time per epoch\", TimePerEpoch))\n",
    "    print(\"{:50s}: {:.3f}\".format(\"Prediction time\", PredictionTime) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    if 'mse_history_by_n' not in locals():\n",
    "        mse_history_by_n = {}\n",
    "    if 'hyperparam_history_by_n' not in locals():\n",
    "        hyperparam_history_by_n = {}\n",
    "\n",
    "    hyperparam_history_by_n[s.n_structs] = hyperparam_history\n",
    "    mse_history_by_n[s.n_structs] = mse_history\n",
    "\n",
    "    print(\"Stored the hyperparameters and mse values for plotting under n={}\".format(s.n_structs) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hyperparam_training = (in_notebook or s.n_epochs > 0)\n",
    "\n",
    "if plot_hyperparam_training:\n",
    "\n",
    "    palette = plt.get_cmap('gist_ncar')#'nipy_spectral')#\"viridis\")\n",
    "    palette_size = palette.N#len(palette.colors)\n",
    "    palette_itr = 0\n",
    "\n",
    "    fig, [[ax00, ax01], [ax10, ax11]] = plt.subplots(nrows=2, ncols = 2, figsize=(16,12))\n",
    "\n",
    "    hyperparam_names = [\"kernel offset\", \"kernel amplitude\", \"observation noise variance\"]\n",
    "\n",
    "    for n in np.sort(list(mse_history_by_n.keys())):\n",
    "        mse_history = mse_history_by_n[n]\n",
    "        hyperparam_history = hyperparam_history_by_n[n]\n",
    "        if not len(hyperparam_history):\n",
    "            continue\n",
    "        color = palette(palette_itr)\n",
    "        palette_itr = (palette_itr + 30) % palette_size\n",
    "        print(\"The title axes are not assigned correctly. Currently fixing.\")\n",
    "\n",
    "\n",
    "        # hyperparameters on axes 00, 01, 10\n",
    "        hyperparams = np.swapaxes(hyperparam_history, 0, 1)\n",
    "\n",
    "        ax00.plot(*zip(*hyperparams[0]), color=color, label=\"{}\".format(n), lw=3)\n",
    "        ax00.plot(*hyperparams[0][-1], \"o\", color=color)\n",
    "        label00 = hyperparam_names[0]\n",
    "        ax00.set_ylabel(\"{}\".format(label00))\n",
    "    #     #ax00.set_yscale('log')\n",
    "    #     annotation00 = ax00.annotate('{:.1f}'.format(amplitudes[-1][1]) , xy=amplitudes[-1], xycoords='data', xytext=(-30,100),\n",
    "    #                                  textcoords='offset points', bbox={'fc':\"1\"}, arrowprops={'fc':'k'}, zorder=2)\n",
    "        ax00.legend()\n",
    "        ax00.ticklabel_format(useOffset=False)\n",
    "\n",
    "        ax01.plot(*zip(*hyperparams[1]), color=color, label=\"{}\".format(n), lw=3)\n",
    "        ax01.plot(*hyperparams[1][-1], \"o\", color=color)\n",
    "        label01 = hyperparam_names[1]\n",
    "        ax01.set_ylabel(\"{}\".format(label01))\n",
    "    #     #ax01.set_yscale('log')\n",
    "    #     annotation01 = ax01.annotate('{:.1f}'.format(lengths[-1][1]) , xy=lengths[-1], xycoords='data', xytext=(100,-30), \n",
    "    #                                  textcoords='offset points', bbox={'fc':\"1\"}, arrowprops={'fc':'k'}, zorder=2)\n",
    "        ax01.legend()\n",
    "        ax01.ticklabel_format(useOffset=False)\n",
    "\n",
    "        ax10.plot(*zip(*hyperparams[-1]), color=color, label=\"{}\".format(n), lw=3)\n",
    "        ax10.plot(*hyperparams[-1][-1], \"o\", color=color)\n",
    "        label10 = hyperparam_names[-1]\n",
    "        ax10.set_ylabel(\"{}\".format(label10))\n",
    "        #ax10.set_yscale('log')\n",
    "    #     annotation01 = ax10.annotate('{:.1e}'.format(noises[-1][1]) , xy=noises[-1], xycoords='data', xytext=(-30,100),\n",
    "    #                                  textcoords='offset points', bbox={'fc':\"1\"}, arrowprops={'fc':'k'}, zorder=2)\n",
    "    #     ax10.legend()\n",
    "        ax10.ticklabel_format(useOffset=False)\n",
    "\n",
    "        if not len(mse_history):\n",
    "            continue\n",
    "\n",
    "        #loss on axis 11\n",
    "        ax11.plot(*zip(*mse_history), color = color, label=\"{}\".format(n), lw=3)\n",
    "        ax11.plot(*mse_history[-1], \"o\", color=color)\n",
    "        #ax11.set_yscale('log')\n",
    "        bottom, top = ax11.get_ylim()\n",
    "        bottom2, top2 = ax11.get_ylim()\n",
    "        ax11.set_ylabel(\"mse\")\n",
    "        ax11.legend()\n",
    "        ax11.ticklabel_format(useOffset=False)\n",
    "\n",
    "        #fig.suptitle(\"{}\".format(s.n_structs))\n",
    "        \n",
    "        if s.make_output_files:\n",
    "            hyperparameter_results_filename = \"hyperparameter_training\"\n",
    "            plt.savefig(calculation_results_directory + hyperparameter_results_filename)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a regroup function\n",
    "# test_ens_regrouped = test_ens_rescaled.reshape(-1, len(StructureList[0]))\n",
    "# predict_ens_regrouped = predict_ens_rescaled.reshape(-1, len(StructureList[0]))\n",
    "# self_energies_regrouped = [[self_energy(atom.symbol, use_librascal_values=s.use_self_energies) for atom in StructureList[i]] for i in test_indices]\n",
    "# test_global_ens = np.sum(test_ens_regrouped + self_energies_regrouped, axis=1)\n",
    "# predict_global_ens = np.sum(predict_ens_regrouped + self_energies_regrouped, axis=1)\n",
    "test_global_ens = (test_struct_bools @ test_ens_rescaled ).flatten()\n",
    "predict_global_ens = ( test_struct_bools @ predict_ens_rescaled ).flatten()\n",
    "test_global_nats = NAtList[test_indices]\n",
    "\n",
    "\n",
    "if s.prediction_calculation == \"predict_f\":\n",
    "    predict_ens_var_regrouped = predict_ens_var_rescaled.reshape(-1, len(StructureList[0]))\n",
    "    predict_global_ens_var = np.sum(predict_ens_var_regrouped, axis=1)\n",
    "    predict_global_ens_std = predict_global_ens_var ** 0.5 \n",
    "    input_std = (gpr_model.likelihood.variance.numpy() * ens_scaler.scale_[0] **2) ** 0.5\n",
    "    print(\"Our observation noise variance implies our reference error is +/- {:.3} /atom\".format( input_std) )\n",
    "else:\n",
    "    predict_global_ens_std = None\n",
    "plot_energy_errors(model_description = \"gpflow model\",\n",
    "            use_local=True,\n",
    "            global_ens=test_global_ens,   predicted_global_ens= predict_global_ens,\n",
    "            local_ens= test_ens_rescaled, predicted_local_ens = predict_ens_rescaled,\n",
    "            color=\"mediumseagreen\", predicted_stdev = None, n_atoms=test_global_nats, in_notebook=in_notebook )\n",
    "\n",
    "if s.make_output_files:\n",
    "    energy_errors_title = \"energy_predictions\"#\"energy_results\" + today_string + settings_string\n",
    "    energy_errors_plot_filename = calculation_results_directory + energy_errors_title + \".png\"\n",
    "    # check if existing, add number to end if it is\n",
    "    energy_errors_plot_filename = find_unique_filename(energy_errors_plot_filename)\n",
    "    plt.savefig(energy_errors_plot_filename)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if s.use_forces:\n",
    "    predict_frcs_rescaled\n",
    "\n",
    "    fig, axs = plt.subplots(ncols=3, figsize=(20,5))\n",
    "    components = [\"x\", \"y\", \"z\"]\n",
    "    force_max = max(np.max(test_frcs_rescaled), np.max(predict_frcs_rescaled)) + np.std(test_frcs_rescaled)/2\n",
    "    force_min = min(np.min(test_frcs_rescaled), np.min(predict_frcs_rescaled)) - np.std(test_frcs_rescaled)/2\n",
    "\n",
    "    for i in range(3):\n",
    "        axs[i].plot([force_min, force_max], [force_min, force_max], \"-\", c=\"k\")\n",
    "        axs[i].plot(test_frcs_rescaled[:,i], predict_frcs_rescaled[:,i], \"o\", label=\"custom\", c=\"mediumseagreen\", ms=5, alpha=.5, mew=0)\n",
    "        #axs[i].legend()\n",
    "        axs[i].set_xlim(force_min, force_max); axs[i].set_ylim(force_min, force_max)\n",
    "\n",
    "        try:\n",
    "            m, b = np.polyfit(test_frcs_rescaled[:,i], predict_frcs_rescaled[:,i], 1)\n",
    "            r2 = np.corrcoef(test_frcs_rescaled[:,i], predict_frcs_rescaled[:,i])[0,1]\n",
    "            print(\"Least-squares regresion for F{}({}) produces the line line m {}+b with m = {:.5f} and b = {:.5f} which has r2 = {:.5f} \".format(components[i],components[i],components[i],m,b, r2))\n",
    "        except:\n",
    "            pass    \n",
    "\n",
    "    #plt.savefig(\"../media/librascal_database_local_energy_force_learning\")\n",
    "    if s.make_output_files:\n",
    "        force_errors_title = \"force_predictions\"#\"energy_results\" + today_string + settings_string\n",
    "        force_errors_plot_filename = calculation_results_directory + force_errors_title + \".png\"\n",
    "        # check if existing, add number to end if it is\n",
    "        force_errors_plot_filename = find_unique_filename(force_errors_plot_filename)\n",
    "        plt.savefig(force_errors_plot_filename)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Working on improvoing the force error plot\n",
    "\n",
    "# # xfrcs= np.concatenate((test_frcs_rescaled, predict_frcs_rescaled))[:, 0]\n",
    "# xfrcs= predict_frcs_rescaled[:, 0]\n",
    "# outliers_output = separate_outliers(xfrcs,  center_mode=\"mean\", range_mode=\"std_dev\", neighbor_factor=.2,\n",
    "#                                     range_factor=3, percentile_range=50, verbose=True, check_boundary_neighbors=True)\n",
    "# normal_xfrcs, outlier_xfrcs, normal_xfrcs_indices, outlier_xfrcs_indices = outliers_output\n",
    "# plt.plot(normal_xfrcs_indices, normal_xfrcs, \"o\")\n",
    "# plt.plot(outlier_xfrcs_indices, outlier_xfrcs, \"o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Working on improvoing the force error plot\n",
    "\n",
    "# outliers_output = separate_outliers(sorted(xfrcs), center_mode=\"mean\", range_mode=\"percentile\", check_boundary_neighbors=True,\n",
    "#                                     range_factor=1, percentile_range=50, verbose=True)\n",
    "# normal_xfrcs, outlier_xfrcs, normal_xfrcs_indices, outlier_xfrcs_indices = outliers_output\n",
    "# plt.plot(normal_xfrcs_indices, normal_xfrcs, \"o\")\n",
    "# plt.plot(outlier_xfrcs_indices, outlier_xfrcs, \"o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This closes the log file. Probably not necessary since it is at the end of the script, but it's best practice.\n",
    "if s.make_output_files and not in_notebook:\n",
    "    logger.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When converting notebook to script, this line acts as a delimiter. Everything above it will remain. Everything below it will be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save as script "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abort_before_saving_script = True\n",
    "assert not abort_before_saving_script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will save this notebook as a python script with today's date in the code directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to convert miniGAP_functional.ipynb to a notebook.\n",
      "If this fails, check to be sure this notebook is in fact named miniGAP_functional.ipynb.\n",
      "\n",
      "[NbConvertApp] Converting notebook miniGAP_functional.ipynb to script\n",
      "[NbConvertApp] Writing 68702 bytes to ../code/miniGAP_from_notebook_2021_12_10.py\n"
     ]
    }
   ],
   "source": [
    "current_notebook_filename = \"miniGAP_functional.ipynb\"\n",
    "converted_script_filename = \"../code/miniGAP_from_notebook\" + today_string \n",
    "print(\"Attempting to convert {} to a notebook.\\nIf this fails, check to be sure this notebook is in fact named {}.\\n\".format(current_notebook_filename, current_notebook_filename))\n",
    "!jupyter nbconvert --to script \"{current_notebook_filename}\" --output \"{converted_script_filename}\"\n",
    "!sed -i '/When converting notebook to script, this line acts as a delimiter/,$d' \"{converted_script_filename}.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:minigap]",
   "language": "python",
   "name": "conda-env-minigap-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
