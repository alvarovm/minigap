{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# miniGAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can perform all the functions of miniGAP within this notebook or you can create a python script from the last cell of this notebook and run the script in the terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell allows us to time the initial setup of miniGAP if we are running the miniGAP script\n",
    "# in_notebook() is a function that returns True if this code is run from an ipython kernel or False otherwise\n",
    "\n",
    "import sys\n",
    "sys.path.append('../code')\n",
    "from miniGAP_helper_functions import check_if_in_notebook\n",
    "\n",
    "in_notebook = check_if_in_notebook()\n",
    "if not in_notebook:\n",
    "    import time\n",
    "    TimeBeforeStartUp = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell gives us a couple options for debugging Tensorflow.\n",
    "# To enable this debugging, you must change one of the debugging flags to True and run this cell *before* importing Tensorflow\n",
    "\n",
    "tf_cpu_debugging =False\n",
    "if tf_cpu_debugging:\n",
    "    import os\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "    import tensorflow as tf\n",
    "    #tf.debugging.set_log_device_placement(True)\n",
    "    a = tf.constant(1)\n",
    "    \n",
    "    if tf.test.gpu_device_name():\n",
    "        print(\"GPUs recognized by tensorflow:\", tf.config.list_physical_devices('GPU'))\n",
    "    else:\n",
    "        print(\"No GPU found\")\n",
    "\n",
    "tf_gpu_debugging = False\n",
    "if tf_gpu_debugging:\n",
    "#     See here for possible option to reset memory github.com/tensorflow/tensorflow/issues/36465\n",
    "#     import os \n",
    "#     os.environ['TF_GPU_ALLOCATOR']='cuda_malloc_async'\n",
    "    import tensorflow as tf\n",
    "    tf.debugging.set_log_device_placement(True)\n",
    "    a = tf.constant(1)\n",
    "    if tf.test.gpu_device_name():\n",
    "        print(\"GPUs recognized by tensorflow:\", tf.config.list_physical_devices('GPU'))\n",
    "    else:\n",
    "        print(\"No GPU found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import functions from my files\n",
    "import sys\n",
    "sys.path.append('../code')\n",
    "from Molecular_Dynamics import generate_md_traj, make_diatomic\n",
    "from miniGAP_helper_functions import *\n",
    "\n",
    "# import functions from modules\n",
    "import os.path as path\n",
    "import resource\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import gpflow\n",
    "from itertools import islice\n",
    "import matplotlib.pyplot as plt\n",
    "from gpflow.utilities import print_summary\n",
    "import json\n",
    "from collections import namedtuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets the miniGAP home directory. This assumes the notebook or script is located one directory below the home directory.\n",
    "if in_notebook:\n",
    "    miniGAP_parent_directory = \"../\"\n",
    "else:\n",
    "    miniGAP_parent_directory = path.dirname(path.dirname(path.realpath(__file__))) + \"/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input parameter notes:\n",
    "\n",
    "energy_encoding= \"info\" for QM9 or \"normal\" for distorted propenols\n",
    "energy_keyword=\"U0\" for QM9 or ignored for distorted propenols\n",
    "\n",
    "my_priority = #\"efficiency\" for experimenting with something new or otherwise \"consistency\"\n",
    "\n",
    "controls initial train_test_split breaking apart training and test data\n",
    "split_seed = 2\n",
    "\n",
    "controls in-training train_test_split breaking apart training_j and validation_j data\n",
    "valid_split_seed = 2\n",
    "\n",
    "controls multiple tf stochastic processes including:\n",
    "1) Adams optimizer AND \n",
    "2) batching through tf.data.Dataset.from_tensor_slices in training\n",
    "tf_seed = 2\n",
    "\n",
    "controls batching through tf.data.Dataset.from_tensor_slices in training\n",
    "shuffle_seed = 2\n",
    "\n",
    "kernel_type = #\"polynomial\" for actual GAP or \"exponentiated_quadratic\" possibly for debugging\n",
    "\n",
    "prediction_calculation = #\"direct\" OR \"predict_f\" OR \"cholesky\" OR \"alpha\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell allows the miniGAP script to accept commandline parameters\n",
    "# These commandline parameters have priority over the JSON settings\n",
    "# Most, but not all of the JSON settings can be overwritten using a commandline option\n",
    "\n",
    "if not in_notebook:\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    # arguments for debugging\n",
    "    parser.add_argument('--verbose', type=bool, help=\"Print out details at each step\") \n",
    "    parser.add_argument('-vt', '--print_timings', type=bool, help=\"Print out details at each step\") \n",
    "\n",
    "    # arguments specific to forming dataset (including potentially creating md trajectory)\n",
    "    parser.add_argument('-sf', '--structure_file', help=\"Specify a structure file to import. 'None' will be interpretted as using no structure file.\")\n",
    "    parser.add_argument('-cf', '--chemical_formula',  help=\"If no structure file is supplied, you can specify a single structure here and perform md \\\n",
    "    to generate a trajectory that you will use as your dataset. If neither this nor a structure file are supplied, we will use diatomics.\")\n",
    "    parser.add_argument(\"-md\", '--molecular_dynamics', type=bool, help=\"Indicate if you want molecular dynamics performed. Will generate diatomic if no structure given\")\n",
    "    # available as a JSON parameter, but commandline argument is buggy\n",
    "#     parser.add_argument('-mdi', '--md_indices', default=[0], type=int, nargs='*', help=\"If performing molecular dynamics on a structure file with multiple structures, you can give indices of all structures to perform md on.\")\n",
    "    parser.add_argument('-mdi', '--md_index', type=int, help=\"If performing molecular dynamics on a structure file with multiple structures, you can give the index of the structure to perform md on.\")\n",
    "    parser.add_argument('-de', '--diatomic_element',  choices = [\"N\", \"O\", \"H\"], help=\"If generating diatomics, you can specify element\")\n",
    "    parser.add_argument('-dbl', '--diatomic_bond_length',  type=float, help=\"If generating diatomics, you can specify initial bond length\")\n",
    "    parser.add_argument('-mdt', '--md_temp',  type=float, help=\"If performing molecular dynamics, specify temperatutre (K) of MD\")\n",
    "    parser.add_argument('-mda', '--md_algorithm',  choices = [\"VelocityVerlet\", \"Berendsen\"], type=str, help=\"If performing molecular dynamics, specify algorithm of MD\")\n",
    "    parser.add_argument('-mts', '--md_time_step',  type=float, help=\"If performing molecular dynamics, specify time step (fs) of MD\")\n",
    "    parser.add_argument('-mds', '--md_seed',  type=int, help=\"If performing molecular dynamics, change this seed to get different trajectories\")\n",
    "    parser.add_argument('-mec', '--md_energy_calculator',  choices = [\"EMT\", \"LJ\", \"Morse\"], help = \"If performing molecular dynamics, specify ASE energy/force calculator\")\n",
    "    parser.add_argument('-n', '--n_structs',  type=int, help=\"Specify # of md generated structures or # of structures to use from input file\")\n",
    "\n",
    "    # arguments specific to soap\n",
    "    parser.add_argument('--rcut',  type=float, help= \"Choice of SOAP cut off radius\")\n",
    "    parser.add_argument('--nmax',  type=int, help=\"Choice of SOAP n_max\")\n",
    "    parser.add_argument('--lmax',  type=int, help=\"Choice of SOAP l_max\")\n",
    "\n",
    "    # arguments specific to learning\n",
    "    parser.add_argument('-ss', '--split_seed', type=int, help=\"Random seed for cross-validation\")\n",
    "    parser.add_argument('-tf', '--train_fraction', type=float, help=\"Specify the fraction of structures used in training\")\n",
    "    parser.add_argument('-ne', '--n_epochs', type=int, help=\"Number of epochs\")\n",
    "\n",
    "#     some housekeeping\n",
    "#     parser.add_argument('remainder', nargs=argparse.REMAINDER, help=argparse.SUPPRESS)\n",
    "\n",
    "    cmdline_args = parser.parse_args()\n",
    "    cmdline_args_dict = vars(cmdline_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can change the input parameters in the JSON file and rerun the notebook starting from here\n",
    "\n",
    "settings_json_filename = miniGAP_parent_directory + \"code/miniGAP_settings.json\"\n",
    "with open(settings_json_filename, encoding = 'utf-8') as settings_file_object:\n",
    "    default_settings_dict = json.load(settings_file_object)\n",
    "\n",
    "settings_dict = default_settings_dict\n",
    "if not in_notebook:\n",
    "    for setting_name in settings_dict.keys():\n",
    "        if setting_name in cmdline_args_dict.keys():\n",
    "            if cmdline_args_dict[setting_name] != None:\n",
    "                settings_dict[setting_name] = cmdline_args_dict[setting_name]\n",
    "    \n",
    "    for setting_name in cmdline_args_dict.keys():\n",
    "        if setting_name not in settings_dict.keys():\n",
    "            print(\"The commandline argument {} is currently nonfunctional because it does not exist in the JSON file.\".format(setting_name))\n",
    "SettingsNamespace = namedtuple(\"Settings\", settings_dict.keys())\n",
    "s = SettingsNamespace(*settings_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stack size set to unlimited\n",
      "1 GPU(s) recognized by tensorflow: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# This cell handles some basic initialization tasks\n",
    "\n",
    "# Sets the printing format of gpflow model hyperparameters\n",
    "if in_notebook:\n",
    "    gpflow.config.set_default_summary_fmt(\"notebook\")\n",
    "else:\n",
    "    gpflow.config.set_default_summary_fmt(\"grid\")\n",
    "    \n",
    "# Compiles some functions as TensorFlow tf functions\n",
    "mse_tf = tf.function(mse, autograph=False, jit_compile=False)\n",
    "mse_2factor_tf = tf.function(mse_2factor, autograph=False, jit_compile=False)\n",
    "train_hyperparams_without_forces_tf = tf.function(train_hyperparams_without_forces, autograph=False, jit_compile=False)\n",
    "predict_energies_from_weights_tf = tf.function(predict_energies_from_weights, autograph=False, jit_compile=False)\n",
    "\n",
    "# miniGAP uses a lot memory so it is good to allow it access to as much as possible\n",
    "try:\n",
    "    resource.setrlimit( resource.RLIMIT_STACK, ( resource.RLIM_INFINITY, resource.RLIM_INFINITY ) )\n",
    "    if s.verbose:\n",
    "        print(\"Stack size set to unlimited\")\n",
    "except:\n",
    "    print(\"Warning: Unable to raise stack size limit. Typically miniGAP uses a lot of memory and operates better if you allow the stack size to be unlimited. You can try to do this with the command 'ulimit -s unlimited'\")\n",
    "\n",
    "# Check on GPU availability\n",
    "if s.verbose:\n",
    "    print(\"{} GPU(s) recognized by tensorflow:\".format(len(tf.config.list_physical_devices('GPU'))), tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported 1000 structures from ../data/distorted_propenol.db. Structures were taken uniformly from throughout dataset which contains 2000 total structures.\n",
      "Compiling structures into list took 0.82 seconds\n"
     ]
    }
   ],
   "source": [
    "# This cell compiles the structure dataset to be used by miniGAP\n",
    "# You have several options for how to choosing this dataset:\n",
    "# 1) Import structure dataset directly from file\n",
    "# 2) Import a structure from a file and then run an MD simulation with ASE. \n",
    "#    The structures in the MD trajectory will be used as the dataset.\n",
    "# 3) Do not use a file ('\"structure_file\" : null' in the JSON). Specify the chemical formula of a molecule within the g2 collection and then run an MD simulation with ASE.\n",
    "#    The structures in the MD trajectory will be used as the dataset.\n",
    "#    Information about the g2 collection:\n",
    "#      - https://aip.scitation.org/doi/10.1063/1.473182\n",
    "#      - https://wiki.fysik.dtu.dk/ase/ase/build/build.html#molecules\n",
    "# 4) Do not use a file or specify a chemical formula('\"structure_file\" : null'  and '\"chemical_formula\" : null' in the JSON).\n",
    "#    This is like option 3, but the starter molecule will be a diatomic.\n",
    "#    You can specify the diatomic element and initial bond length in the JSON or commandline arguments.\n",
    "\n",
    "# Note 1: If you don't include a path in the filename, miniGAP will look in the /minigap/data/ directory \n",
    "# Note 2: Currently only the force fields with a native ASE implementation are implemented for MD generation of a dataset.\n",
    "#         These forcfield are \"EMT\", \"LJ\", and \"Morse\". All perform very poorly for nearly all molecules and structures.\n",
    "#         An exception is diatomic molecules, for which they capture the most important behavior.\n",
    "# Note 3: You can see here that I use the function 'TickTock'. The real function is 'CompileStructureList'.\n",
    "#         'TickTock' is is just for timing purposes. See the next cell for more details.\n",
    "\n",
    "StructureList, TimeCompileStructures = TickTock(CompileStructureList, s, in_notebook, miniGAP_parent_directory)\n",
    "if s.print_timings:\n",
    "    print(\"Compiling structures into list took {:.2f} seconds\".format(TimeCompileStructures))\n",
    "\n",
    "\n",
    "ns_atoms = np.unique([len(struct) for struct in StructureList])\n",
    "assert len(ns_atoms) == 1\n",
    "n_atoms = ns_atoms[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You will see the TickTock function used throughout this notebook\n",
    "# It is a helper function that allows me to time other functions concisely.\n",
    "# For an example usage set the below flag to True and inspect the code\n",
    "\n",
    "see_example_of_TickTock_usage = False\n",
    "if see_example_of_TickTock_usage:\n",
    "    def example_function(a, b, c=\"DEFAULT_VALUE\", d=\"DEFAULT_VALUE\", function_call_type=\"normal\"):\n",
    "        summation=0\n",
    "        for i in range(a):\n",
    "            summation += b\n",
    "        print(\"This {} function call accepted the argument '{}' for the positional parameter 'a'\".format(function_call_type, a))\n",
    "        print(\"This {} function call accepted the argument '{}' for the positional parameter 'b'\".format(function_call_type, b))\n",
    "        print(\"This {} function call accepted the argument '{}' for the keyword parameter 'c'\".format(function_call_type, c))\n",
    "        print(\"This {} function call accepted the argument '{}' for the keyword parameter 'd'\".format(function_call_type, d))\n",
    "        return summation\n",
    "    \n",
    "    a_value = 1234567\n",
    "    b_value = 1\n",
    "    c_value = \"CAT\"\n",
    "    d_value = \"DOG\"\n",
    "    \n",
    "    # You can use the function the normal way:\n",
    "    normal_function_output = example_function(a_value, b_value, c=c_value, d=d_value, function_call_type=\"normal\")\n",
    "    print(\"This normal function call returned {}\\n\".format(normal_function_output) )\n",
    "    \n",
    "    # You can time the function with a slight modification, no extra lines needed\n",
    "    TickTock_function_output, function_time = TickTock(example_function, a_value, b_value, c=c_value, d=d_value, function_call_type=\"TickTock\")\n",
    "    print(\"This TickTock function call returned {}\".format(TickTock_function_output) )\n",
    "    print(\"This TickTock function call took {:.2f} seconds to run\".format(function_time) )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set convert flag to True and run this cell to convert your file to a database right now from this notebook\n",
    "# I do not recommend this. The database creation is very slow and will lock you out from running any cells for a long time.\n",
    "# It is better to do this from a terminal with the script convert_to_db.py\n",
    "# For example, you can run the command '/relative/path/to/minigap/code/convert_to_db.py structure_file_a.xyz structure_file_b.xyz structure_file_c.xyz'\n",
    "# This command would create the files structure_file_a.db, structure_file_b.db and structure_file_c.db in your /relative/path/to/minigap/data/ directory\n",
    "# To overwrite an existing .db file use the --existing_file_behavior flag\n",
    "# convert_to_db.py --help will explain some more details\n",
    "convert_to_db_here_and_now = False\n",
    "if convert_to_db_here_and_now and in_notebook:\n",
    "    !python ../code/convert_to_db.py $s.structure_file \n",
    "    # Use the following line instead of the previous line if you need to overwrite an existing database\n",
    "    # !python ../code/convert_to_db.py -efb overwrite $s.structure_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell completes the timing started in the first cell if this code is executed from a script\n",
    "\n",
    "if s.print_timings and not in_notebook:\n",
    "    TimeAfterStartUp = time.time()\n",
    "    TimeStartUp = TimeAfterStartUp - TimeBeforeStartUp\n",
    "    print(\"Completed tje initial setup of miniGAP (including importing structures) in {:.2f} seconds\".format(TimeStartUp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this flag to True if you want to visualize your structure within this jupyter notebook (no pop-up window)\n",
    "# For more details on the plotting function, refer to the Visualize_Structures notebook\n",
    "\n",
    "check_structures_visually = False\n",
    "if check_structures_visually:\n",
    "    from Visualize_Structures import Structure3DAnimation\n",
    "    # You can display the animation in one line if you are not within an if statement.\n",
    "    # But you need to explicitly call display() if you are within an if statement:\n",
    "    # 'Structure3DAnimation(StructureList).Plot()'\n",
    "    html_animation = Structure3DAnimation(StructureList).Plot()\n",
    "    display(html_animation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathered energy and structure info in 1.88 seconds\n"
     ]
    }
   ],
   "source": [
    "# This cell \n",
    "\n",
    "[EnergyList, ForceList, PosList], TimeGather = TickTock(GatherStructureInfo, StructureList, gather_forces = s.use_forces, use_self_energies=s.use_self_energies, \n",
    "                                                     energy_encoding = s.energy_encoding,  energy_keyword = s.energy_keyword)\n",
    "gather_forces_message= \", force\" if s.use_forces else \"\"\n",
    "if s.print_timings:\n",
    "    print(\"Gathered energy{} and structure info in {:.2f} seconds\".format(gather_forces_message, TimeGather))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated SOAP descriptors in 1.20 seconds\n"
     ]
    }
   ],
   "source": [
    "[SoapDerivativeList, SoapList], TimeSoap = TickTock(GenerateDescriptorsAndDerivatives, StructureList, s.nmax, s.lmax, s.rcut, s.smear, s.attach_SOAP_center, s.is_periodic, s.use_forces)\n",
    "calculate_derivatives_message = \" and derivatives\" if s.use_forces else \"\"\n",
    "if s.print_timings:\n",
    "    print(\"Generated SOAP descriptors{} in {:.2f} seconds\".format(calculate_derivatives_message, TimeSoap))\n",
    "elif s.verbose:\n",
    "    print(\"Generated SOAP descriptors{}.\".format(calculate_derivatives_message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reformatted data to build model in 0.33 seconds.\n"
     ]
    }
   ],
   "source": [
    "out_data, TimePrepare = TickTock(PrepareDataForTraining, \n",
    "                                sp_list=SoapList, \n",
    "                                dsp_dx_list = SoapDerivativeList, \n",
    "                                en_list = EnergyList,\n",
    "                                frc_list = ForceList ,\n",
    "                                pos_list = PosList, \n",
    "                                split_seed = s.split_seed, \n",
    "                                prepare_forces = s.use_forces, \n",
    "                                train_fract = s.train_fraction,\n",
    "                                scale_soaps = s.scale_soaps\n",
    "                                )\n",
    "\n",
    "if s.print_timings:\n",
    "    print(\"Reformatted data to build model in {:.2f} seconds.\".format(TimePrepare))\n",
    "\n",
    "if not s.use_forces:\n",
    "    train_sps_full, test_sps_full, train_ens, test_ens, train_indices, test_indices, soap_scaler, ens_scaler, ens_var = out_data\n",
    "else:\n",
    "    train_sps_full, test_sps_full, train_ens, test_ens, train_indices, test_indices, soap_scaler, ens_scaler, ens_var, train_dsp_dx, test_dsp_dx, train_frcs, test_frcs, frcs_var = out_data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using any sparsity.\n"
     ]
    }
   ],
   "source": [
    "n_samples_full, n_features_full = train_sps_full.shape\n",
    "SparsifySoapsOutput, TimeSparsify = TickTock( SparsifySoaps, train_soaps = train_sps_full, test_soaps = test_sps_full, train_energies=train_ens, sparsify_samples=s.sparse_gpflow, \n",
    "                                    n_samples=s.n_sparse, sparsify_features=s.sparse_features, n_features=s.n_sparse_features, selection_method=\"PCovCUR\",\n",
    "                                    score_tolerance=1e-5, score_threshold=1e-5, iterative_selection=False, plot_importances=False) \n",
    "train_sps, sparse_train_sps, test_sps = SparsifySoapsOutput\n",
    "if s.sparse_gpflow or s.sparse_features:\n",
    "    sparsity_samples_message = \"\" if not s.sparse_gpflow else \"samples ({} --> {})\".format(n_samples_full, sparse_train_sps.shape[0])\n",
    "    sparsity_features_message = \"\" if not s.sparse_features else \"features ({} --> {})\".format(n_features_full, train_sps.shape[-1])\n",
    "    sparsity_both_message = \" and \" if (s.sparse_gpflow and s.sparse_features) else \"\"\n",
    "    if s.print_timings:\n",
    "        print(\"Sparsified model {}{}{} in {:.2f} seconds.\".format( sparsity_samples_message, sparsity_both_message, sparsity_features_message, TimeSparsify) )\n",
    "    elif s.verbose:\n",
    "        print(\"Sparsified model {}{}{}\".format( sparsity_samples_message, sparsity_both_message, sparsity_features_message) )\n",
    "else:\n",
    "    if s.verbose:\n",
    "        print(\"Not using any sparsity.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future investigations for hyperparameter training\n",
    "---\n",
    "1. Does Adam optimizer have problems sometimes within a tf.function?\n",
    "2. Custom loss function vs optimizer.minimize\n",
    "3. Set certain variable untrainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using a degree 2 polynomial kernel.\n",
      "Alert: Double check the training validity for degree =/= 1 when not using predict_f\n",
      "Training using 8100 atoms without batching.\n",
      "Epoch 0\n",
      "Printing in mse (not compiled)\n",
      "TRACING train_hyperparams_without_forces\n",
      "Printing in mse (not compiled)\n",
      "TRACING train_hyperparams_without_forces\n",
      "predict energies =  [[-0.88300073068866436]\n",
      " [-0.51430591387234514]\n",
      " [1.1643518333424445]]\n",
      "gradients =  (-0.0014388502360508388, 0.0011266768661520375)\n",
      "valid_ens[:3] = [-0.86893915 -0.64888878  1.08192877]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.020506509360746814 + 0 = 0.020506509360746814 \n",
      "Epoch 0,  mse = 0.020506509360746814\n",
      "predict energies =  [[-0.88327803695622875]\n",
      " [-0.51448093290927865]\n",
      " [1.1661034730009052]]\n",
      "gradients =  (-0.0013766658697360514, 0.001083480395065114)\n",
      "valid_ens[:3] = [-0.86893915 -0.64888878  1.08192877]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.020456377119211562 + 0 = 0.020456377119211562 \n",
      "Epoch 1,  mse = 0.020456377119211562\n",
      "predict energies =  [[-0.88354602163392215]\n",
      " [-0.51467246869331718]\n",
      " [1.1678181075629361]]\n",
      "gradients =  (-0.001315065970869075, 0.0010402946525305428)\n",
      "valid_ens[:3] = [-0.86893915 -0.64888878  1.08192877]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.02040837160510628 + 0 = 0.02040837160510628 \n",
      "Epoch 2,  mse = 0.02040837160510628\n",
      "predict energies =  [[-0.88380435157910342]\n",
      " [-0.51488000008491275]\n",
      " [1.1694921153296511]]\n",
      "gradients =  (-0.0012541132347216703, 0.00099716308431632445)\n",
      "valid_ens[:3] = [-0.86893915 -0.64888878  1.08192877]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.020362538688313003 + 0 = 0.020362538688313003 \n",
      "Epoch 3,  mse = 0.020362538688313003\n",
      "predict energies =  [[-0.8840528143211972]\n",
      " [-0.51510298691671486]\n",
      " [1.1711226187681238]]\n",
      "gradients =  (-0.0011938596399812488, 0.00095412379306323756)\n",
      "valid_ens[:3] = [-0.86893915 -0.64888878  1.08192877]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.020318904712808374 + 0 = 0.020318904712808374 \n",
      "Epoch 4,  mse = 0.020318904712808374\n",
      "predict energies =  [[-0.88429124738803033]\n",
      " [-0.51534082710119145]\n",
      " [1.172707141999997]]\n",
      "gradients =  (-0.0011343586458020596, 0.000911217975796133)\n",
      "valid_ens[:3] = [-0.86893915 -0.64888878  1.08192877]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.020277486863836982 + 0 = 0.020277486863836982 \n",
      "Epoch 5,  mse = 0.020277486863836982\n",
      "predict energies =  [[-0.88451953596403177]\n",
      " [-0.51559286790383052]\n",
      " [1.1742435548960577]]\n",
      "gradients =  (-0.0010756673131573722, 0.00086849173668964092)\n",
      "valid_ens[:3] = [-0.86893915 -0.64888878  1.08192877]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.020238294786976033 + 0 = 0.020238294786976033 \n",
      "Epoch 6,  mse = 0.020238294786976033\n",
      "predict energies =  [[-0.884737605474406]\n",
      " [-0.51585838196089906]\n",
      " [1.1757300391890495]]\n",
      "gradients =  (-0.0010178467094281423, 0.00082599633279626815)\n",
      "valid_ens[:3] = [-0.86893915 -0.64888878  1.08192877]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.020201330901737494 + 0 = 0.020201330901737494 \n",
      "Epoch 7,  mse = 0.020201330901737494\n",
      "predict energies =  [[-0.88494542155950706]\n",
      " [-0.51613658102684212]\n",
      " [1.1771650896848684]]\n",
      "gradients =  (-0.00096096174580468166, 0.00078378830123989685)\n",
      "valid_ens[:3] = [-0.86893915 -0.64888878  1.08192877]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.020166590058330092 + 0 = 0.020166590058330092 \n",
      "Epoch 8,  mse = 0.020166590058330092\n",
      "predict energies =  [[-0.88514300131775192]\n",
      " [-0.51642659961950876]\n",
      " [1.1785475173660331]]\n",
      "gradients =  (-0.000905080493528396, 0.00074192887101352551)\n",
      "valid_ens[:3] = [-0.86893915 -0.64888878  1.08192877]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.020134059570512335 + 0 = 0.020134059570512335 \n",
      "Epoch 9,  mse = 0.020134059570512335\n",
      "Epoch 10\n",
      "predict energies =  [[-0.88533037715585627]\n",
      " [-0.51672750414238688]\n",
      " [1.1798764276863036]]\n",
      "gradients =  (-0.00085027333873439678, 0.00070048367142075939)\n",
      "valid_ens[:3] = [-0.86893915 -0.64888878  1.08192877]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.020103718616764396 + 0 = 0.020103718616764396 \n",
      "Epoch 10,  mse = 0.020103718616764396\n",
      "predict energies =  [[-0.88550764242014457]\n",
      " [-0.51703827022676119]\n",
      " [1.1811512172601362]]\n",
      "gradients =  (-0.000796612638232292, 0.00065952201426213808)\n",
      "valid_ens[:3] = [-0.86893915 -0.64888878  1.08192877]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.020075538312540414 + 0 = 0.020075538312540414 \n",
      "Epoch 11,  mse = 0.020075538312540414\n",
      "predict energies =  [[-0.88567491113128671]\n",
      " [-0.51735782596328728]\n",
      " [1.182371567814]]\n",
      "gradients =  (-0.00074417107273852708, 0.00061911632500765443)\n",
      "valid_ens[:3] = [-0.86893915 -0.64888878  1.08192877]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.02004948127832387 + 0 = 0.02004948127832387 \n",
      "Epoch 12,  mse = 0.02004948127832387\n",
      "predict energies =  [[-0.88583233911796022]\n",
      " [-0.517685008644948]\n",
      " [1.1835374123769393]]\n",
      "gradients =  (-0.000693021289002586, 0.00057934096649421226)\n",
      "valid_ens[:3] = [-0.86893915 -0.64888878  1.08192877]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.0200255019932418 + 0 = 0.0200255019932418 \n",
      "Epoch 13,  mse = 0.0200255019932418\n",
      "predict energies =  [[-0.88598011659347176]\n",
      " [-0.518018605355375]\n",
      " [1.1846489397370774]]\n",
      "gradients =  (-0.00064323459981885552, 0.0005402718455899835)\n",
      "valid_ens[:3] = [-0.86893915 -0.64888878  1.08192877]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.020003546368935355 + 0 = 0.020003546368935355 \n",
      "Epoch 14,  mse = 0.020003546368935355\n",
      "predict energies =  [[-0.88611846155391238]\n",
      " [-0.51835733194354261]\n",
      " [1.1857065781722336]]\n",
      "gradients =  (-0.00059487974002759084, 0.00050198501774397386)\n",
      "valid_ens[:3] = [-0.86893915 -0.64888878  1.08192877]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.01998355251276363 + 0 = 0.01998355251276363 \n",
      "Epoch 15,  mse = 0.01998355251276363\n",
      "predict energies =  [[-0.886247623513198]\n",
      " [-0.51869986093504361]\n",
      " [1.186710947208387]]\n",
      "gradients =  (-0.00054802263591001432, 0.00046455608524725904)\n",
      "valid_ens[:3] = [-0.86893915 -0.64888878  1.08192877]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.01996545059900154 + 0 = 0.01996545059900154 \n",
      "Epoch 16,  mse = 0.01996545059900154\n",
      "predict energies =  [[-0.8863678809789004]\n",
      " [-0.51904481078116049]\n",
      " [1.1876628532108813]]\n",
      "gradients =  (-0.00050272447103685142, 0.00042805898427799417)\n",
      "valid_ens[:3] = [-0.86893915 -0.64888878  1.08192877]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.01994916362312948 + 0 = 0.01994916362312948 \n",
      "Epoch 17,  mse = 0.01994916362312948\n",
      "predict energies =  [[-0.88647953936534118]\n",
      " [-0.51939077008250034]\n",
      " [1.188563286448229]]\n",
      "gradients =  (-0.00045904177248744238, 0.0003925652608338235)\n",
      "valid_ens[:3] = [-0.86893915 -0.64888878  1.08192877]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.019934607858002475 + 0 = 0.019934607858002475 \n",
      "Epoch 18,  mse = 0.019934607858002475\n",
      "predict energies =  [[-0.88658291073703521]\n",
      " [-0.51973630586352493]\n",
      " [1.189413337999738]]\n",
      "gradients =  (-0.00041702474512033891, 0.00035814282581558328)\n",
      "valid_ens[:3] = [-0.86893915 -0.64888878  1.08192877]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.019921693869909753 + 0 = 0.019921693869909753 \n",
      "Epoch 19,  mse = 0.019921693869909753\n",
      "Epoch 20\n",
      "predict energies =  [[-0.88667834537621837]\n",
      " [-0.5200799769261103]\n",
      " [1.1902142696077205]]\n",
      "gradients =  (-0.00037671749898799366, 0.00032485551905761956)\n",
      "valid_ens[:3] = [-0.86893915 -0.64888878  1.08192877]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.019910326836757537 + 0 = 0.019910326836757537 \n",
      "Epoch 20,  mse = 0.019910326836757537\n",
      "predict energies =  [[-0.88676619918708766]\n",
      " [-0.52042034950929872]\n",
      " [1.1909673955608964]]\n",
      "gradients =  (-0.00033815685190523824, 0.00029276200409329348)\n",
      "valid_ens[:3] = [-0.86893915 -0.64888878  1.08192877]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.019900407996981395 + 0 = 0.019900407996981395 \n",
      "Epoch 21,  mse = 0.019900407996981395\n",
      "predict energies =  [[-0.88684684106508738]\n",
      " [-0.52075599262283745]\n",
      " [1.1916741416050769]]\n",
      "gradients =  (-0.00030137240235437145, 0.00026191545276931107)\n",
      "valid_ens[:3] = [-0.86893915 -0.64888878  1.08192877]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.019891835365199877 + 0 = 0.019891835365199877 \n",
      "Epoch 22,  mse = 0.019891835365199877\n",
      "predict energies =  [[-0.88692064223214628]\n",
      " [-0.52108552259379537]\n",
      " [1.1923359721763893]]\n",
      "gradients =  (-0.00026638570899772951, 0.0002323625055990932)\n",
      "valid_ens[:3] = [-0.86893915 -0.64888878  1.08192877]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.01988450500193939 + 0 = 0.01988450500193939 \n",
      "Epoch 23,  mse = 0.01988450500193939\n",
      "predict energies =  [[-0.88698799557728969]\n",
      " [-0.52140759951577842]\n",
      " [1.1929543901347179]]\n",
      "gradients =  (-0.0002332110243255716, 0.0002041434520885111)\n",
      "valid_ens[:3] = [-0.86893915 -0.64888878  1.08192877]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.019878311678368406 + 0 = 0.019878311678368406 \n",
      "Epoch 24,  mse = 0.019878311678368406\n",
      "predict energies =  [[-0.88704928499934543]\n",
      " [-0.52172094406676339]\n",
      " [1.1935309526912425]]\n",
      "gradients =  (-0.00020185436981272863, 0.00017729136261003891)\n",
      "valid_ens[:3] = [-0.86893915 -0.64888878  1.08192877]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.019873150621088492 + 0 = 0.019873150621088492 \n",
      "Epoch 25,  mse = 0.019873150621088492\n",
      "predict energies =  [[-0.88710488273503418]\n",
      " [-0.522024353184132]\n",
      " [1.1940671972138528]]\n",
      "gradients =  (-0.00017231430734060589, 0.00015183202330321119)\n",
      "valid_ens[:3] = [-0.86893915 -0.64888878  1.08192877]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.019868918208198905 + 0 = 0.019868918208198905 \n",
      "Epoch 26,  mse = 0.019868918208198905\n",
      "predict energies =  [[-0.887155181584208]\n",
      " [-0.52231670428450938]\n",
      " [1.1945646778472006]]\n",
      "gradients =  (-0.00014458194155619365, 0.00012778413967671532)\n",
      "valid_ens[:3] = [-0.86893915 -0.64888878  1.08192877]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.019865512980062134 + 0 = 0.019865512980062134 \n",
      "Epoch 27,  mse = 0.019865512980062134\n",
      "predict energies =  [[-0.88720053252837827]\n",
      " [-0.52259698300395663]\n",
      " [1.1950249482442312]]\n",
      "gradients =  (-0.00011864124883856006, 0.00010515905323614465)\n",
      "valid_ens[:3] = [-0.86893915 -0.64888878  1.08192877]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.019862836617730424 + 0 = 0.019862836617730424 \n",
      "Epoch 28,  mse = 0.019862836617730424\n",
      "predict energies =  [[-0.88724130584074623]\n",
      " [-0.52286426381428686]\n",
      " [1.1954495394012126]]\n",
      "gradients =  (-9.4469616510384213e-05, 8.3960991305129511e-05)\n",
      "valid_ens[:3] = [-0.86893915 -0.64888878  1.08192877]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.01986079509534916 + 0 = 0.01986079509534916 \n",
      "Epoch 29,  mse = 0.01986079509534916\n",
      "Epoch 30\n",
      "predict energies =  [[-0.88727784623783812]\n",
      " [-0.523117755480358]\n",
      " [1.1958399489060541]]\n",
      "gradients =  (-7.2038498999300177e-05, 6.4187357554650791e-05)\n",
      "valid_ens[:3] = [-0.86893915 -0.64888878  1.08192877]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.019859299059481972 + 0 = 0.019859299059481972 \n",
      "Epoch 30,  mse = 0.019859299059481972\n",
      "predict energies =  [[-0.88731048478164576]\n",
      " [-0.52335677759967414]\n",
      " [1.1961976635687432]]\n",
      "gradients =  (-5.1313440764748364e-05, 4.5829074570049955e-05)\n",
      "valid_ens[:3] = [-0.86893915 -0.64888878  1.08192877]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.01985826456287691 + 0 = 0.01985826456287691 \n",
      "Epoch 31,  mse = 0.01985826456287691\n",
      "predict energies =  [[-0.88733953822864842]\n",
      " [-0.5235807711778383]\n",
      " [1.1965241552202754]]\n",
      "gradients =  (-3.2255047383512175e-05, 2.8870795369366428e-05)\n",
      "valid_ens[:3] = [-0.86893915 -0.64888878  1.08192877]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.019857613914691357 + 0 = 0.019857613914691357 \n",
      "Epoch 32,  mse = 0.019857613914691357\n",
      "predict energies =  [[-0.88736529752084492]\n",
      " [-0.52378930953718406]\n",
      " [1.1968208063522023]]\n",
      "gradients =  (-1.481934813602967e-05, 1.3291301283590966e-05)\n",
      "valid_ens[:3] = [-0.86893915 -0.64888878  1.08192877]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.01985727556633896 + 0 = 0.01985727556633896 \n",
      "Epoch 33,  mse = 0.01985727556633896\n",
      "predict energies =  [[-0.887388050082916]\n",
      " [-0.52398208902260113]\n",
      " [1.1970890431711185]]\n",
      "gradients =  (1.0413413824052198e-06, -9.3565481340279422e-07)\n",
      "valid_ens[:3] = [-0.86893915 -0.64888878  1.08192877]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.01985718478444102 + 0 = 0.01985718478444102 \n",
      "Epoch 34,  mse = 0.01985718478444102\n",
      "predict energies =  [[-0.88740804664757145]\n",
      " [-0.52415893987700091]\n",
      " [1.1973302035369562]]\n",
      "gradients =  (1.5378460272136296e-05, -1.3841575569860944e-05)\n",
      "valid_ens[:3] = [-0.86893915 -0.64888878  1.08192877]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.01985728364824214 + 0 = 0.01985728364824214 \n",
      "Epoch 35,  mse = 0.01985728364824214\n",
      "predict energies =  [[-0.887425530659793]\n",
      " [-0.52431980265345857]\n",
      " [1.1975456360306105]]\n",
      "gradients =  (2.8245857934899865e-05, -2.54618559975768e-05)\n",
      "valid_ens[:3] = [-0.86893915 -0.64888878  1.08192877]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.01985752082759852 + 0 = 0.01985752082759852 \n",
      "Epoch 36,  mse = 0.01985752082759852\n",
      "predict energies =  [[-0.88744072396553109]\n",
      " [-0.52446473067984045]\n",
      " [1.19773659591783]]\n",
      "gradients =  (3.9699978138730413e-05, -3.5835990292909258e-05)\n",
      "valid_ens[:3] = [-0.86893915 -0.64888878  1.08192877]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.01985785172958888 + 0 = 0.01985785172958888 \n",
      "Epoch 37,  mse = 0.01985785172958888\n",
      "predict energies =  [[-0.88745382974694387]\n",
      " [-0.52459389965087433]\n",
      " [1.197904367540255]]\n",
      "gradients =  (4.9799245951963333e-05, -4.5006824205976264e-05)\n",
      "valid_ens[:3] = [-0.86893915 -0.64888878  1.08192877]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.019858238243789187 + 0 = 0.019858238243789187 \n",
      "Epoch 38,  mse = 0.019858238243789187\n",
      "predict energies =  [[-0.88746502960458662]\n",
      " [-0.52470758158812525]\n",
      " [1.1980501786849445]]\n",
      "gradients =  (5.860378015620088e-05, -5.3020412375094904e-05)\n",
      "valid_ens[:3] = [-0.86893915 -0.64888878  1.08192877]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.019858648654349442 + 0 = 0.019858648654349442 \n",
      "Epoch 39,  mse = 0.019858648654349442\n",
      "Epoch 40\n",
      "predict energies =  [[-0.88747448873305812]\n",
      " [-0.52480614328498709]\n",
      " [1.1981752306451696]]\n",
      "gradients =  (6.6174433544619049e-05, -5.9924904700216486e-05)\n",
      "valid_ens[:3] = [-0.86893915 -0.64888878  1.08192877]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.01985905667130145 + 0 = 0.01985905667130145 \n",
      "Epoch 40,  mse = 0.01985905667130145\n",
      "predict energies =  [[-0.88748236347935172]\n",
      " [-0.52489004692293961]\n",
      " [1.1982806839073106]]\n",
      "gradients =  (7.2573383026689614e-05, -6.5770980167227007e-05)\n",
      "valid_ens[:3] = [-0.86893915 -0.64888878  1.08192877]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.01985944174996458 + 0 = 0.01985944174996458 \n",
      "Epoch 41,  mse = 0.01985944174996458\n",
      "predict energies =  [[-0.88748879006468939]\n",
      " [-0.52495982034709443]\n",
      " [1.1983676932665841]]\n",
      "gradients =  (7.7862889611271524e-05, -7.0610743402531065e-05)\n",
      "valid_ens[:3] = [-0.86893915 -0.64888878  1.08192877]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.01985978810342172 + 0 = 0.01985978810342172 \n",
      "Epoch 42,  mse = 0.01985978810342172\n",
      "predict energies =  [[-0.88749389212642193]\n",
      " [-0.52501606491582409]\n",
      " [1.1984374083772786]]\n",
      "gradients =  (8.2105842135957174e-05, -7.4497848868561355e-05)\n",
      "valid_ens[:3] = [-0.86893915 -0.64888878  1.08192877]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.019860084351938513 + 0 = 0.019860084351938513 \n",
      "Epoch 43,  mse = 0.019860084351938513\n",
      "predict energies =  [[-0.88749777217829307]\n",
      " [-0.52505944830629347]\n",
      " [1.1984908970578891]]\n",
      "gradients =  (8.5365103300850953e-05, -7.7486940856066731e-05)\n",
      "valid_ens[:3] = [-0.86893915 -0.64888878  1.08192877]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.019860323209779127 + 0 = 0.019860323209779127 \n",
      "Epoch 44,  mse = 0.019860323209779127\n",
      "predict energies =  [[-0.88750054028405712]\n",
      " [-0.52509066087446232]\n",
      " [1.1985292410353856]]\n",
      "gradients =  (8.7703392379727407e-05, -7.9633478153355927e-05)\n",
      "valid_ens[:3] = [-0.86893915 -0.64888878  1.08192877]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.019860500642172527 + 0 = 0.019860500642172527 \n",
      "Epoch 45,  mse = 0.019860500642172527\n",
      "predict energies =  [[-0.88750228305481627]\n",
      " [-0.52511047017471024]\n",
      " [1.1985535061820483]]\n",
      "gradients =  (8.9183527951196273e-05, -8.0993274684302419e-05)\n",
      "valid_ens[:3] = [-0.86893915 -0.64888878  1.08192877]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.019860615591961908 + 0 = 0.019860615591961908 \n",
      "Epoch 46,  mse = 0.019860615591961908\n",
      "predict energies =  [[-0.887503090073527]\n",
      " [-0.52511964160214741]\n",
      " [1.1985647231382628]]\n",
      "gradients =  (8.9867228296722224e-05, -8.1622465902139661e-05)\n",
      "valid_ens[:3] = [-0.86893915 -0.64888878  1.08192877]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.0198606693552653 + 0 = 0.0198606693552653 \n",
      "Epoch 47,  mse = 0.0198606693552653\n",
      "predict energies =  [[-0.88750302516687962]\n",
      " [-0.525118982509101]\n",
      " [1.19856391059627]]\n",
      "gradients =  (8.9816135129577174e-05, -8.1577137662132339e-05)\n",
      "valid_ens[:3] = [-0.86893915 -0.64888878  1.08192877]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.01986066542945194 + 0 = 0.01986066542945194 \n",
      "Epoch 48,  mse = 0.01986066542945194\n",
      "predict energies =  [[-0.88750217955179245]\n",
      " [-0.52510930187837657]\n",
      " [1.1985520670623859]]\n",
      "gradients =  (8.9091308566890265e-05, -8.091325891366815e-05)\n",
      "valid_ens[:3] = [-0.86893915 -0.64888878  1.08192877]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.01986060867518957 + 0 = 0.01986060867518957 \n",
      "Epoch 49,  mse = 0.01986060867518957\n",
      "Epoch 50\n",
      "predict energies =  [[-0.88750060486695159]\n",
      " [-0.52509142481939486]\n",
      " [1.1985301634257473]]\n",
      "gradients =  (8.7752585910934322e-05, -7.9685946046217582e-05)\n",
      "valid_ens[:3] = [-0.86893915 -0.64888878  1.08192877]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.01986050489863008 + 0 = 0.01986050489863008 \n",
      "Epoch 50,  mse = 0.01986050489863008\n",
      "predict energies =  [[-0.88749836491984369]\n",
      " [-0.52506617093083652]\n",
      " [1.1984991450019689]]\n",
      "gradients =  (8.5858804045321063e-05, -7.7950063135876814e-05)\n",
      "valid_ens[:3] = [-0.86893915 -0.64888878  1.08192877]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.019860361059011986 + 0 = 0.019860361059011986 \n",
      "Epoch 51,  mse = 0.019860361059011986\n",
      "predict energies =  [[-0.88749553320016228]\n",
      " [-0.52503435076714633]\n",
      " [1.1984599656111385]]\n",
      "gradients =  (8.3467988939385507e-05, -7.5759122934258589e-05)\n",
      "valid_ens[:3] = [-0.86893915 -0.64888878  1.08192877]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.019860183978089686 + 0 = 0.019860183978089686 \n",
      "Epoch 52,  mse = 0.019860183978089686\n",
      "predict energies =  [[-0.88749214807527677]\n",
      " [-0.52499675961623171]\n",
      " [1.1984135237200666]]\n",
      "gradients =  (8.0636762405685708e-05, -7.3165913191243335e-05)\n",
      "valid_ens[:3] = [-0.86893915 -0.64888878  1.08192877]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.01985998100348222 + 0 = 0.01985998100348222 \n",
      "Epoch 53,  mse = 0.01985998100348222\n",
      "predict energies =  [[-0.8874882755753345]\n",
      " [-0.524954190820957]\n",
      " [1.198360695796187]]\n",
      "gradients =  (7.7420585042040774e-05, -7.0221938717986431e-05)\n",
      "valid_ens[:3] = [-0.86893915 -0.64888878  1.08192877]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.019859759363587748 + 0 = 0.019859759363587748 \n",
      "Epoch 54,  mse = 0.019859759363587748\n",
      "predict energies =  [[-0.88748396982860789]\n",
      " [-0.524907394836587]\n",
      " [1.1983023640617931]]\n",
      "gradients =  (7.3873178026526154e-05, -6.6977014195141e-05)\n",
      "valid_ens[:3] = [-0.86893915 -0.64888878  1.08192877]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.01985952569766984 + 0 = 0.01985952569766984 \n",
      "Epoch 55,  mse = 0.01985952569766984\n",
      "predict energies =  [[-0.8874792917155796]\n",
      " [-0.524857092345555]\n",
      " [1.198239372133862]]\n",
      "gradients =  (7.00463749025021e-05, -6.3479607859109462e-05)\n",
      "valid_ens[:3] = [-0.86893915 -0.64888878  1.08192877]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.01985928646595216 + 0 = 0.01985928646595216 \n",
      "Epoch 56,  mse = 0.01985928646595216\n",
      "predict energies =  [[-0.88747428265239692]\n",
      " [-0.52480399788059107]\n",
      " [1.1981725179938412]]\n",
      "gradients =  (6.5990753164411192e-05, -5.9776421022555576e-05)\n",
      "valid_ens[:3] = [-0.86893915 -0.64888878  1.08192877]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.019859047359802904 + 0 = 0.019859047359802904 \n",
      "Epoch 57,  mse = 0.019859047359802904\n",
      "predict energies =  [[-0.88746901539875744]\n",
      " [-0.52474877208309667]\n",
      " [1.1981025823606326]]\n",
      "gradients =  (6.1754526851649783e-05, -5.5911885627737623e-05)\n",
      "valid_ens[:3] = [-0.86893915 -0.64888878  1.08192877]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.019858813302253916 + 0 = 0.019858813302253916 \n",
      "Epoch 58,  mse = 0.019858813302253916\n",
      "predict energies =  [[-0.88746351617394315]\n",
      " [-0.52469203001615883]\n",
      " [1.1980303414440558]]\n",
      "gradients =  (5.7384213962043129e-05, -5.1928947838271725e-05)\n",
      "valid_ens[:3] = [-0.86893915 -0.64888878  1.08192877]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.019858588700509088 + 0 = 0.019858588700509088 \n",
      "Epoch 59,  mse = 0.019858588700509088\n",
      "Epoch 60\n",
      "predict energies =  [[-0.887457859916082]\n",
      " [-0.52463437444076155]\n",
      " [1.1979564861578873]]\n",
      "gradients =  (5.2923514947050138e-05, -4.7867743515666652e-05)\n",
      "valid_ens[:3] = [-0.86893915 -0.64888878  1.08192877]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.019858376904646202 + 0 = 0.019858376904646202 \n",
      "Epoch 60,  mse = 0.019858376904646202\n",
      "predict energies =  [[-0.88745207597664189]\n",
      " [-0.52457634898315586]\n",
      " [1.1978817170711262]]\n",
      "gradients =  (4.8414174774986246e-05, -4.3766646178577457e-05)\n",
      "valid_ens[:3] = [-0.86893915 -0.64888878  1.08192877]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.019858180775987708 + 0 = 0.019858180775987708 \n",
      "Epoch 61,  mse = 0.019858180775987708\n",
      "predict energies =  [[-0.88744622859704281]\n",
      " [-0.52451846659114909]\n",
      " [1.1978066703232755]]\n",
      "gradients =  (4.389539226097236e-05, -3.9661146545253659e-05)\n",
      "valid_ens[:3] = [-0.86893915 -0.64888878  1.08192877]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.019858002130982273 + 0 = 0.019858002130982273 \n",
      "Epoch 62,  mse = 0.019858002130982273\n",
      "predict energies =  [[-0.88744036298662121]\n",
      " [-0.524461188440816]\n",
      " [1.1977319653551732]]\n",
      "gradients =  (3.9403816505415717e-05, -3.5584610218848775e-05)\n",
      "valid_ens[:3] = [-0.86893915 -0.64888878  1.08192877]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.019857842324645807 + 0 = 0.019857842324645807 \n",
      "Epoch 63,  mse = 0.019857842324645807\n",
      "predict energies =  [[-0.88743451816826091]\n",
      " [-0.52440493000619337]\n",
      " [1.1976581549837522]]\n",
      "gradients =  (3.4973284558103418e-05, -3.1567405649899689e-05)\n",
      "valid_ens[:3] = [-0.86893915 -0.64888878  1.08192877]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.01985770179148112 + 0 = 0.01985770179148112 \n",
      "Epoch 64,  mse = 0.01985770179148112\n",
      "predict energies =  [[-0.8874287540032898]\n",
      " [-0.52435008555248575]\n",
      " [1.1975857744865697]]\n",
      "gradients =  (3.063486095212023e-05, -2.7637822684509807e-05)\n",
      "valid_ens[:3] = [-0.86893915 -0.64888878  1.08192877]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.019857580789316733 + 0 = 0.019857580789316733 \n",
      "Epoch 65,  mse = 0.019857580789316733\n",
      "predict energies =  [[-0.88742308979113438]\n",
      " [-0.524296984019582]\n",
      " [1.1975152964182449]]\n",
      "gradients =  (2.6416722414931287e-05, -2.3820977254953605e-05)\n",
      "valid_ens[:3] = [-0.86893915 -0.64888878  1.08192877]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.01985747876148745 + 0 = 0.01985747876148745 \n",
      "Epoch 66,  mse = 0.01985747876148745\n",
      "predict energies =  [[-0.88741758235327106]\n",
      " [-0.52424592106371981]\n",
      " [1.1974471507166469]]\n",
      "gradients =  (2.2344302784204489e-05, -2.013936636013256e-05)\n",
      "valid_ens[:3] = [-0.86893915 -0.64888878  1.08192877]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.019857394690532784 + 0 = 0.019857394690532784 \n",
      "Epoch 67,  mse = 0.019857394690532784\n",
      "predict energies =  [[-0.88741226261349315]\n",
      " [-0.52419716116281234]\n",
      " [1.1973817213589237]]\n",
      "gradients =  (1.8439951763992574e-05, -1.6612930059531041e-05)\n",
      "valid_ens[:3] = [-0.86893915 -0.64888878  1.08192877]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.01985732739854913 + 0 = 0.01985732739854913 \n",
      "Epoch 68,  mse = 0.01985732739854913\n",
      "predict energies =  [[-0.88740715763826128]\n",
      " [-0.52415090950317167]\n",
      " [1.1973193554724464]]\n",
      "gradients =  (1.4723161147593006e-05, -1.3258745859606515e-05)\n",
      "valid_ens[:3] = [-0.86893915 -0.64888878  1.08192877]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.019857275349367237 + 0 = 0.019857275349367237 \n",
      "Epoch 69,  mse = 0.019857275349367237\n",
      "Epoch 70\n",
      "predict energies =  [[-0.88740229892906064]\n",
      " [-0.52410735527384944]\n",
      " [1.1972603390298726]]\n",
      "gradients =  (1.1210667947209041e-05, -1.0091653239337392e-05)\n",
      "valid_ens[:3] = [-0.86893915 -0.64888878  1.08192877]\n",
      "Adding mse_ens_j_i to mse_ens_j: 0.019857237195850263 + 0 = 0.019857237195850263 \n",
      "Epoch 70,  mse = 0.019857237195850263\n"
     ]
    }
   ],
   "source": [
    "# Initialize kernels and model hyperparameters\n",
    "tf.random.set_seed(s.tf_seed)\n",
    "\n",
    "TimeBeforePreEpoch = time.time()\n",
    "\n",
    "\n",
    "noise_init = 1e-4 #.001# 0.0005499093576274776 #1.625e-4\n",
    "obs_noise = tf.Variable(noise_init, dtype=s.dtype, name=\"noise\")\n",
    "\n",
    "degree=2\n",
    "kernel = pick_kernel(s.kernel_type, amplitude=1, verbose=s.verbose, degree=degree)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Split training data into training and validation sets\n",
    "# Now validation set acts as temporary est set\n",
    "# train_test_split and tensorflow tensors don't get along so I temporarily convert them back to numpy arrays\n",
    "\n",
    "train_indices_j, valid_indices_j  = train_test_split(np.arange(len(train_sps)), random_state = s.valid_split_seed, test_size=(1-s.valid_fract))\n",
    "\n",
    "train_sps_j, valid_sps_j = train_sps[train_indices_j], train_sps[valid_indices_j]\n",
    "train_ens_j, valid_ens_j = train_ens[train_indices_j], train_ens[valid_indices_j]\n",
    "\n",
    "if s.use_forces: \n",
    "    train_dsp_dx_j, valid_dsp_dx_j = train_dsp_dx[train_indices_j], train_dsp_dx[valid_indices_j]\n",
    "    train_frcs_j, valid_frcs_j = train_frcs[train_indices_j], train_frcs[valid_indices_j]\n",
    "\n",
    "# Convert to tensorflow constant tensors\n",
    "# train_sps_j = tf.constant(train_sps_j, dtype=s.dtype)\n",
    "# train_ens_j = tf.constant(train_ens_j, dtype=s.dtype)\n",
    "valid_sps_j = tf.constant(valid_sps_j, dtype=s.dtype)\n",
    "# valid_ens_j = tf.constant(valid_ens_j, dtype=s.dtype)\n",
    "\n",
    "if s.sparse_gpflow:\n",
    "    sparse_train_sps = tf.Variable(sparse_train_sps, shape=sparse_train_sps.shape, dtype=s.dtype, trainable=False)\n",
    "\n",
    "if s.use_forces:\n",
    "    train_dsp_dx_j = tf.constant(train_dsp_dx_j, dtype=s.dtype)\n",
    "    train_frcs_j = tf.constant(train_frcs_j, dtype=s.dtype)    \n",
    "    valid_dsp_dx_j = tf.constant(valid_dsp_dx_j, dtype=s.dtype)\n",
    "    valid_frcs_j = tf.constant(valid_frcs_j, dtype=s.dtype)        \n",
    "\n",
    "test_sps = tf.constant(test_sps, dtype=s.dtype)\n",
    "\n",
    "\n",
    "# Batch data if  training set is larger than batch_size_max\n",
    "if len(train_sps_j) < s.batch_size_max:\n",
    "    iterations_per_epoch = 1\n",
    "    batch_size = len(train_sps_j)\n",
    "    if s.verbose:\n",
    "        print(\"Training using {} atoms without batching.\".format(len(train_sps_j)))\n",
    "else:\n",
    "    iterations_per_epoch = int(np.ceil(len(train_sps_j)/s.batch_size_max))\n",
    "    batch_size = int(np.ceil(len(train_sps_j)/iterations_per_epoch))\n",
    "    if s.verbose:\n",
    "        print(\"Training using {} atoms total using {} batches with {} atoms per batch.\".format( len(train_sps_j), iterations_per_epoch, batch_size ))\n",
    "\n",
    "# training(out_data)\n",
    "        \n",
    "TimeBeforeEpoch0 = time.time()\n",
    "\n",
    "\n",
    "mse_history = []    \n",
    "hyperparam_history = []\n",
    "\n",
    "\n",
    "\n",
    "# import warnings\n",
    "# \n",
    "# from gpflow.models import VGP, GPR, SGPR, SVGP\n",
    "# from gpflow.optimizers import NaturalGradient\n",
    "# from gpflow.optimizers.natgrad import XiSqrtMeanVar\n",
    "# from gpflow import set_trainable\n",
    "\n",
    "\n",
    "if s.my_priority == \"efficiency\":\n",
    "    # I don't know what this does\n",
    "    autotune = tf.data.experimental.AUTOTUNE\n",
    "    \n",
    "    \n",
    "    batches = (\n",
    "        tf.data.Dataset.from_tensor_slices((train_sps_j, train_ens_j))\n",
    "        .prefetch(autotune) \n",
    "        .shuffle(buffer_size=len(train_sps_j), seed=s.shuffle_seed)\n",
    "        .repeat(count=None)\n",
    "        .batch(batch_size)\n",
    "    )\n",
    "    \n",
    "    batch_iterator = iter(batches)\n",
    "\n",
    "    # I also don't know why we use this\n",
    "    from gpflow.ci_utils import ci_niter, ci_range\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=s.learn_rate)\n",
    "    \n",
    "else:\n",
    "    batches = (\n",
    "        tf.data.Dataset.from_tensor_slices((train_sps_j, train_ens_j)) \n",
    "        .shuffle(buffer_size=len(train_sps_j), seed=s.shuffle_seed) \n",
    "        .repeat(count=None)\n",
    "        .batch(batch_size)\n",
    "    )    \n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=s.learn_rate)\n",
    "#     optimizer = tf.keras.optimizers.SGD(learning_rate=s.learn_rate)\n",
    "    \n",
    "train_hyperparams_without_forces_tf = tf.function(train_hyperparams_without_forces, autograph=False, jit_compile=False)\n",
    "\n",
    "# new code to make tf.function training work\n",
    "# --------------------------------------------\n",
    "train_sps_j_i = tf.Variable(train_sps[:batch_size], shape=(batch_size, train_sps.shape[-1]), dtype=s.dtype, trainable=False )\n",
    "train_ens_j_i = tf.Variable(train_ens[:batch_size], shape=(batch_size, 1), dtype=s.dtype, trainable=False ) \n",
    "if s.sparse_gpflow:\n",
    "    if sparse_train_sps.shape[0] >= batch_size:\n",
    "        print(\"Warning: Batch size is not greater than sparse soap size.\\nThis may cause errors in the predict_f function which assumes the inducing points to be fewer than the data points.\")\n",
    "    if s.my_priority == \"efficiency\":\n",
    "        gpr_model = gpflow.models.SVGP( kernel=kernel, likelihood=gpflow.likelihoods.Gaussian(),  inducing_variable=sparse_train_sps)\n",
    "        gpr_model.likelihood.variance.assign(obs_noise)\n",
    "        gpflow.set_trainable(gpr_model.q_mu, False)\n",
    "        gpflow.set_trainable(gpr_model.q_sqrt, False)\n",
    "    else:\n",
    "        gpr_model = gpflow.models.SGPR(data=(train_sps_j_i, train_ens_j_i), kernel=kernel, noise_variance=obs_noise, inducing_variable=sparse_train_sps)\n",
    "else:\n",
    "    if s.my_priority == \"efficiency\":\n",
    "        # it seems I cannot use  noise_variance=obs_noise for this which makes it not GAP...\n",
    "        gpr_model = gpflow.models.VGP( data=(train_sps_j_i, train_ens_j_i), kernel=kernel, likelihood=gpflow.likelihoods.Gaussian())\n",
    "    else:\n",
    "        gpr_model = gpflow.models.GPR( data=(train_sps_j_i, train_ens_j_i), kernel=kernel, noise_variance=obs_noise)\n",
    "# --------------------------------------------\n",
    "\n",
    "\n",
    "print_frequency = max(s.min_print_frequency, int(s.n_epochs/10))\n",
    "\n",
    "if s.my_priority == \"efficiency\":\n",
    "    hyperparam_history.append([(0, np.exp(var.numpy() )) for var in gpr_model.trainable_variables])  \n",
    "    gpr_objective = gpr_model.training_loss_closure(batch_iterator,  compile=True)\n",
    "    for j in range(ci_niter(s.n_epochs)):\n",
    "        if not j % print_frequency:\n",
    "            print(\"Epoch {}\".format(j))\n",
    "        optimizer.minimize(gpr_objective, var_list=gpr_model.trainable_variables)\n",
    "        mse_history.append((j+1, gpr_model.elbo(data=(train_sps, train_ens))))\n",
    "        hyperparam_history.append([(j+1, np.exp(var.numpy()) ) for var in gpr_model.trainable_variables]) \n",
    "    #optimizer.minimize(gpr_model.training_loss, gpr_model.trainable_variables, options=dict(maxiter=s.n_epochs))\n",
    "        \n",
    "elif s.my_priority == \"consistency\":\n",
    "\n",
    "    hyperparam_history.append([(0, var.numpy()) for var in gpr_model.trainable_parameters])  \n",
    "    for j in range(s.n_epochs):\n",
    "        if not j % print_frequency:\n",
    "            print(\"Epoch {}\".format(j))\n",
    "            #print(\" \".join([\"{} = {:.2e} \".format(var.name, np.exp(var.numpy())) for var in trainable_variables]))\n",
    "\n",
    "        mse_ens_j = 0\n",
    "        for i, (train_sps_j_i, train_ens_j_i) in enumerate(islice(batches, iterations_per_epoch)):\n",
    "            if not s.use_forces: #and not s.sparse_gpflow :\n",
    "                gpr_model.data[0].assign(train_sps_j_i)\n",
    "                gpr_model.data[1].assign(train_ens_j_i)        \n",
    "                mse_ens_j_i = train_hyperparams_without_forces_tf(gpr_model, valid_sps_j, valid_ens_j, optimizer)\n",
    "                print(\"valid_ens[:3] = {}\".format( valid_ens_j[:3].flatten()) )\n",
    "#                 print(mse_ens_j_i.numpy(), valid_ens_j[:3].numpy().flatten(), train_ens_j_i[:3].numpy().flatten()  )\n",
    "\n",
    "\n",
    "            else:\n",
    "                print(\"Using older approach (not converted to tf.function yet)\")\n",
    "                with tf.GradientTape() as tape:\n",
    "                    with tf.GradientTape(watch_accessed_variables=False) as tape_sps:\n",
    "                        tape_sps.watch(valid_sps_j)\n",
    "                        if s.sparse_gpflow:\n",
    "                            gpr_model = gpflow.models.SGPR(data=(train_sps_j_i, train_ens_j_i), kernel=kernel, inducing_variable=sparse_train_sps)\n",
    "    #                         gpflow.set_trainable(gpr_model.inducing_variable, False)\n",
    "                            if i < 3:\n",
    "                                print_summary(gpr_model)            \n",
    "                        else:\n",
    "                            gpr_model.data[0].assign(train_sps_j_i)\n",
    "                            gpr_model.data[1].assign(train_ens_j_i)\n",
    "                            #gpr_model = gpflow.models.GPR(data=(train_sps_j_i, train_ens_j_i), kernel=kernel)\n",
    "                        #gpr_model.likelihood.variance.assign(obs_noise)                \n",
    "                        predict_ens_j_i = gpr_model.predict_f(valid_sps_j)[0]\n",
    "\n",
    "        #                 gpr_model = gpflow.models.GPR(data=(sps_j_i, train_ens_j_i), kernel=kernel_gpf)\n",
    "        #                 gpr_model.likelihood.variance.assign(obs_noise_gpf)\n",
    "        #                 predict_ens_j_i_gpf = gpr_model.predict_f(valid_sps_j)\n",
    "\n",
    "                    if s.use_forces:\n",
    "                        predict_d_ens_j_i = tape_sps.gradient(predict_ens_j_i, valid_sps_j)\n",
    "                        # In the following line I needed to include '* n_atoms' after breaking energies into local energies\n",
    "                        # The reason is that I am effectively breaking the connection between E and F when doing that\n",
    "                        # F = -dE/dx =/= -dE_local/dx where E_local = E/n_atoms - E_free\n",
    "                        # When I split energies into local energies I initially calculated -dE_local/dx which is -dE/dx / n_atoms\n",
    "                        # This fix is prone to breaking the code and is not robust to systems with different structure size\n",
    "                        # Need to improve this with a better fix\n",
    "                        predict_frcs_j_i = -1*np.einsum('ijk,ik->ij', valid_dsp_dx_j, predict_d_ens_j_i) * n_atoms\n",
    "                        mse_j_i = mse_2factor_tf(predict_ens_j_i, valid_ens_j, 1/ens_var,\n",
    "                                                predict_frcs_j_i, valid_frcs_j, 1/frcs_var)\n",
    "                        mse_ens_j_i = mse_tf(predict_ens_j_i, valid_ens_j)\n",
    "                    else:\n",
    "                        mse_j_i = mse_tf(predict_ens_j_i, valid_ens_j)\n",
    "                        mse_ens_j_i = mse_j_i\n",
    "\n",
    "\n",
    "        #         grads = tape.gradient(mse_j_i, trainable_variables)\n",
    "        #         optimizer.apply_gradients(zip(grads, trainable_variables))\n",
    "                grads = tape.gradient(mse_j_i, gpr_model.trainable_variables)\n",
    "                # print(gpr_model.trainable_variables[0])#grads[0])\n",
    "                optimizer.apply_gradients(zip(grads, gpr_model.trainable_variables))\n",
    "                if i < 3:\n",
    "                    print_summary(gpr_model)\n",
    "\n",
    "                if not gpr_model.data[0][0,0].numpy() == train_sps_j_i[0,0].numpy() :\n",
    "                    print(\"ERRORERRORERRORERRORERRORERRORERROR\")\n",
    "\n",
    "            print(\"Adding mse_ens_j_i to mse_ens_j: {} + {} = {} \".format(mse_ens_j_i.numpy(), mse_ens_j , mse_ens_j_i.numpy() + mse_ens_j  ))\n",
    "            mse_ens_j += mse_ens_j_i\n",
    "\n",
    "        mse_ens_j /= iterations_per_epoch\n",
    "        print(\"Epoch {},  mse = {}\".format(j, mse_ens_j))\n",
    "        mse_history.append((j+1, mse_ens_j))\n",
    "        hyperparam_history.append([(j+1, var.numpy()) for var in gpr_model.trainable_parameters])    \n",
    "else:\n",
    "    print(\"{} is not a reconized value for my_priority.\\n Training did not occur.\".format(s.my_priority))\n",
    "\n",
    "\n",
    "\n",
    "TimeBeforeWeights = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am currently (11/30) converting the hyperparameter learning in this cell into a function\n",
    "# Next will be the post-hyperparameter part of the learning (in the next cell)\n",
    "\n",
    "# # Initialize kernels and model hyperparameters\n",
    "\n",
    "\n",
    "\n",
    "# def train_hyperparams(train_sps, train_ens, sparse_train_sps, kernel, settings):\n",
    "#     tf.random.set_seed(settings.tf_seed)\n",
    "\n",
    "\n",
    "#     noise_init = 1e-4 #.001# 0.0005499093576274776 #1.625e-4\n",
    "#     obs_noise = tf.Variable(noise_init, dtype=settings.dtype, name=\"noise\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     # Split training data into training and validation sets\n",
    "#     # Now validation set acts as temporary est set\n",
    "#     # train_test_split and tensorflow tensors don't get along so I temporarily convert them back to numpy arrays\n",
    "\n",
    "#     train_indices_j, valid_indices_j  = train_test_split(np.arange(len(train_sps)), random_state = settings.valid_split_seed, test_size=(1-settings.valid_fract))\n",
    "\n",
    "#     train_sps_j, valid_sps_j = train_sps[train_indices_j], train_sps[valid_indices_j]\n",
    "#     train_ens_j, valid_ens_j = train_ens[train_indices_j], train_ens[valid_indices_j]\n",
    "\n",
    "#     if settings.use_forces: \n",
    "#         train_dsp_dx_j, valid_dsp_dx_j = train_dsp_dx[train_indices_j], train_dsp_dx[valid_indices_j]\n",
    "#         train_frcs_j, valid_frcs_j = train_frcs[train_indices_j], train_frcs[valid_indices_j]\n",
    "\n",
    "#     # Convert to tensorflow constant tensors\n",
    "#     train_sps_j = tf.constant(train_sps_j, dtype=settings.dtype)\n",
    "#     train_ens_j = tf.constant(train_ens_j, dtype=settings.dtype)\n",
    "#     valid_sps_j = tf.constant(valid_sps_j, dtype=settings.dtype)\n",
    "#     valid_ens_j = tf.constant(valid_ens_j, dtype=settings.dtype)\n",
    "#     if settings.sparse_gpflow:\n",
    "#         sparse_train_sps = tf.Variable(sparse_train_sps, shape=sparse_train_spsettings.shape, dtype=settings.dtype, trainable=False)\n",
    "\n",
    "#     if settings.use_forces:\n",
    "#         train_dsp_dx_j = tf.constant(train_dsp_dx_j, dtype=settings.dtype)\n",
    "#         train_frcs_j = tf.constant(train_frcs_j, dtype=settings.dtype)    \n",
    "#         valid_dsp_dx_j = tf.constant(valid_dsp_dx_j, dtype=settings.dtype)\n",
    "#         valid_frcs_j = tf.constant(valid_frcs_j, dtype=settings.dtype)        \n",
    "\n",
    "#     test_sps = tf.constant(test_sps, dtype=settings.dtype)\n",
    "\n",
    "\n",
    "#     # Batch data if  training set is larger than batch_size_max\n",
    "#     if len(train_sps_j) < settings.batch_size_max:\n",
    "#         iterations_per_epoch = 1\n",
    "#         batch_size = len(train_sps_j)\n",
    "#         if s.verbose:\n",
    "#             print(\"Training using {} atoms without batching.\".format(len(train_sps_j)))\n",
    "#     else:\n",
    "#         iterations_per_epoch = int(np.ceil(len(train_sps_j)/settings.batch_size_max))\n",
    "#         batch_size = int(np.ceil(len(train_sps_j)/iterations_per_epoch))\n",
    "#         if s.verbose:\n",
    "#             print(\"Training using {} atoms total using {} batches with {} atoms per batch.\".format( len(train_sps_j), iterations_per_epoch, batch_size ))\n",
    "\n",
    "#     # training(out_data)\n",
    "\n",
    "#     TimeBeforeEpoch0 = time.time()\n",
    "\n",
    "\n",
    "#     mse_history = []    \n",
    "#     hyperparam_history = []\n",
    "\n",
    "\n",
    "\n",
    "#     # import warnings\n",
    "#     # \n",
    "#     # from gpflow.models import VGP, GPR, SGPR, SVGP\n",
    "#     # from gpflow.optimizers import NaturalGradient\n",
    "#     # from gpflow.optimizers.natgrad import XiSqrtMeanVar\n",
    "#     # from gpflow import set_trainable\n",
    "\n",
    "\n",
    "#     if settings.my_priority == \"efficiency\":\n",
    "#         # I don't know what this does\n",
    "#         autotune = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "\n",
    "#         batches = (\n",
    "#             tf.data.Dataset.from_tensor_slices((train_sps_j, train_ens_j))\n",
    "#             .prefetch(autotune) \n",
    "#             .shuffle(buffer_size=len(train_sps_j), seed=settings.shuffle_seed)\n",
    "#             .repeat(count=None)\n",
    "#             .batch(batch_size)\n",
    "#         )\n",
    "\n",
    "#         batch_iterator = iter(batches)\n",
    "\n",
    "#         # I also don't know why we use this\n",
    "#         from gpflow.ci_utils import ci_niter, ci_range\n",
    "\n",
    "#         optimizer = tf.keras.optimizers.Adam(learning_rate=settings.learn_rate)\n",
    "\n",
    "#     else:\n",
    "#         batches = (\n",
    "#             tf.data.Dataset.from_tensor_slices((train_sps_j, train_ens_j)) \n",
    "#             .shuffle(buffer_size=len(train_sps_j), seed=settings.shuffle_seed) \n",
    "#             .repeat(count=None)\n",
    "#             .batch(batch_size)\n",
    "#         )    \n",
    "\n",
    "#         optimizer = tf.keras.optimizers.Adam(learning_rate=settings.learn_rate)\n",
    "#         #optimizer = tf.keras.optimizers.SGD(learning_rate=settings.learn_rate)\n",
    "\n",
    "\n",
    "\n",
    "#     # new code to make tf.function training work\n",
    "#     # --------------------------------------------\n",
    "#     train_sps_j_i = tf.Variable(train_sps[:batch_size], shape=(batch_size, train_sps.shape[-1]), dtype=settings.dtype, trainable=False )\n",
    "#     train_ens_j_i = tf.Variable(train_ens[:batch_size], shape=(batch_size, 1), dtype=settings.dtype, trainable=False ) \n",
    "#     if settings.sparse_gpflow:\n",
    "#         if sparse_train_sps.shape[0] >= batch_size:\n",
    "#             print(\"Warning: Batch size is not greater than sparse soap size.\\nThis may cause errors in the predict_f function which assumes the inducing points to be fewer than the data points.\")\n",
    "#         if settings.my_priority == \"efficiency\":\n",
    "#             gpr_model = gpflow.models.SVGP( kernel=kernel, likelihood=gpflow.likelihoods.Gaussian(),  inducing_variable=sparse_train_sps)\n",
    "#             gpr_model.likelihood.variance.assign(obs_noise)\n",
    "#             gpflow.set_trainable(gpr_model.q_mu, False)\n",
    "#             gpflow.set_trainable(gpr_model.q_sqrt, False)\n",
    "#         else:\n",
    "#             gpr_model = gpflow.models.SGPR(data=(train_sps_j_i, train_ens_j_i), kernel=kernel, noise_variance=obs_noise, inducing_variable=sparse_train_sps)\n",
    "#     else:\n",
    "#         if settings.my_priority == \"efficiency\":\n",
    "#             # it seems I cannot use  noise_variance=obs_noise for this which makes it not GAP...\n",
    "#             gpr_model = gpflow.models.VGP( data=(train_sps_j_i, train_ens_j_i), kernel=kernel, likelihood=gpflow.likelihoods.Gaussian())\n",
    "#         else:\n",
    "#             gpr_model = gpflow.models.GPR( data=(train_sps_j_i, train_ens_j_i), kernel=kernel, noise_variance=obs_noise)\n",
    "#     # --------------------------------------------\n",
    "\n",
    "\n",
    "#     print_frequency = max(settings.min_print_frequency, int(settings.n_epochs/10))\n",
    "\n",
    "#     if settings.my_priority == \"efficiency\":\n",
    "#         hyperparam_history.append([(0, np.exp(var.numpy() )) for var in gpr_model.trainable_variables])  \n",
    "#         gpr_objective = gpr_model.training_loss_closure(batch_iterator,  compile=True)\n",
    "#         for j in range(ci_niter(settings.n_epochs)):\n",
    "#             if not j % print_frequency:\n",
    "#                 print(\"Epoch {}\".format(j))\n",
    "#             optimizer.minimize(gpr_objective, var_list=gpr_model.trainable_variables)\n",
    "#             mse_history.append((j+1, gpr_model.elbo(data=(train_sps, train_ens))))\n",
    "#             hyperparam_history.append([(j+1, np.exp(var.numpy()) ) for var in gpr_model.trainable_variables]) \n",
    "#         #optimizer.minimize(gpr_model.training_loss, gpr_model.trainable_variables, options=dict(maxiter=settings.n_epochs))\n",
    "\n",
    "#     elif settings.my_priority == \"consistency\":\n",
    "\n",
    "#         hyperparam_history.append([(0, var.numpy()) for var in gpr_model.trainable_parameters])  \n",
    "#         for j in range(settings.n_epochs):\n",
    "#             if not j % print_frequency:\n",
    "#                 print(\"Epoch {}\".format(j))\n",
    "#                 #print(\" \".join([\"{} = {:.2e} \".format(var.name, np.exp(var.numpy())) for var in trainable_variables]))\n",
    "\n",
    "#             mse_ens_j = 0\n",
    "#             for i, (train_sps_j_i, train_ens_j_i) in enumerate(islice(batches, iterations_per_epoch)):\n",
    "#                 if not settings.use_forces: #and not settings.sparse_gpflow :\n",
    "#                     gpr_model.data[0].assign(train_sps_j_i)\n",
    "#                     gpr_model.data[1].assign(train_ens_j_i)        \n",
    "#                     mse_ens_j_i = train_hyperparams_without_forces_tf(gpr_model, valid_sps_j, valid_ens_j)\n",
    "#                     print(\"valid_ens[:3] = {}\".format( valid_ens_j[:3].numpy().flatten()) )\n",
    "#     #                 print(mse_ens_j_i.numpy(), valid_ens_j[:3].numpy().flatten(), train_ens_j_i[:3].numpy().flatten()  )\n",
    "\n",
    "\n",
    "#                 else:\n",
    "#                     print(\"Using older approach (not converted to tf.function yet)\")\n",
    "#                     with tf.GradientTape() as tape:\n",
    "#                         with tf.GradientTape(watch_accessed_variables=False) as tape_sps:\n",
    "#                             tape_sps.watch(valid_sps_j)\n",
    "#                             if settings.sparse_gpflow:\n",
    "#                                 gpr_model = gpflow.models.SGPR(data=(train_sps_j_i, train_ens_j_i), kernel=kernel, inducing_variable=sparse_train_sps)\n",
    "#         #                         gpflow.set_trainable(gpr_model.inducing_variable, False)\n",
    "#                                 if i < 3:\n",
    "#                                     print_summary(gpr_model)            \n",
    "#                             else:\n",
    "#                                 gpr_model.data[0].assign(train_sps_j_i)\n",
    "#                                 gpr_model.data[1].assign(train_ens_j_i)\n",
    "#                                 #gpr_model = gpflow.models.GPR(data=(train_sps_j_i, train_ens_j_i), kernel=kernel)\n",
    "#                             #gpr_model.likelihood.variance.assign(obs_noise)                \n",
    "#                             predict_ens_j_i = gpr_model.predict_f(valid_sps_j)[0]\n",
    "\n",
    "#             #                 gpr_model = gpflow.models.GPR(data=(sps_j_i, train_ens_j_i), kernel=kernel_gpf)\n",
    "#             #                 gpr_model.likelihood.variance.assign(obs_noise_gpf)\n",
    "#             #                 predict_ens_j_i_gpf = gpr_model.predict_f(valid_sps_j)\n",
    "\n",
    "#                         if settings.use_forces:\n",
    "#                             predict_d_ens_j_i = tape_sps.gradient(predict_ens_j_i, valid_sps_j)\n",
    "#                             # In the following line I needed to include '* n_atoms' after breaking energies into local energies\n",
    "#                             # The reason is that I am effectively breaking the connection between E and F when doing that\n",
    "#                             # F = -dE/dx =/= -dE_local/dx where E_local = E/n_atoms - E_free\n",
    "#                             # When I split energies into local energies I initially calculated -dE_local/dx which is -dE/dx / n_atoms\n",
    "#                             # This fix is prone to breaking the code and is not robust to systems with different structure size\n",
    "#                             # Need to improve this with a better fix\n",
    "#                             predict_frcs_j_i = -1*np.einsum('ijk,ik->ij', valid_dsp_dx_j, predict_d_ens_j_i) * n_atoms\n",
    "#                             mse_j_i = mse_2factor_tf(predict_ens_j_i, valid_ens_j, 1/ens_var,\n",
    "#                                                     predict_frcs_j_i, valid_frcs_j, 1/frcs_var)\n",
    "#                             mse_ens_j_i = mse_tf(predict_ens_j_i, valid_ens_j)\n",
    "#                         else:\n",
    "#                             mse_j_i = mse_tf(predict_ens_j_i, valid_ens_j)\n",
    "#                             mse_ens_j_i = mse_j_i\n",
    "\n",
    "\n",
    "#             #         grads = tape.gradient(mse_j_i, trainable_variables)\n",
    "#             #         optimizer.apply_gradients(zip(grads, trainable_variables))\n",
    "#                     grads = tape.gradient(mse_j_i, gpr_model.trainable_variables)\n",
    "#                     # print(gpr_model.trainable_variables[0])#grads[0])\n",
    "#                     optimizer.apply_gradients(zip(grads, gpr_model.trainable_variables))\n",
    "#                     if i < 3:\n",
    "#                         print_summary(gpr_model)\n",
    "\n",
    "#                     if not gpr_model.data[0][0,0].numpy() == train_sps_j_i[0,0].numpy() :\n",
    "#                         print(\"ERRORERRORERRORERRORERRORERRORERROR\")\n",
    "\n",
    "#                 print(\"Adding mse_ens_j_i to mse_ens_j: {} + {} = {} \".format(mse_ens_j_i.numpy(), mse_ens_j , mse_ens_j_i.numpy() + mse_ens_j  ))\n",
    "#                 mse_ens_j += mse_ens_j_i\n",
    "\n",
    "#             mse_ens_j /= iterations_per_epoch\n",
    "#             print(\"Epoch {},  mse = {}\".format(j, mse_ens_j))\n",
    "#             mse_history.append((j+1, mse_ens_j))\n",
    "#             hyperparam_history.append([(j+1, var.numpy()) for var in gpr_model.trainable_parameters])    \n",
    "#     else:\n",
    "#         print(\"{} is not a reconized value for my_priority.\\n Training did not occur.\".format(settings.my_priority))\n",
    "    \n",
    "#     return gpr_model\n",
    "\n",
    "\n",
    "\n",
    "# TimeBeforeWeights = time.time()\n",
    "# train_hyperparams(train_sps, train_ens, sparse_train_sps, kernel=kernel, settings=s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "TimeBeforeWeights = time.time()\n",
    "print(\"Calculating weights\")\n",
    "\n",
    "if s.my_priority == \"efficiency\" and s.sparse_gpflow == True:\n",
    "    gpr_model =gpr_model\n",
    "elif s.my_priority == \"consistency\":\n",
    "    if s.sparse_gpflow:\n",
    "        gpr_model = gpflow.models.SGPR(data=(train_sps, train_ens), kernel=kernel, noise_variance = gpr_model.likelihood.variance, inducing_variable  = sparse_train_sps)\n",
    "    else:\n",
    "        gpr_model = gpflow.models.GPR( data=(train_sps, train_ens), kernel=kernel, noise_variance = gpr_model.likelihood.variance)      \n",
    "\n",
    "print_summary(gpr_model)\n",
    "\n",
    "if s.sparse_gpflow:\n",
    "    if s.prediction_calculation in (\"direct\", \"cholesky\"):\n",
    "        print(\"Alert: {} prediction approach not implemented for sparse model. Using alpha approach instead.\".format(s.prediction_calculation))\n",
    "        trained_weights = gpr_model.posterior().alpha\n",
    "    elif s.prediction_calculation == \"alpha\":\n",
    "        print(\"Attempting to calculate trained weights using alpha method for sparse gpr model.\")\n",
    "        trained_weights = gpr_model.posterior().alpha\n",
    "        print(\"Successfully calculated trained weights using alpha method for sparse gpr model.\")\n",
    "else:\n",
    "    if s.prediction_calculation in (\"direct\", \"cholesky\"):\n",
    "        KNN = gpr_model.kernel(train_sps)\n",
    "        KNN_diag = tf.linalg.diag_part(KNN)\n",
    "        variance_diag = tf.fill(tf.shape(KNN_diag), gpr_model.likelihood.variance)\n",
    "        KNN_plus_variance = tf.linalg.set_diag(KNN, KNN_diag + variance_diag)\n",
    "        if s.prediction_calculation == \"direct\":\n",
    "            KNN_inv =  tf.linalg.inv(KNN_plus_variance)\n",
    "            trained_weights = tf.matmul(KNN_inv, train_ens)\n",
    "        else:\n",
    "            LNN = tf.linalg.cholesky(KNN_plus_variance)\n",
    "            LNN_inv = tf.linalg.inv(LNN)\n",
    "            KNN_inv_from_L = tf.matmul(LNN_inv, LNN_inv,transpose_a=True)\n",
    "            trained_weights = tf.matmul(KNN_inv_from_L, train_ens)\n",
    "    elif s.prediction_calculation == \"alpha\":\n",
    "        print(\"ERROR: alpha not implemented for gpflow GPR. Skipping prediction\")\n",
    "    \n",
    "TimeAfterTraining = time.time()\n",
    "\n",
    "\n",
    "with tf.GradientTape(watch_accessed_variables=False) as tape_sps:\n",
    "    tape_sps.watch(test_sps)  \n",
    "    print(\"Predicting final energies\")\n",
    "    if s.prediction_calculation == \"predict_f\":\n",
    "        predict_ens, predict_ens_var = gpr_model.predict_f(test_sps)\n",
    "    else:\n",
    "        if s.sparse_gpflow:\n",
    "            predict_ens = tf.reshape( predict_energies_from_weights_tf(trained_weights, sparse_train_sps, test_sps, degree), [-1,1])\n",
    "        else:\n",
    "            predict_ens = tf.reshape( predict_energies_from_weights_tf(trained_weights,        train_sps, test_sps, degree), [-1,1])\n",
    "\n",
    "test_ens_rescaled = ens_scaler.inverse_transform(test_ens)\n",
    "predict_ens_rescaled = ens_scaler.inverse_transform(predict_ens)\n",
    "if s.prediction_calculation == \"predict_f\":\n",
    "    predict_ens_var_rescaled =  np.array(predict_ens_var * ens_scaler.scale_ **2)\n",
    "    \n",
    "if s.use_forces:\n",
    "    print(\"Predicting final forces\")    \n",
    "    predict_d_ens = tape_sps.gradient(predict_ens, test_sps)\n",
    "    predict_frcs = -1*np.einsum('ijk,ik->ij', test_dsp_dx, predict_d_ens) * n_atoms\n",
    "\n",
    "    test_frcs_rescaled = test_frcs * ens_scaler.scale_\n",
    "    predict_frcs_rescaled = predict_frcs * ens_scaler.scale_\n",
    "\n",
    "TimeAfterPrediction = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainingCellNonEpochsTraining = TimeBeforeEpoch0 - TimeBeforePreEpoch + TimeAfterTraining - TimeBeforeWeights \n",
    "if s.n_epochs:\n",
    "    TimePerEpoch = (TimeBeforeWeights - TimeBeforeEpoch0)/s.n_epochs\n",
    "else:\n",
    "    TimePerEpoch = \"N/A\"\n",
    "PredictionTime = TimeAfterPrediction - TimeAfterTraining\n",
    "\n",
    "if s.print_timings:\n",
    "    print(\"{:50s}: {:.3f}\".format(\"Training time outside of epochs in training cell\", TrainingCellNonEpochsTraining))\n",
    "    if s.n_epochs:\n",
    "        print(\"{:50s}: {:.3f}\".format( \"Training time per epoch\", TimePerEpoch))\n",
    "    print(\"{:50s}: {:.3f}\".format(\"Prediction time\", PredictionTime) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if in_notebook:\n",
    "    if 'mse_history_by_n' not in locals():\n",
    "        mse_history_by_n = {}\n",
    "    if 'hyperparam_history_by_n' not in locals():\n",
    "        hyperparam_history_by_n = {}\n",
    "\n",
    "    hyperparam_history_by_n[s.n_structs] = hyperparam_history\n",
    "    mse_history_by_n[s.n_structs] = mse_history\n",
    "\n",
    "    print(\"Stored the hyperparameters and mse values for plotting under n={}\".format(s.n_structs) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hyperparam_training = in_notebook\n",
    "\n",
    "if plot_hyperparam_training:\n",
    "\n",
    "    palette = plt.get_cmap('gist_ncar')#'nipy_spectral')#\"viridis\")\n",
    "    palette_size = palette.N#len(palette.colors)\n",
    "    palette_itr = 0\n",
    "\n",
    "    fig, [[ax00, ax01], [ax10, ax11]] = plt.subplots(nrows=2, ncols = 2, figsize=(16,12))\n",
    "\n",
    "    hyperparam_names = [\"kernel offset\", \"kernel amplitude\", \"observation noise variance\"]\n",
    "\n",
    "    for n in np.sort(list(mse_history_by_n.keys())):\n",
    "        mse_history = mse_history_by_n[n]\n",
    "        hyperparam_history = hyperparam_history_by_n[n]\n",
    "        if not len(hyperparam_history):\n",
    "            continue\n",
    "        color = palette(palette_itr)\n",
    "        palette_itr = (palette_itr + 30) % palette_size\n",
    "\n",
    "\n",
    "\n",
    "        # hyperparameters on axes 00, 01, 10\n",
    "        hyperparams = np.swapaxes(hyperparam_history, 0, 1)\n",
    "\n",
    "        ax00.plot(*zip(*hyperparams[0]), color=color, label=\"{}\".format(n), lw=3)\n",
    "        ax00.plot(*hyperparams[0][-1], \"o\", color=color)\n",
    "        label00 = hyperparam_names[0]\n",
    "        ax00.set_ylabel(\"{}\".format(label00))\n",
    "    #     #ax00.set_yscale('log')\n",
    "    #     annotation00 = ax00.annotate('{:.1f}'.format(amplitudes[-1][1]) , xy=amplitudes[-1], xycoords='data', xytext=(-30,100),\n",
    "    #                                  textcoords='offset points', bbox={'fc':\"1\"}, arrowprops={'fc':'k'}, zorder=2)\n",
    "        ax00.legend()\n",
    "        ax00.ticklabel_format(useOffset=False)\n",
    "\n",
    "        ax01.plot(*zip(*hyperparams[1]), color=color, label=\"{}\".format(n), lw=3)\n",
    "        ax01.plot(*hyperparams[1][-1], \"o\", color=color)\n",
    "        label01 = hyperparam_names[1]\n",
    "        ax01.set_ylabel(\"{}\".format(label01))\n",
    "    #     #ax01.set_yscale('log')\n",
    "    #     annotation01 = ax01.annotate('{:.1f}'.format(lengths[-1][1]) , xy=lengths[-1], xycoords='data', xytext=(100,-30), \n",
    "    #                                  textcoords='offset points', bbox={'fc':\"1\"}, arrowprops={'fc':'k'}, zorder=2)\n",
    "        ax01.legend()\n",
    "        ax01.ticklabel_format(useOffset=False)\n",
    "\n",
    "        ax10.plot(*zip(*hyperparams[-1]), color=color, label=\"{}\".format(n), lw=3)\n",
    "        ax10.plot(*hyperparams[-1][-1], \"o\", color=color)\n",
    "        label10 = hyperparam_names[-1]\n",
    "        ax10.set_ylabel(\"{}\".format(label10))\n",
    "        #ax10.set_yscale('log')\n",
    "    #     annotation01 = ax10.annotate('{:.1e}'.format(noises[-1][1]) , xy=noises[-1], xycoords='data', xytext=(-30,100),\n",
    "    #                                  textcoords='offset points', bbox={'fc':\"1\"}, arrowprops={'fc':'k'}, zorder=2)\n",
    "    #     ax10.legend()\n",
    "        ax10.ticklabel_format(useOffset=False)\n",
    "\n",
    "        if not len(mse_history):\n",
    "            continue\n",
    "\n",
    "        #loss on axis 11\n",
    "        ax11.plot(*zip(*mse_history), color = color, label=\"{}\".format(n), lw=3)\n",
    "        ax11.plot(*mse_history[-1], \"o\", color=color)\n",
    "        #ax11.set_yscale('log')\n",
    "        bottom, top = ax11.get_ylim()\n",
    "        bottom2, top2 = ax11.get_ylim()\n",
    "        ax11.set_ylabel(\"mse\")\n",
    "        ax11.legend()\n",
    "        ax11.ticklabel_format(useOffset=False)\n",
    "\n",
    "        #fig.suptitle(\"{}\".format(s.n_structs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a regroup function\n",
    "test_ens_regrouped = test_ens_rescaled.reshape(-1, len(StructureList[0]))\n",
    "predict_ens_regrouped = predict_ens_rescaled.reshape(-1, len(StructureList[0]))\n",
    "self_energies_regrouped = [[self_energy(atom.symbol, use_librascal_values=True) for atom in StructureList[i]] for i in test_indices]\n",
    "test_global_ens = np.sum(test_ens_regrouped + self_energies_regrouped, axis=1)\n",
    "predict_global_ens = np.sum(predict_ens_regrouped + self_energies_regrouped, axis=1)\n",
    "\n",
    "if s.prediction_calculation == \"predict_f\":\n",
    "    predict_ens_var_regrouped = predict_ens_var_rescaled.reshape(-1, len(StructureList[0]))\n",
    "    predict_global_ens_var = np.sum(predict_ens_var_regrouped, axis=1)\n",
    "    predict_global_ens_std = predict_global_ens_var ** 0.5 \n",
    "    input_std = (gpr_model.likelihood.variance.numpy() * ens_scaler.scale_[0] **2) ** 0.5\n",
    "    print(\"Our observation noise variance implies our reference error is +/- {:.3} /atom\".format( input_std) )\n",
    "else:\n",
    "    predict_global_ens_std = None\n",
    "plot_errors(model_description = \"gpflow model\",\n",
    "            use_local=True,\n",
    "            global_ens=test_global_ens,   predicted_global_ens= predict_global_ens,\n",
    "            local_ens= test_ens_rescaled, predicted_local_ens = predict_ens_rescaled,\n",
    "            color=\"mediumseagreen\", predicted_stdev = None, n_atoms=n_atoms )\n",
    "\n",
    "settings_string = \"\"\n",
    "important_settings = [\"nmax\", \"lmax\", \"rcut\", \"n_structs\", \"n_sparse\", \"n_epochs\"]\n",
    "for key, value in settings_dict.items():\n",
    "    if key in important_settings:\n",
    "        settings_string += \"_\" +str(key) + \"_\" + str(value)\n",
    "import datetime as dt \n",
    "today = dt.datetime.today()\n",
    "date_string = \"_{:02d}_{:02d}_{:d}\".format(today.month, today.day, today.year)\n",
    "# check if existing, add number to end if it is\n",
    "energy_results_title = \"energy_results\" + date_string + settings_string\n",
    "plt.savefig(miniGAP_parent_directory + \"results/\" + energy_results_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if s.use_forces:\n",
    "    predict_frcs_rescaled\n",
    "\n",
    "    fig, axs = plt.subplots(ncols=3, figsize=(20,5))\n",
    "    components = [\"x\", \"y\", \"z\"]\n",
    "    force_max = max(np.max(test_frcs_rescaled), np.max(predict_frcs_rescaled)) + np.std(test_frcs_rescaled)/2\n",
    "    force_min = min(np.min(test_frcs_rescaled), np.min(predict_frcs_rescaled)) - np.std(test_frcs_rescaled)/2\n",
    "\n",
    "    for i in range(3):\n",
    "\n",
    "        axs[i].plot([force_min, force_max], [force_min, force_max], \"-\", c=\"k\")\n",
    "        axs[i].plot(test_frcs_rescaled[:,i], test_frcs_rescaled[:,i], \"o\", c=\"k\", ms=4)\n",
    "        axs[i].plot(test_frcs_rescaled[:,i], predict_frcs_rescaled[:,i], \"o\", label=\"custom\", c=\"mediumseagreen\", ms=5, alpha=.5)\n",
    "        #axs[i].legend()\n",
    "        axs[i].set_xlim(force_min, force_max); axs[i].set_ylim(force_min, force_max)\n",
    "\n",
    "        try:\n",
    "            m, b = np.polyfit(test_frcs_rescaled[:,i], predict_frcs_rescaled[:,i], 1)\n",
    "            r2 = np.corrcoef(test_frcs_rescaled[:,i], predict_frcs_rescaled[:,i])[0,1]\n",
    "            print(\"Least-squares regresion for F{}({}) produces the line line m {}+b with m = {:.5f} and b = {:.5f} which has r2 = {:.5f} \".format(components[i],components[i],components[i],m,b, r2))\n",
    "        except:\n",
    "            pass    \n",
    "\n",
    "    #plt.savefig(\"../media/librascal_database_local_energy_force_learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When converting notebook to script, this line acts as a delimiter. Everything above it will remain. Everything below it will be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save as script "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will save this notebook as a python script with today's date in the code directory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook miniGAP_minimal.ipynb to script\n",
      "[NbConvertApp] Writing 48145 bytes to ../code/miniGAP_from_notebook_12_01_2021.py\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt \n",
    "today = dt.datetime.today()\n",
    "converted_notebook_path = \"../code/miniGAP_from_notebook_{:02d}_{:02d}_{:d}\".format(today.month, today.day, today.year)\n",
    "!jupyter nbconvert --to script miniGAP_minimal.ipynb --output \"{converted_notebook_path}\"\n",
    "!sed -i '/When converting notebook to script, this line acts as a delimiter/,$d' \"{converted_notebook_path}.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:minigap]",
   "language": "python",
   "name": "conda-env-minigap-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
